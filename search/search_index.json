{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI on GKE Assets","text":"<p>This repository contains assets related to AI/ML workloads on Google Kubernetes Engine (GKE).</p>"},{"location":"#overview","title":"Overview","text":"<p>Run optimized AI/ML workloads with Google Kubernetes Engine (GKE) platform orchestration capabilities. A robust AI/ML platform considers the following layers:</p> <ul> <li>Infrastructure orchestration that support GPUs and TPUs for training and serving workloads at scale</li> <li>Flexible integration with distributed computing and data processing frameworks</li> <li>Support for multiple teams on the same infrastructure to maximize utilization of resources</li> </ul>"},{"location":"#infrastructure","title":"Infrastructure","text":"<p>The AI-on-GKE application modules assumes you already have a functional GKE cluster. If not, follow the instructions under infrastructure/README.md to install a Standard or Autopilot GKE cluster.</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 infrastructure\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 platform.tfvars\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 versions.tf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u2514\u2500\u2500 tutorial.md\n</code></pre> <p>To deploy new GKE cluster update the <code>platform.tfvars</code> file with the appropriate values and then execute below terraform commands:</p> <pre><code>terraform init\nterraform apply -var-file platform.tfvars\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>The repo structure looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 applications\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u2514\u2500\u2500 ray\n\u251c\u2500\u2500 contributing.md\n\u251c\u2500\u2500 dcgm-on-gke\n\u2502   \u251c\u2500\u2500 grafana\n\u2502   \u2514\u2500\u2500 quickstart\n\u251c\u2500\u2500 gke-a100-jax\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 build_push_container.sh\n\u2502   \u251c\u2500\u2500 kubernetes\n\u2502   \u2514\u2500\u2500 train.py\n\u251c\u2500\u2500 gke-batch-refarch\n\u2502   \u251c\u2500\u2500 01_gke\n\u2502   \u251c\u2500\u2500 02_platform\n\u2502   \u251c\u2500\u2500 03_low_priority\n\u2502   \u251c\u2500\u2500 04_high_priority\n\u2502   \u251c\u2500\u2500 05_compact_placement\n\u2502   \u251c\u2500\u2500 06_jobset\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cloudbuild-create.yaml\n\u2502   \u251c\u2500\u2500 cloudbuild-destroy.yaml\n\u2502   \u251c\u2500\u2500 create-platform.sh\n\u2502   \u251c\u2500\u2500 destroy-platform.sh\n\u2502   \u2514\u2500\u2500 images\n\u251c\u2500\u2500 gke-disk-image-builder\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cli\n\u2502   \u251c\u2500\u2500 go.mod\n\u2502   \u251c\u2500\u2500 go.sum\n\u2502   \u251c\u2500\u2500 imager.go\n\u2502   \u2514\u2500\u2500 script\n\u251c\u2500\u2500 gke-dws-examples\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 dws-queues.yaml\n\u2502   \u251c\u2500\u2500 job.yaml\n\u2502   \u2514\u2500\u2500 kueue-manifests.yaml\n\u251c\u2500\u2500 gke-online-serving-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 gke-tpu-examples\n\u2502   \u251c\u2500\u2500 single-host-inference\n\u2502   \u2514\u2500\u2500 training\n\u251c\u2500\u2500 indexed-job\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 mnist.py\n\u251c\u2500\u2500 jobset\n\u2502   \u2514\u2500\u2500 pytorch\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u251c\u2500\u2500 saxml-on-gke\n\u2502   \u251c\u2500\u2500 httpserver\n\u2502   \u2514\u2500\u2500 single-host-inference\n\u251c\u2500\u2500 training-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 tutorial.md\n\u2514\u2500\u2500 tutorials\n    \u251c\u2500\u2500 e2e-genai-langchain-app\n    \u251c\u2500\u2500 finetuning-llama-7b-on-l4\n    \u2514\u2500\u2500 serving-llama2-70b-on-l4-gpus\n</code></pre>"},{"location":"#jupyter-hub","title":"Jupyter Hub","text":"<p>This repository contains a Terraform template for running JupyterHub on Google Kubernetes Engine. We've also included some example notebooks ( under <code>applications/ray/example_notebooks</code>), including one that serves a GPT-J-6B model with Ray AIR (see here for the original notebook). To run these, follow the instructions at applications/ray/README.md to install a Ray cluster.</p> <p>This jupyter module deploys the following resources, once per user: - JupyterHub deployment - User namespace - Kubernetes service accounts</p> <p>Learn more about JupyterHub on GKE here</p>"},{"location":"#ray","title":"Ray","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine.</p> <p>This module deploys the following, once per user: - User namespace - Kubernetes service accounts - Kuberay cluster - Prometheus monitoring - Logging container</p> <p>Learn more about Ray on GKE here</p>"},{"location":"#important-considerations","title":"Important Considerations","text":"<ul> <li>Make sure to configure terraform backend to use GCS bucket, in order to persist terraform state across different environments.</li> </ul>"},{"location":"#licensing","title":"Licensing","text":"<ul> <li>The use of the assets contained in this repository is subject to compliance with Google's AI Principles</li> <li>See LICENSE</li> </ul>"},{"location":"cloudshell-tutorial/","title":"Cloudshell tutorial","text":""},{"location":"cloudshell-tutorial/#lets-get-started","title":"Let's get started!","text":"<p>Welcome to the Cloudshell tutorial for AI on GKE!</p> <p>This guide will show you how to prepare a GKE cluster and install the AI applications on GKE. It'll also walk you through the configuration files that can be provided with custom inputs and commands that will complete the tutorial.</p> <p>Time to complete: About 30 minutes</p> <p>Prerequisites: GCP project linked with a Cloud Billing account</p> <p>To begin, click Start.</p>"},{"location":"cloudshell-tutorial/#what-is-ai-on-gke","title":"What is AI-on-GKE","text":"<p>This tutorial Terraform &amp; Cloud Build to provision the infrastructure as well as deploy the workloads</p> <p>You'll be performing the following activities:</p> <ol> <li>Set project-id for gcloud CLI</li> <li>Update terraform variable values to create infrastructure</li> <li>Update terraform variable values to provide workload configuration</li> <li>Create a GCS bucket to store terraform state</li> <li>Configure service account to be used for deployment</li> <li>Submit Cloud build job to create infrastructure &amp; deploy workloads</li> </ol> <p>To get started, click Next.</p>"},{"location":"cloudshell-tutorial/#step-0-set-your-project","title":"Step 0: Set your project","text":"<p>To set your Cloud Platform project for this terminal session use:</p> <pre><code>gcloud config set project [PROJECT_ID]\n</code></pre> <p>All the resources will be created in this project</p>"},{"location":"cloudshell-tutorial/#step-1-provide-platform-inputs-parameters-for-terraform","title":"Step 1: Provide PLATFORM Inputs Parameters for Terraform","text":"<p>Here on step 1 you need to update the PLATFORM terraform tfvars file (located in ./platform/platform.tfvars) to provide the input parameters to allow terraform code execution to provision GKE resources. This will include the input parameters in the form of key value pairs. Update the values as per your requirements.</p> <p> Open platform.tfvars  </p> <p>Update <code>project_id</code> and review the other default values.</p> <p>Tip: Click the highlighted text above to open the file in your cloudshell editor.</p> <p>You can find tfvars examples in the tfvars_examples folder.</p>"},{"location":"cloudshell-tutorial/#step-2-provide-application-inputs-parameters-for-terraform","title":"Step 2: Provide APPLICATION Inputs Parameters for Terraform","text":"<p>Here on step 2 you need to update the APPLICATION terraform tfvars file (located in ./workloads/workloads.tfvars) to provide the input parameters to allow terraform code execution to provision the APPLICATION WORKLOADS. This will include the input parameters in the form of key value pairs. Update the values as per your requirements.</p> <p> Open workloads.tfvars </p> <p>Update <code>project_id</code> and review the other default values.</p> <p>Tip: Click the highlighted text above to open the file on your cloudshell.</p>"},{"location":"cloudshell-tutorial/#step-3-optional-configure-terraform-gcs-backend","title":"Step 3: [Optional] Configure Terraform GCS backend","text":"<p>You can also configure the GCS bucket to persist the terraform state file for further usage. To configure the terraform backend you need to have a GCS bucket already created. This needs to be done both for PLATFORM and APPLICATION stages.</p>"},{"location":"cloudshell-tutorial/#optional-create-gcs-bucket","title":"[Optional] Create GCS Bucket","text":"<p>In case you don't have a GCS bucket already, you can create using terraform or gcloud command as well. Refer below for the gcloud command line to create a new GCS bucket.</p> <pre><code>gcloud storage buckets create gs://BUCKET_NAME\n</code></pre> <p>Tip: Click the copy button on the side of the code box to paste the command in the Cloud Shell terminal to run it.</p>"},{"location":"cloudshell-tutorial/#optional-modify-platform-terraform-state-backend","title":"[Optional] Modify PLATFORM Terraform State Backend","text":"<p>Modify the ./platform/backend.tf and uncomment the code and update the backend bucket name.  Open ./platform/backend.tf  </p> <p>After changes file will look like below:</p> <pre><code>terraform {\n backend \"gcs\" {\n   bucket  = \"BUCKET_NAME\"\n   prefix  = \"terraform/state\"\n }\n}\n</code></pre> <p>Refer here for more details. </p>"},{"location":"cloudshell-tutorial/#optional-application-terraform-state-backend","title":"[Optional] APPLICATION Terraform State Backend","text":"<p>Modify the ./workloads/backend.tf and uncomment the code and update the backend bucket name.  Open ./workloads/backend.tf </p> <p>After changes file will look like below:</p> <pre><code>terraform {\n backend \"gcs\" {\n   bucket  = \"BUCKET_NAME\"\n   prefix  = \"terraform/state/workloads\"\n }\n}\n</code></pre> <p>Refer here for more details. </p>"},{"location":"cloudshell-tutorial/#step-4-configure-cloud-build-service-account","title":"Step 4: Configure Cloud Build Service Account","text":"<p>The Cloud Build service that orchestrates the environment creation requires a Service Account. Please run the following steps to create it and grant the roles required for deployment.</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\ngcloud iam service-accounts create aiongke --display-name=\"AI on GKE Service Account\"\n./iam/iam_policy.sh\n</code></pre>"},{"location":"cloudshell-tutorial/#step-5-run-terraform-apply-using-cloud-build","title":"Step 5: Run Terraform Apply using Cloud Build","text":"<p>You are ready to deploy your resources now! <code>cloudbuild.yaml</code> is already prepared with all the steps required to deploy the application. </p> <p>Run the below command to submit Cloud Build job to deploy the resources:</p> <pre><code>gcloud beta builds submit --config=cloudbuild.yaml --substitutions=_PLATFORM_VAR_FILE=\"platform.tfvars\",_WORKLOADS_VAR_FILE=\"workloads.tfvars\"\n</code></pre> <p>Monitor the terminal for the log link and status for cloud build jobs.</p>"},{"location":"cloudshell-tutorial/#step-6-optional-delete-resources-created","title":"Step 6: [Optional] Delete resources created","text":"<p>You can now delete the resources</p> <pre><code>gcloud beta builds submit --config=cloudbuild_delete.yaml  --substitutions=_PLATFORM_VAR_FILE=\"platform.tfvars\",_WORKLOADS_VAR_FILE=\"workloads.tfvars\"\n</code></pre> <p>Monitor the terminal for the log link and status for cloud build jobs.</p>"},{"location":"cloudshell-tutorial/#congratulations","title":"Congratulations","text":"<p>You're all set!</p> <p>You can now access your cluster and applications.</p>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/","title":"Guide to Serving Mistral 7B-Instruct v0.1 on GKE Utilizing Nvidia L4-GPUs","text":"<p>Learn how to serve the Mistral 7B instruct v0.1 chat model on GKE using just 1 x L4 GPU. This tutorial adapts the HF-text-generation-inference project for serving Mistral AI's model.</p>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terminal Setup: Ensure you have kubectl and gcloud installed. Using Cloud Shell is highly  recommended for its simplicity and built-in tools.</li> <li>GPU Quota: Confirm you have the quota for at least one L4 GPU in your Google Cloud account.</li> <li>Model Access: Secure access to the Mistral 7B model by agreeing to the terms on Hugging Face, which typically involves creating an account and accepting the model's use conditions.</li> <li>Ensure you currently have installed a stable version of Transformers, 4.34.0 or newer.</li> <li>(OPTIONAL) If you intend to utilize the HPA, (horizontal pod autoscaler) in order to scale for incoming requests please make sure that the 'maxReplicas' assignment in your mistral-7b.yaml HorizontalPodAutoscaler section is configured to equal or be less than the number of GPUs you have available for the deployment. Additionally, ensure that you have a DCGM (Data Center GPU Manager) NVIDIA pod configured within your Kubernetes cluster to collect GPU metrics. Look at DCGM documentation for guidance on setting up and configuring this pod properly. This is essential for the Horizontal Pod Autoscaler (HPA) to accurately scale based on GPU utilization. Without proper GPU metrics, the autoscaler won't be able to make informed scaling decisions, potentially leading to under or over-provisioning of resources. Integrate the DCGM pod within your cluster's monitoring system to provide real-time GPU performance data to the HPA.+</li> </ul>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#gpu-memory-allocation","title":"GPU-Memory Allocation","text":"<p>For the Mistral-7B-Instruct-v0.1 model without quantization, the memory requirement is approximately 14.2 GB. Given that an L4 GPU has 24 GB of GPU memory, a single L4 GPU is sufficient to run the model, including some overhead for operational processes. This setup ensures effective deployment on a single GPU without the need for additional resources.</p>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#feature-specific-to-model","title":"Feature-specific to Model","text":"<ul> <li>Model: Mistral-7B-instruct-v0.1, a transformer-based architecture.</li> <li>Attention Mechanisms:<ul> <li>Grouped-Query Attention: GQA categorizes query heads into groups, with each group sharing a common key and value projection. This setup allows for three variations: GQA-1, which is similar to MQA; GQA-H, mirroring the concept of MHA; and GQA-G, an intermediate state that balances between efficiency and expressiveness. By organizing query heads into groups, GQA reduces memory overhead and allows for nuanced control over the model's performance, effectively mitigating challenges related to memory bandwidth in large context scenarios</li> <li>Sliding-Window Attention: This attention mechanism limits the focus of the model to a nearby set of positions around each token, enhancing the model's ability to capture and utilize local contextual information more effectively, improving understanding and generation of text that relies on close proximity relationships.</li> <li>Byte-fallback BPE tokenizer: A tokenizer that combines byte-level encoding with Byte Pair Encoding (BPE) to ensure a balance between efficiency and coverage, allowing for the encoding of a wide range of text inputs, including those with uncommon characters or symbols, by falling back to byte-level representation when necessary.</li> </ul> </li> </ul> <p>Set your region and project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get-value project)\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#gke-cluster-creation","title":"GKE Cluster Creation:","text":"<pre><code># Adjust if your specific setup has different requirements:\ngcloud container clusters create mistral-cluster-gke  \\\n    --location=${REGION} \\\n    --node-locations=${REGION} \\\n    --project= ${PROJECT_ID} \\\n    --machine-type=n1-standard-4 \\\n    --no-enable-master-authorized-networks \\\n    --addons=GcsFuseCsiDriver \\\n    --num-nodes=3 \\\n    --min-nodes=1 \\\n    --max-nodes=5 \\\n    --enable-ip-alias \\\n    --enable-image-streaming \\\n    --enable-shielded-nodes \\\n    --shielded-secure-boot \\\n    --shielded-integrity-monitoring \\\n    --workload-pool=${PROJECT_ID}svc.id.goog\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#node-pool-with-single-l4-gpu","title":"Node Pool with Single L4 GPU:","text":"<p>Create a node pool for deploying Mistral 7B with a single L4 GPU {1 x L4}:</p> <pre><code>gcloud container node-pools create mistral-gpu-pool \\\n    --cluster=mistral-cluster \\\n    --region=${REGION} \\\n    --project=${PROJECT_ID}} \\\n    --machine-type=g2-standard-12 \\\n    --accelerator=type=nvidia-l4,count=1,gpu-driver-version=latest \\\n    --ephemeral-storage-local-ssd=count=2 \\\n    --node-locations=${ZONE} \\\n    --num-nodes=1 \\\n    --enable-autoscaling \\\n    --min-nodes=1 \\\n    --max-nodes=2 \\\n    --node-labels=accelerator=nvidia-gpu\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#hugging-face-authentication","title":"Hugging Face Authentication:","text":"<p>Obtain your Hugging Face token for model access: https://huggingface.co/settings/tokens For Hugging Face authentication and to download the Mistral model:</p> <pre><code>export HF_TOKEN=&lt;your-token&gt;\nkubectl create secret generic mistral-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Please use the given YAML file named mistral-7b.yaml with the provided content: Mistral 7B instruct-deployment.</p> <p>Deploy Mistral 7B with the following command:</p> <pre><code>kubectl apply -f mistral-7b.yaml\n</code></pre> <p>Assess the details of your deployment using the following command (Adjust if you utilized a different name in your YAML):</p> <pre><code>kubectl describe deployment mistral-7b -n default\n</code></pre> <p>Your output should resemble the following; please make sure the details are in accordance with your declarations and deployment needs: </p> <pre><code>Name:                   mistral-7b\nNamespace:              default\nCreationTimestamp:      Fri, 15 Mar 2024 11:47:53 -0700\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=mistral-7b\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=mistral-7b\n  Containers:\n   mistral-7b:\n    Image:      us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n    Port:       8080/TCP\n    Host Port:  0/TCP\n    Limits:\n      nvidia.com/gpu:  1\n    Environment:\n      MODEL_ID:   mistralai/Mistral-7B-Instruct-v0.1\n      NUM_SHARD:  1\n      PORT:       8080\n      QUANTIZE:   bitsandbytes-nf4\n    Mounts:\n      /data from data (rw)\n      /dev/shm from dshm (rw)\n  Volumes:\n   dshm:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\n   data:\n    Type:          HostPath (bare host directory volume)\n    Path:          /mnt/stateful_partition/kube-ephemeral-ssd/mistral-data\n    HostPathType:  \nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   mistral-7b-7b8bd5c7f4 (1/1 replicas created)\nEvents:          &lt;none&gt;\n</code></pre> <p>Test the deployment by forwarding the port and using curl to send a prompt:</p> <pre><code>kubectl port-forward deployment/mistral-7b 8080:8080\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#check-for-deployment-and-retrieve-load-balancer-external-exposed-ip-address","title":"Check for deployment and retrieve Load-Balancer external exposed ip address:","text":"<p>List Services: First, list all services in the namespace (assuming default namespace as per your YAML) to make sure your service is up and running.</p> <pre><code>kubectl get svc --namespace=default\n</code></pre> <p>Attach correct labels to load balancer deployment:</p> <pre><code>kubectl label service mistral-7b-service LLM_deployment=true model_LoadBalancer=true Ingress_Point=true\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#generate-load-balancer-details","title":"Generate Load Balancer Details:","text":"<p>To describe all services of type Load Balancer in your Kubernetes cluster, you should be able to see the details of your mistral LB here and get the ingress as well as external ip. (The external exposed ip will be important for connecting it to external services).</p> <pre><code>kubectl get services --all-namespaces -o wide | grep LoadBalancer | while read -r namespace name type cluster_ip external_ip ports age; do echo \"Describing service $name in namespace $namespace:\"; kubectl describe service -n $namespace $name; done\n</code></pre> <p>Your Load Balancer details should resemble the following Describing service mistral-7b-service in namespace default:</p> <pre><code>ame:                     mistral-7b-service\nNamespace:                default\nLabels:                   Ingress_Point=true\n                          LLM_deployment=true\n                          model_LoadBalancer=true\nAnnotations:              cloud.google.com/neg: {\"ingress\":true}\nSelector:                 app=mistral-7b\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.88.55.248\nIPs:                      10.88.55.248\nLoadBalancer Ingress:     34.125.177.85   # This ingress serves as your external IP\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  32683/TCP\nEndpoints:                10.92.3.4:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#optional-if-using-hpa-make-sure-hpa-horizontal-pod-autoscalar-is-working-please-see-hpa-documentation-and-preconfigurations-needed","title":"OPTIONAL: (If using HPA) Make sure HPA Horizontal Pod Autoscalar is working (Please see HPA documentation and preconfigurations needed):","text":"<p>Please make sure you have adjusted the yaml file appropriately.</p> <pre><code># kubectl describe hpa mistral-7b-hpa\n</code></pre>"},{"location":"genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#test-the-model-deployment","title":"Test the model deployment","text":"<p>Interact with the Model via curl, replace ip address with your external load balancer ip obtained from the kubectl get svc --namespace=default command earlier.</p> <pre><code># Define the instruction\ninstruction=\"INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief, malice, or misunderstanding. Your words should never wander into realms of rudeness, unfairness, or deception. Ensure your guidance is free from any bias and shines with positivity. If a query seems like a riddle without an answer or strays too far from Wonderland's wisdom, kindly explain why it doesn't hold water in our world rather than leading someone astray. Should a question's answer elude you in the vastness of Wonderland, refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\n</code></pre> <p>Measure the latency of a model deployment by timing a POST request to a model's generate endpoint, calculating the total response time and the latency per generated token. Make sure deployment latency is within tolerneces, otherwise your cluster/nodepool is misconfigured.</p> <pre><code>#### Test model deployment latency, Please make sure you differentiate hot and cold latency.\nstart=$(date +%s.%N)\n\ninstruction=\"[INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief. Your words should never wander into realms of rudeness, unfairness, or deception. refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\nend=$(date +%s.%N)\n\n# Extract the number of generated tokens from the response\ngenerated_tokens=$(echo \"$response\" | jq '.[0].details.generated_tokens')\n\n\n# If jq didn't find the field, or it's not a number, default to 1 to avoid division by zero\nif ! [[ \"$generated_tokens\" =~ ^[0-9]+$ ]]; then\n  generated_tokens=1\nfi\n\ntotal_latency=$(echo \"$end - $start\" | bc)\n\n# Calculate latency per generated token / This is meant to calculate general latency, might vary depending on your own setup. \nlatency_per_token=$(echo \"scale=6; $total_latency / $generated_tokens\" | bc)\n\necho \"Total Latency: $total_latency seconds\"\necho \"Generated Tokens: $generated_tokens\"\necho \"Latency per Generated Token: $latency_per_token seconds\"\n\n</code></pre> <p>Visit the API docs at http://localhost:8080/docs for more details.</p> <p>This README provides a concise guide to deploying the Mistral 7B instruct v.01 model, listed above are key steps and adjustments needed for a general sample deployment. Ensure to replace placeholders and commands with the specific details of your GKE setup and Mistralv01-instruct model deployment.</p>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/","title":"Guide to Serving Mixtral 8x7 Model on GKE Utilizing Nvidia L4-GPUs","text":"<p>This guide walks you through the process of serving the Mixtral 8x7 model on Google Kubernetes Engine (GKE) leveraging Nvidia L4 GPUs. We'll adapt from the previous Mistral model deployment, focusing on a quadpod L4 GPU setup for enhanced performance.</p>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terminal Setup: Ensure kubectl and gcloud are installed. Google Cloud Shell is recommended for its ease of use and built-in tools.</li> <li>GPU Quota: Confirm you have the quota for at least four L4 GPUs in your Google Cloud account.</li> <li>Model Access: Secure access to the Mixtral 8x7 model by agreeing to terms on a designated platform, typically involving account creation and acceptance of use conditions. Transformers Library: Ensure you have installed a stable version of the Transformers library, version 4.34.0 or newer.</li> <li>HPA (Optional): If you plan to use the Horizontal Pod Autoscaler (HPA) to scale for incoming requests, ensure the 'maxReplicas' assignment in your mixtral-8x7.yaml HorizontalPodAutoscaler section is set to equal or be less than the number of GPUs available for deployment.</li> </ul>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#gpu-memory-allocation-and-quantization-strategy","title":"GPU-Memory Allocation and Quantization Strategy","text":"<p>GPU-Memory Allocation and Quantization Strategy When deploying the Mixtral 8x7 model, it's crucial to assess both the memory requirements and the computational efficiency, especially when leveraging Nvidia L4 GPUs, each with 24 GB of GPU memory. A key factor in this consideration is the use of quantization techniques to optimize model performance and memory usage.</p> <p>Currently, the deployment employs 4-bit quantization (using the bitsandbytes-nf4 method), which significantly reduces the memory footprint of the model while maintaining a balance between performance and resource allocation. This quantization strategy allows the model to be efficiently served on a single L4 GPU for standard operations.</p> <p>However, should you opt for a larger quantization scale (upwards to no quantization), accommodating the increased memory and computational demands may necessitate scaling up to eight L4 GPUs to achieve optimal production performance. Consequently, it's advisable to adjust the number of shards to 8 in such scenarios to fully leverage the enhanced processing capacity and ensure that the deployment is scaled appropriately to handle the increased workload efficiently.</p>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#feature-specific-to-model","title":"Feature-specific to Model","text":"<ul> <li>Model: Mixtral8x7B-instruct-v0.1, a transformer-based architecture contained in a Mixture of experts with sparse gating model.</li> <li> <p>Attention Mechanisms:</p> <ul> <li>Grouped-Query Attention (GQA): The GQA framework is an innovative approach to managing memory and computational resources more efficiently. By categorizing attention heads into groups, each sharing a common key and value projection, GQA facilitates three distinct configurations:</li> <li>GQA-1, akin to MQA (Mixed Query Attention), focuses on enhancing the model's responsiveness and agility in handling queries.</li> <li>GQA-H, reflective of the traditional Multi-Head Attention (MHA) mechanism, balances complexity with computational efficiency.</li> <li>GQA-G, an intermediate form, optimizes both expressiveness and efficiency. This organization of query heads into groups minimizes memory overhead and enables precise control over performance, addressing challenges in scenarios requiring extensive context processing.</li> <li>Sliding-Window Attention: This technique restricts the model's attention span to a localized set of positions around each token. By doing so, it significantly boosts the ability to grasp and leverage context-specific information, sharpening the model's comprehension and generation capabilities, especially for text requiring nuanced understanding of proximate relationships.</li> </ul> </li> <li> <p>Tokenization:</p> <ul> <li>Hybrid-fallback Tokenizer: Building on the byte-fallback BPE tokenizer's foundation, the Hybrid-fallback tokenizer introduces an adaptive mechanism that seamlessly switches between byte-level and subword encoding. This dual approach ensures comprehensive coverage across a diverse array of textual inputs, from highly frequent words to rare or out-of-vocabulary terms, guaranteeing no loss in textual fidelity.</li> </ul> </li> <li>Mixture of Experts (MoE) Mixtral utilizes a unique component within its transformer architecture known as the MoE (Mixture-of-Experts) layer. This layer functions by intelligently directing input across a dynamically chosen subset of expert networks, each distinct in their training on various segments of data or specific tasks. By engaging this method, Mixtral is equipped to harness specialized knowledge and insights from these experts. This approach significantly enhances the model's capability for generating and analyzing text with greater accuracy and depth.</li> </ul> <p>Set your region and project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get-value project)\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#gke-cluster-creation","title":"GKE Cluster Creation:","text":"<pre><code>gcloud container clusters create mixtral8x7-cluster-gke \\\n  --region=${REGION} \\\n  --node-locations=${REGION} \\\n  --project=${PROJECT_ID} \\\n  --machine-type=n2d-standard-8 \\\n  --no-enable-master-authorized-networks \\\n  --addons=HorizontalPodAutoscaling \\\n  --addons=HttpLoadBalancing \\\n  --addons=GcePersistentDiskCsiDriver \\\n  --addons=GcsFuseCsiDriver \\\n  --num-nodes=4 \\\n  --min-nodes=3 \\\n  --max-nodes=6 \\\n  --enable-ip-alias \\\n  --enable-image-streaming \\\n  --enable-shielded-nodes \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --logging=SYSTEM,WORKLOAD \\\n  --monitoring=SYSTEM \\\n  --enable-autoupgrade \\\n  --enable-autorepair \\\n  --network=\"projects/${PROJECT_ID}/global/networks/default\" \\\n  --subnetwork=\"projects/${PROJECT_ID}/regions/${REGION}/subnetworks/default\" \\\n  --tags=web,sftp \\\n  --labels=env=production,team=mixtral8x7 \\\n  --release-channel=regular\n\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#node-pool-with-single-l4-gpu","title":"Node Pool with Single L4 GPU:","text":"<p>Create a node pool for deploying Mixtral 7B with quadpod deployment L4 GPU {4 x L4}:</p> <pre><code>gcloud container node-pools create mixtral-moe-gpu-pool \\\n  --cluster=mixtral8x7-cluster-gke  \\\n  --project=gke-aishared-dev \\\n  --machine-type=g2-standard-48 \\\n  --ephemeral-storage-local-ssd=count=4 \\\n  --accelerator=type=nvidia-l4,count=4 \\\n  --node-locations=us-west4-a \\\n  --enable-image-streaming \\\n  --num-nodes=1 \\\n  --enable-autoscaling \\\n  --min-nodes=1 \\\n  --max-nodes=2 \\\n  --node-labels=accelerator=nvidia-gpu \\\n  --workload-metadata=GKE_METADATA\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#hugging-face-authentication","title":"Hugging Face Authentication:","text":"<p>Obtain your Hugging Face token for model access: https://huggingface.co/settings/tokens For Hugging Face authentication and to download the Mixtral8x7 instruct model:</p> <pre><code>export HF_TOKEN=&lt;your-token&gt;\nkubectl create secret generic mixtral-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Please use the given YAML file named mixtral-8x7b.yaml with the provided content: Mixtral8x7Bv0.1 instruct-deployment.</p> <p>Deploy Mixtral8x7B with the following command:</p> <pre><code>kubectl apply -f mixtral8x7b.yaml\n</code></pre> <p>Assess the details of your deployment using the following command (Adjust if you utilized a different name in your YAML):</p> <pre><code>kubectl describe deployment mixtral8x7b -n default\n</code></pre> <p>Your output should resemble the following; please make sure the details are in accordance with your declarations and deployment needs: </p> <pre><code>Name:                   mixtral8x7b\nNamespace:              default\nCreationTimestamp:      Fri, 22 Mar 2024 12:09:13 -0700\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 9\nSelector:               app=mixtral8x7b\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=mixtral8x7b\n  Containers:\n   mixtral8x7b:\n    Image:      us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n    Port:       8080/TCP\n    Host Port:  0/TCP\n    Limits:\n      cpu:             5\n      memory:          42Gi\n      nvidia.com/gpu:  4\n    Requests:\n      cpu:             5\n      memory:          42Gi\n      nvidia.com/gpu:  4\n    Environment:\n      QUANTIZE:   bitsandbytes-nf4\n      MODEL_ID:   mistralai/Mixtral-8x7B-Instruct-v0.1\n      NUM_SHARD:  2\n      PORT:       8080\n    Mounts:\n      /data from ephemeral-volume (rw)\n      /dev/shm from dshm (rw)\n  Volumes:\n   dshm:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\n   data:\n    Type:          HostPath (bare host directory volume)\n    Path:          /mnt/stateful_partition/kube-ephemeral-ssd/mixtral-data\n    HostPathType:  \n   ephemeral-volume:\n    Type:          EphemeralVolume (an inline specification for a volume that gets created and deleted with the pod)\n    StorageClass:  premium-rwo\n    Volume:        \n    Labels:            type=ephemeral\n    Annotations:       &lt;none&gt;\n    Capacity:      \n    Access Modes:  \n    VolumeMode:    Filesystem\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nNewReplicaSet:   mixtral8x7b-5c888947fd (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  43m   deployment-controller  Scaled up replica set mixtral8x7b-5c888947fd to 1\n</code></pre> <p>Test the deployment by forwarding the port and using curl to send a prompt:</p> <pre><code>kubectl port-forward deployment/mixtral8x7b 8080:8080\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#check-for-deployment-and-retrieve-load-balancer-external-exposed-ip-address","title":"Check for deployment and retrieve Load-Balancer external exposed ip address:","text":"<p>List Services: First, list all services in the namespace (assuming default namespace as per your YAML) to make sure your service is up and running.</p> <pre><code>kubectl get svc --namespace=default\n</code></pre> <p>Attach correct labels to load balancer deployment:</p> <pre><code>kubectl label service mixtral8x7b-service LLM_deployment=true model_LoadBalancer=true Ingress_Point=true\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#generate-load-balancer-details","title":"Generate Load Balancer Details:","text":"<p>To describe all services of type Load Balancer in your Kubernetes cluster, you should be able to see the details of your mixtral-LB here and get the ingress as well as external ip. (The external exposed ip will be important for connecting it to external services).</p> <pre><code>kubectl get services --all-namespaces -o wide | grep LoadBalancer | while read -r namespace name type cluster_ip external_ip ports age; do echo \"Describing service $name in namespace $namespace:\"; kubectl describe service -n $namespace $name; done\n</code></pre> <p>Your Load Balancer details should resemble the following Describing service mixtral8x7b-service in namespace default:</p> <pre><code>Name:                     mixtral8x7b-service\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=mixtral8x7b\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.65.140.207\nIPs:                      10.65.140.207\nLoadBalancer Ingress:     34.16.133.177\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  30640/TCP\nEndpoints:                10.92.1.10:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#optional-if-using-hpa-make-sure-hpa-horizontal-pod-autoscalar-is-working-please-see-hpa-documentation-and-preconfigurations-needed","title":"OPTIONAL: (If using HPA) Make sure HPA Horizontal Pod Autoscalar is working (Please see HPA documentation and preconfigurations needed):","text":"<p>Please make sure you have adjusted the yaml file appropriately.</p> <pre><code># kubectl describe hpa mixtral-8x7b-hpa\n</code></pre>"},{"location":"genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#test-the-model-deployment","title":"Test the model deployment","text":"<p>Interact with the Model via curl, replace ip address with your external load balancer ip obtained from the kubectl get svc --namespace=default command earlier.</p> <pre><code># Define the instruction\ninstruction=\"INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief, malice, or misunderstanding. Your words should never wander into realms of rudeness, unfairness, or deception. Ensure your guidance is free from any bias and shines with positivity. If a query seems like a riddle without an answer or strays too far from Wonderland's wisdom, kindly explain why it doesn't hold water in our world rather than leading someone astray. Should a question's answer elude you in the vastness of Wonderland, refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\n</code></pre> <p>Measure the latency of a model deployment by timing a POST request to a model's generate endpoint, calculating the total response time and the latency per generated token. Make sure deployment latency is within tolerneces, otherwise your cluster/nodepool is misconfigured.</p> <pre><code>#### Test model deployment latency, Please make sure you differentiate hot and cold latency.\nstart=$(date +%s.%N)\n\ninstruction=\"[INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief. Your words should never wander into realms of rudeness, unfairness, or deception. refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\nend=$(date +%s.%N)\n\n# Extract the number of generated tokens from the response\ngenerated_tokens=$(echo \"$response\" | jq '.[0].details.generated_tokens')\n\n\n# If jq didn't find the field, or it's not a number, default to 1 to avoid division by zero\nif ! [[ \"$generated_tokens\" =~ ^[0-9]+$ ]]; then\n  generated_tokens=1\nfi\n\ntotal_latency=$(echo \"$end - $start\" | bc)\n\n# Calculate latency per generated token / This is meant to calculate general latency, might vary depending on your own setup. \nlatency_per_token=$(echo \"scale=6; $total_latency / $generated_tokens\" | bc)\n\necho \"Total Latency: $total_latency seconds\"\necho \"Generated Tokens: $generated_tokens\"\necho \"Latency per Generated Token: $latency_per_token seconds\"\n\n</code></pre> <p>Visit the API docs at http://localhost:8080/docs for more details.</p> <p>This README provides a concise guide to deploying the Mixtral8x7B instruct v.01 model, listed above are key steps and adjustments needed for a general sample deployment. Ensure to replace placeholders and commands with the specific details of your GKE setup and Mixtral-v.01-instruct model deployment.</p>"},{"location":"genAI-LLM/e2e-genai-langchain-app/","title":"E2E GenAI application with Langchain, Ray, Flask API backend, React frontend","text":"<p>In this tutorial you will deploy a end-to-end application that will use GenAI model from Hugging Face on the backend, Ray Serve for inference, Flask API backend, and simple React frontend.</p>"},{"location":"genAI-LLM/e2e-genai-langchain-app/#before-you-begin","title":"Before you begin","text":"<p>Create or select an existing GCP project and open Cloud Shell. You can use these steps</p>"},{"location":"genAI-LLM/e2e-genai-langchain-app/#infrastructure-installation","title":"Infrastructure Installation","text":"<ol> <li>If needed, <code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git</code></li> <li>Create a new GKE cluster and install the kuberay operator<ol> <li>cd <code>gke-platform</code></li> <li>Edit <code>variables.tf</code> with your GCP settings. Make sure you change <code>project_id</code> and <code>cluster_name</code>.</li> <li>Run <code>terraform init</code></li> <li>Run <code>terraform apply</code></li> </ol> </li> <li>Configure credentials to point to the cluster: <code>gcloud container clusters get-credentials &lt;cluster-name&gt; --location=&lt;region&gt;</code></li> <li>Install Ray on GKE<ol> <li>cd <code>ray-on-gke/user</code></li> <li>Edit variables.tf with your GCP settings. Make sure you set: <code>project_id</code>, <code>namespace</code>, <code>service_account</code>.</li> <li>Note the namespace setting. All microservices in this sample will be deployed to this same namespace for simplicity.</li> <li>Run <code>terraform init</code></li> <li>Run <code>terraform apply</code></li> </ol> </li> <li>Install Jupyter on GKE. These steps are needed to experimentation (see the section below). You can skip it if you want to go straight to building the application. <ol> <li>cd <code>jupyter-on-gke</code></li> <li>Edit <code>variables.tf</code> with your GCP settings. Make sure that you set <code>project_id</code>, <code>project_number</code>, <code>namespace</code>. Use the same namespace as above.</li> <li>Configure higher resource limits and guarantees. In <code>jupyter_config/config.yaml</code> change the following:     <code>yaml     singleuser:     cpu:         limit: 1         guarantee: .5     memory:         limit: 4G         guarantee: 1G</code></li> <li>Run terraform init</li> <li>Run terraform apply</li> </ol> </li> </ol>"},{"location":"genAI-LLM/e2e-genai-langchain-app/#experimentation","title":"Experimentation","text":"<p>Experiment with the model in Jupyter Notebook: 1. Get the address of your Jupyter hub:  <code>kubectl get service proxy-public -n &lt;namespace name&gt; -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code> 1. Configure IAP and open Jupyter Hub by following the steps in here. 1. From JupyterHub open this notebook: <code>https://raw.githubusercontent.com/GoogleCloudPlatform/ai-on-gke/main/tutorials/langchain/nb1.ipynb</code> and run it step by step 1. The first section shows how to run the model directly 1. The second section shows how to do the same using <code>Ray Serve</code>.</p>"},{"location":"genAI-LLM/e2e-genai-langchain-app/#build-the-end-to-end-application","title":"Build the end-to-end application","text":"<ol> <li> <p>We used Jupyter Notebook to experiment, but now let's build the Flask backend that calls into <code>Ray Serve</code>.</p> <ol> <li>Observe <code>model.py</code>: it loads the model and creates <code>Ray.Serve</code> function that uses <code>Langchain</code> library to run two nested prompts.</li> <li>Observe <code>main.py</code>: it uses Flask framework to create API route that calls into <code>Ray.Serve</code> endpoint</li> <li>Containerize and deploy the backend image to the registry. Do these steps from <code>backend</code> directory:     <code>bash     PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;     # configure GCR     gcloud auth configure-docker     # build the image     docker build -t hf-lc-ray:latest .     # tag the image for GCR     docker tag hf-lc-ray:latest gcr.io/${PROJECT_ID}/hf-lc-ray:latest     # push the image to GCR     docker push gcr.io/${PROJECT_ID}/hf-lc-ray:latest</code></li> <li>Deploy backend to the cluster. Open <code>src/backend/deploy.yaml</code> and change <code>PROJECT_ID</code> to your project (you can also use <code>sed</code>:  <code>sed -i \"s/YOUR_PROJECT/${PROJECT_ID}/\" src/backend/deploy.yaml</code> ). Then run     <code>bash     kubectl apply -f deploy.yaml -n &lt;YOUR_NAMESPACE&gt;</code></li> <li>Find backend IP on the services page: <code>hf-lc-ray-service</code>:  <code>kubectl get service hf-lc-ray-service -n &lt;namespace name&gt; -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code></li> <li>To test that backend works you can run:     <code>ENDPOINT='http://&lt;IP&gt;/run'     curl -XPOST \"${ENDPOINT}?text=football\"</code>     You will get a response similar to:     <code>[\"a football player is a player who plays for a team\",\"Un joueur de football est un player qui joue pour un \\u00e9quipe.\"]</code></li> </ol> </li> <li> <p>Finally, let's deploy React frontend. Note that in a production distributed application, you can use K8s Ingress with routes for <code>backend</code> and <code>frontend</code> to avoid taking dependcy on the IP, this approach is provided for simplicity.</p> <ol> <li>Update the <code>API_ENDPOINT</code> in <code>src/frontend/src/index.tsx</code></li> <li>Containerize and deploy the frontend image to the registry. Do these steps from <code>src/frontend</code> directory:     <code>bash     PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;     # configure GCR     gcloud auth configure-docker     # build the image     docker build -t hf-lc-ray-fe:latest .     # tag the image for GCR     docker tag hf-lc-ray-fe:latest gcr.io/${PROJECT_ID}/hf-lc-ray-fe:latest     # push the image to GCR     docker push gcr.io/${PROJECT_ID}/hf-lc-ray-fe:latest</code></li> <li>Deploy frontend to the cluster. Open <code>src/frontend/deploy.yaml</code> and change <code>PROJECT_ID</code> to your project (you can also use <code>sed</code>:  <code>sed -i \"s/YOUR_PROJECT/${PROJECT_ID}/\" src/frontend/deploy.yaml</code> ). Then run     <code>bash     kubectl apply -f deploy.yaml -n &lt;YOUR_NAMESPACE&gt;</code></li> <li>Find frontend IP on the services page: <code>hf-lc-ray-fe-service</code>: </li> <li>Click to navigate and give it a try! </li> </ol> </li> </ol>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/","title":"Tutorial: Finetuning Gemma 2b on GKE using L4 GPUs","text":"<p>We\u2019ll walk through fine-tuning a Gemma 2b model using GKE using 8 x L4 GPUs. L4 GPUs are suitable for many use cases beyond serving models. We will demonstrate how the L4 GPU is a great option for fine tuning LLMs, at a fraction of the cost of using a higher end GPU.</p> <p>Let\u2019s get started and fine-tune Gemma 2B on the b-mc2/sql-create-context dataset using GKE. Parameter Efficient Fine Tuning (PEFT) and LoRA is used so fine-tuning is posible on GPUs with less GPU memory.</p> <p>As part of this tutorial, you will get to do the following:</p> <ol> <li>Prepare your environment with a GKE cluster in     Autopilot mode.</li> <li>Create a finetune container.</li> <li>Use GPU to finetune the Gemma 2B model and upload the model to huggingface.</li> </ol>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>Create a Hugging Face account, if you don't already have one.</li> <li>Ensure your project has sufficient quota for GPUs. To learn more, see About GPUs and Allocation quotas.</li> <li>To get access to the Gemma models for deployment to GKE, you must first sign the license consent agreement then generate a Hugging Face access token. Make sure the token has <code>Write</code> permission.</li> </ul>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#creating-the-gke-cluster-with-l4-nodepools","title":"Creating the GKE cluster with L4 nodepools","text":"<p>Let\u2019s start by setting a few environment variables that will be used throughout this post. You should modify these variables to meet your environment and needs. </p> <p>Download the code and files used throughout the tutorial:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4\n</code></pre> <p>Run the following commands to set the env variables and make sure to replace <code>&lt;my-project-id&gt;</code>:</p> <pre><code>gcloud config set project &lt;my-project-id&gt;\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport HF_TOKEN=&lt;YOUR_HF_TOKEN&gt;\nexport CLUSTER_NAME=finetune-gemma\n</code></pre> <p>Note: You might have to rerun the export commands if for some reason you reset your shell and the variables are no longer set. This can happen for example when your Cloud Shell disconnects.</p> <p>Create the GKE cluster by running:</p> <pre><code>gcloud container clusters create-auto ${CLUSTER_NAME} \\\n  --project=${PROJECT_ID} \\\n  --region=${REGION} \\\n  --release-channel=rapid \\\n  --cluster-version=1.29\n</code></pre>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#create-a-kubernetes-secret-for-hugging-face-credentials","title":"Create a Kubernetes secret for Hugging Face credentials","text":"<p>In your shell session, do the following:</p> <ol> <li> <p>Configure <code>kubectl</code> to communicate with your cluster:</p> <p><code>sh   gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}</code></p> </li> <li> <p>Create a Kubernetes Secret that contains the Hugging Face token:</p> <p><code>sh   kubectl create secret generic hf-secret \\     --from-literal=hf_api_token=${HF_TOKEN} \\     --dry-run=client -o yaml | kubectl apply -f -</code></p> </li> </ol>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#containerize-the-code-with-docker-and-cloud-build","title":"Containerize the Code with Docker and Cloud Build","text":"<ol> <li> <p>Create an Artifact Registry Docker Repository</p> <p><code>sh gcloud artifacts repositories create gemma \\     --project=${PROJECT_ID} \\     --repository-format=docker \\     --location=us \\     --description=\"Gemma Repo\"</code></p> </li> <li> <p>Execute the build and create inference container image.</p> <p><code>sh gcloud builds submit .</code></p> </li> </ol>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#run-finetune-job-on-gke","title":"Run Finetune Job on GKE","text":"<ol> <li>Open the <code>finetune.yaml</code> manifest.</li> <li>Edit the <code>image</code> name with the container image built with Cloud Build and <code>NEW_MODEL</code> environment variable value. This <code>NEW_MODEL</code> will be the name of the model you would save as a public model in your Hugging Face account.</li> <li> <p>Run the following command to create the finetune job:</p> <p><code>sh kubectl apply -f finetune.yaml</code></p> </li> <li> <p>Monitor the job by running:</p> <p><code>sh watch kubectl get pods</code></p> </li> <li> <p>You can check the logs of the job by running:</p> <p><code>sh kubectl logs -f -l app=gemma-finetune</code></p> </li> <li> <p>Once the job is completed, you can check the model in Hugging Face.</p> </li> </ol>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#serve-the-finetuned-model-on-gke","title":"Serve the Finetuned Model on GKE","text":"<p>To deploy the finetuned model on GKE you can follow the instructions from Deploy a pre-trained Gemma model on Hugging Face TGI or vLLM. Select the Gemma 2B instruction and change the <code>MODEL_ID</code> to <code>&lt;YOUR_HUGGING_FACE_PROFILE&gt;/gemma-2b-sql-finetuned</code>.</p>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#set-up-port-forwarding","title":"Set up port forwarding","text":"<p>Once the model is deploye, run the following command to set up port forwarding to the model:</p> <pre><code>kubectl port-forward service/llm-service 8000:8000\n</code></pre> <p>The output is similar to the following:</p> <pre><code>Forwarding from 127.0.0.1:8000 -&gt; 8000\n</code></pre>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#interact-with-the-model-using-curl","title":"Interact with the model using curl","text":"<p>Once the model is deployed In a new terminal session, use curl to chat with your model:</p> <p>The following example command is for TGI.</p> <pre><code>USER_PROMPT=\"Question: What is the total number of attendees with age over 30 at kubecon eu? Context: CREATE TABLE attendees (name VARCHAR, age INTEGER, kubecon VARCHAR)\"\n\ncurl -X POST http://localhost:8000/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d @- &lt;&lt;EOF\n{\n    \"inputs\": \"${USER_PROMPT}\",\n    \"parameters\": {\n        \"temperature\": 0.1,\n        \"top_p\": 0.95,\n        \"max_new_tokens\": 25\n    }\n}\nEOF\n</code></pre> <p>The following output shows an example of the model response:</p> <pre><code>{\"generated_text\":\" Answer: SELECT COUNT(age) FROM attendees WHERE age &gt; 30 AND kubecon = 'eu'\\n\"}\n</code></pre>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#clean-up","title":"Clean Up","text":"<p>To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.</p>"},{"location":"genAI-LLM/finetuning-gemma-2b-on-l4/#delete-the-deployed-resources","title":"Delete the deployed resources","text":"<p>To avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:</p> <pre><code>gcloud container clusters delete ${CLUSTER_NAME} \\\n  --region=${REGION}\n</code></pre>"},{"location":"genAI-LLM/finetuning-llama-7b-on-l4/","title":"Tutorial: Finetuning Llama 7b on GKE using L4 GPUs","text":"<p>We\u2019ll walk through fine-tuning a Llama 2 7B model using GKE using 8 x L4 GPUs. L4 GPUs are suitable for many use cases beyond serving models. We will demonstrate how the L4 GPU is a great option for fine tuning LLMs, at a fraction of the cost of using a higher end GPU.</p> <p>Let\u2019s get started and fine-tune Llama 2 7B on the dell-research-harvard/AmericanStories dataset using GKE. Parameter Efficient Fine Tuning (PEFT) and LoRA is used so fine-tuning is posible on GPUs with less GPU memory. </p> <p>As part of this tutorial, you will get to do the following:</p> <ul> <li>Create a GKE cluster with an autoscaling L4 GPU nodepool</li> <li>Run a Kubernetes Job to download Llama 2 7B and fine-tune using L4 GPUs</li> </ul> <p></p>"},{"location":"genAI-LLM/finetuning-llama-7b-on-l4/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>L4 GPUs quota to be able to run additional 8 L4 GPUs</li> <li>Request access to Meta Llama models by submitting the request access form</li> <li>Agree to the Llama 2 terms on the Llama 2 7B HF model in HuggingFace</li> </ul>"},{"location":"genAI-LLM/finetuning-llama-7b-on-l4/#creating-the-gke-cluster-with-l4-nodepools","title":"Creating the GKE cluster with L4 nodepools","text":"<p>Let\u2019s start by setting a few environment variables that will be used throughout this post. You should modify these variables to meet your environment and needs. </p> <p>Download the code and files used throughout the tutorial:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4\n</code></pre> <p>Run the following commands to set the env variables and make sure to replace <code>&lt;my-project-id&gt;</code>:</p> <pre><code>gcloud config set project &lt;my-project-id&gt;\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport BUCKET_NAME=${PROJECT_ID}-llama-l4\nexport SERVICE_ACCOUNT=\"l4-demo@${PROJECT_ID}.iam.gserviceaccount.com\"\n</code></pre> <p>Note: You might have to rerun the export commands if for some reason you reset your shell and the variables are no longer set. This can happen for example when your Cloud Shell disconnects.</p> <p>Create the GKE cluster by running:</p> <pre><code>gcloud container clusters create l4-demo --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=${REGION}-a \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --labels=\"ai-on-gke=l4-demo\" \\\n  --addons GcsFuseCsiDriver\n</code></pre> <p>(Optional) In environments where external IP addresses are not allowed you can add the following arguments to the create GKE cluster command:</p> <pre><code>  --no-enable-master-authorized-networks \\\n  --enable-private-nodes  --master-ipv4-cidr 172.16.0.32/28\n</code></pre> <p>Let\u2019s create a nodepool for our finetuning which will use 8 L4 GPUs per VM. Create the <code>g2-standard-96</code> nodepool by running:</p> <pre><code>gcloud container node-pools create g2-standard-96 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=8,gpu-driver-version=latest \\\n  --machine-type g2-standard-96 \\\n  --ephemeral-storage-local-ssd=count=8 \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=0 --min-nodes=0 --max-nodes=3 \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --node-locations ${REGION}-a,${REGION}-b --region ${REGION}\n</code></pre> <p>Note: The <code>--node-locations</code> flag might have to be adjusted based on which region you choose. Please check which zones the L4 GPUs are available if you change the region to something other than <code>us-central1</code>.</p> <p>The nodepool has been created and is scaled down to 0 nodes. So you are not paying for any GPUs until you start launching Kubernetes Pods that request GPUs.</p>"},{"location":"genAI-LLM/finetuning-llama-7b-on-l4/#run-a-kubernetes-job-to-fine-tune-llama-2-7b","title":"Run a Kubernetes job to fine-tune Llama 2 7B","text":"<p>Finetuning requires a base model and a dataset. For this post, the dell-research-harvard/AmericanStories dataset will be used to fine-tune the Llama 2 7B base model. GCS will be used for storing the base model. GKE with GCSFuse is used to transparently save the fine-tuned model to GCS. This provides a cost efficient way to store and serve the model and only pay for the storage used by the model.</p>"},{"location":"genAI-LLM/finetuning-llama-7b-on-l4/#configuring-gcs-and-required-permissions","title":"Configuring GCS and required permissions","text":"<p>Create a GCS bucket to store our models:</p> <pre><code>gcloud storage buckets create gs://${BUCKET_NAME}\n</code></pre> <p>The model loading Job will write to GCS. So let\u2019s create a Google Service Account that has read and write permissions to the GCS bucket. Then create a Kubernetes Service Account named <code>l4-demo</code> that is able to use the Google Service Account.</p> <p>To do this, first create a new Google Service Account:</p> <pre><code>gcloud iam service-accounts create l4-demo\n</code></pre> <p>Assign the required GCS permissions to the Google Service Account:</p> <pre><code>gcloud storage buckets add-iam-policy-binding gs://${BUCKET_NAME} \\\n  --member=\"serviceAccount:${SERVICE_ACCOUNT}\" --role=roles/storage.admin\n</code></pre> <p>Allow the Kubernetes Service Account <code>l4-demo</code> in the <code>default</code> namespace to use the Google Service Account:</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \\\n  --role roles/iam.workloadIdentityUser \\\n  --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/l4-demo]\"\n</code></pre> <p>Create a new Kubernetes Service Account:</p> <pre><code>kubectl create serviceaccount l4-demo\nkubectl annotate serviceaccount l4-demo iam.gke.io/gcp-service-account=l4-demo@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre> <p>Hugging face requires authentication to download the Llama 2 7B HF model, which means an access token is required to download the model.</p> <p>You can get your access token from huggingface.com &gt; Settings &gt; Access Tokens. Make sure to copy it and then use it in the next step when you create the Kubernetes Secret.</p> <p>Create a Secret to store your HuggingFace token which will be used by the Kubernetes job:</p> <pre><code>kubectl create secret generic l4-demo \\\n  --from-literal=\"HF_TOKEN=&lt;paste-your-own-token&gt;\"\n</code></pre> <p>Let's use Kubernetes Job to download the Llama 2 7B model from HuggingFace. The file <code>download-model.yaml</code> in this repo shows how to do this:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: model-loader\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: loader\n        gke-gcsfuse/volumes: \"true\"\n        gke-gcsfuse/memory-limit: 400Mi\n        gke-gcsfuse/ephemeral-storage-limit: 30Gi\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: loader\n        image: python:3.11\n        command:\n        - /bin/bash\n        - -c\n        - |\n          pip install huggingface_hub\n          mkdir -p /gcs-mount/llama2-7b\n          python3 - &lt;&lt; EOF\n          from huggingface_hub import snapshot_download\n          model_id=\"meta-llama/Llama-2-7b-hf\"\n          snapshot_download(repo_id=model_id, local_dir=\"/gcs-mount/llama2-7b\",\n                            local_dir_use_symlinks=False, revision=\"main\",\n                            ignore_patterns=[\"*.safetensors\", \"model.safetensors.index.json\"])\n          EOF\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: l4-demo\n              key: HF_TOKEN\n        volumeMounts:\n        - name: gcs-fuse-csi-ephemeral\n          mountPath: /gcs-mount\n      serviceAccountName: l4-demo\n      volumes:\n      - name: gcs-fuse-csi-ephemeral\n        csi:\n          driver: gcsfuse.csi.storage.gke.io\n          volumeAttributes:\n            bucketName: ${BUCKET_NAME}\n            mountOptions: \"implicit-dirs\"\n</code></pre> <p>Run the Kubernetes Job to download the the Llama 2 7B model to the bucket created previously:</p> <pre><code>envsubst &lt; download-model.yaml | kubectl apply -f -\n</code></pre> <p>Note: <code>envsubst</code> is used to replace <code>${BUCKET_NAME}</code> inside <code>download-model.yaml</code> with your own bucket.</p> <p>Give it a minute to start running, once up you can watch the logs of the job by running:</p> <pre><code>kubectl logs -f -l job-name=model-loader\n</code></pre> <p>Once the job has finished you can verify the model has been downloaded by running:</p> <pre><code>gcloud storage ls -l gs://$BUCKET_NAME/llama2-7b/\n</code></pre> <p>Let\u2019s write our finetuning job code by using the HuggingFace library for training.</p> <p>The <code>fine-tune.py</code> file in this repo will be used to do the finetuning. Let's take a look what's inside:</p> <pre><code>from pathlib import Path\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\nimport torch\n\n# /gcs-mount will mount the GCS bucket created earlier\nmodel_path = \"/gcs-mount/llama2-7b\"\nfinetuned_model_path = \"/gcs-mount/llama2-7b-american-stories\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n\ndataset = load_dataset(\"dell-research-harvard/AmericanStories\",\n    \"subset_years\",\n    year_list=[\"1809\", \"1810\", \"1811\", \"1812\", \"1813\", \"1814\", \"1815\"]\n)\ndataset = concatenate_datasets(dataset.values())\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\n\ndata = dataset.map(lambda x: tokenizer(\n    x[\"article\"], padding='max_length', truncation=True))\n\nlora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n lora_dropout=0.05,\n bias=\"none\",\n task_type=\"CAUSAL_LM\"\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntraining_args = TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        num_train_epochs=1,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=finetuned_model_path,\n        optim=\"paged_adamw_32bit\",\n)\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=data,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n\ntrainer.train()\n\n# Merge the fine tuned layer with the base model and save it\n# you can remove the line below if you only want to store the LoRA layer\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(finetuned_model_path)\ntokenizer.save_pretrained(finetuned_model_path)\n# Beginning of story in the dataset\nprompt = \"\"\"\nIn the late action between Generals\n\n\nBrown and Riall, it appears our men fought\nwith a courage and perseverance, that would\n\"\"\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.8,\n    max_length=100,\n)\nprint(tokenizer.batch_decode(gen_tokens)[0])\n</code></pre> <p>Let\u2019s review the high level of what we\u2019ve included in <code>fine-tune.py</code>. First we load the base model from GCS using GCS Fuse. Then we load the dataset from HuggingFace. The finetuning uses PEFT which stands for Parameter-Efficient Fine-Tuning. It is a technique that allows you to fine tune an LLM using a smaller number of parameters, which makes it more efficient, flexible and less computationally expensive.</p> <p>The fine-tuned model initially are saved as separate LoRA weights. In the <code>fine-tune.py</code> script, the base model and LoRA weights are merged so the fine-tuned model can be used as a standalone model. This does utilize more storage than needed, but in return you get better compatibility with different libraries for serving.</p> <p>Now we need to run the <code>fine-tune.py`` script inside a container that has all the depdencies. The container image at</code>us-docker.pkg.dev/google-samples/containers/gke/llama-7b-fine-tune-example<code>includes the</code>fine-tune.py<code>script and all required depencies. Alternatively, you can build and publish the image yourself by using the</code>Dockerfile` in this repo.</p> <p>Verify your environment variables are still set correctly:</p> <pre><code>echo \"Bucket: $BUCKET_NAME\"\n</code></pre> <p>Let's use a Kubernetes Job to fine-tune the model. The file <code>fine-tune.yaml</code> in this repo already has the following content:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: finetune-job\n  namespace: default\nspec:\n  backoffLimit: 2\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: finetuner\n        gke-gcsfuse/volumes: \"true\"\n        gke-gcsfuse/memory-limit: 400Mi\n        gke-gcsfuse/ephemeral-storage-limit: 30Gi\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: finetuner\n        image: us-docker.pkg.dev/google-samples/containers/gke/llama-7b-fine-tune-example\n        resources:\n          limits:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: gcs-fuse-csi-ephemeral\n          mountPath: /gcs-mount\n      serviceAccountName: l4-demo\n      volumes:\n      - name: gcs-fuse-csi-ephemeral\n        csi:\n          driver: gcsfuse.csi.storage.gke.io\n          volumeAttributes:\n            bucketName: $BUCKET_NAME\n            mountOptions: \"implicit-dirs\"\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-l4\n      restartPolicy: OnFailure\n</code></pre> <p>Run the fine-tuning Job:</p> <pre><code>envsubst &lt; fine-tune.yaml | kubectl apply -f -\n</code></pre> <p>Verify that the file Job was created and that <code>$IMAGE</code> and <code>$BUCKET_NAME</code> got replaced with the correct values. A Pod should have been created, which you can verify by running:</p> <pre><code>kubectl describe pod -l job-name=finetune-job\n</code></pre> <p>You should see a <code>pod triggered scale-up</code> message under Events after about 30 seconds. Then it will take another 2 minutes for a new GKE node with 8 x L4 GPUs to spin up. Once the Pod gets into running state you can watch the logs of the training:</p> <pre><code>kubectl logs -f -l job-name=finetune-job\n</code></pre> <p>You can watch the training steps and observe the loss go down over time. The training took 22 minutes and 10 seconds for me when I ran it, your results might differ. </p> <p>Once Job completes, you should see a fine-tuned model in your GCS bucket under the <code>llama2-7b-american-stories</code> path. Verify by running:</p> <pre><code>gcloud storage ls -l gs://$BUCKET_NAME/llama2-7b-american-stories\n</code></pre> <p>Congratulations! You have now successfully fine tuned a Llama 2 7B model on old American Stories from 1809 to 1815. Stay tuned for a follow up blog post on how to serve a HuggingFace model from GCS using GKE and GCSfuse. In the meantime you can take a look at the Basaran project for serving HuggingFace models interactively with a Web UI.</p>"},{"location":"genAI-LLM/serving-llama2-70b-on-l4-gpus/","title":"Tutorial: Serving Llama 2 70b on GKE L4 GPUs","text":"<p>Learn how to serve Llama 2 70b chat model on GKE using just 2 x L4 GPUs. For this post the text-generation-inference project is used for serving.</p>"},{"location":"genAI-LLM/serving-llama2-70b-on-l4-gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>L4 GPUs quota to be able to run additional 2 L4 GPUs</li> <li>Request access to Meta Llama models by submitting the request access form</li> <li>Agree to the Llama 2 terms on the Llama 2 70B Chat HF model in HuggingFace</li> </ul> <p>Choose your region and set your project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get project)\n</code></pre> <p>Create a GKE cluster:</p> <pre><code>gcloud container clusters create l4-demo --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=$REGION-a \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --addons GcsFuseCsiDriver   \\\n  --no-enable-master-authorized-networks \\\n  --machine-type n2d-standard-4 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 5 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --enable-ip-alias\n</code></pre> <p>Create a nodepool where each VM has 2 x L4 GPU:</p> <pre><code>gcloud container node-pools create g2-standard-24 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\n  --machine-type g2-standard-24 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=0 --min-nodes=0 --max-nodes=3 \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --node-locations $REGION-a,$REGION-b --region $REGION --spot\n</code></pre> <p>Hugging Face requires authentication to download the Llama-2-70b-chat-hf model, which means an access token is required to download the model.</p> <p>You can get your access token from huggingface.com &gt; Settings &gt; Access Tokens. Afterwards, set your HuggingFace token as an environment variable:</p> <pre><code>export HF_TOKEN=&lt;paste-your-own-token&gt;\n</code></pre> <p>Create a Secret to store your HuggingFace token which will be used by the K8s job:</p> <pre><code>kubectl create secret generic l4-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Create a file named <code>text-generation-interface.yaml</code> with the following content:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llama-2-70b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llama-2-70b\n  template:\n    metadata:\n      labels:\n        app: llama-2-70b\n    spec:\n      containers:\n      - name: llama-2-70b\n        image: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        env:\n        - name: MODEL_ID\n          value: meta-llama/Llama-2-70b-chat-hf\n        - name: NUM_SHARD\n          value: \"2\"\n        - name: PORT \n          value: \"8080\"\n        - name: QUANTIZE\n          value: bitsandbytes-nf4\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: l4-demo\n              key: HF_TOKEN\n        volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n          - mountPath: /data\n            name: data\n      volumes:\n         - name: dshm\n           emptyDir:\n              medium: Memory\n         - name: data\n           hostPath:\n            path: /mnt/stateful_partition/kube-ephemeral-ssd/llama-data\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-l4\n</code></pre> <p>Create the deployment for serving:</p> <pre><code>kubectl apply -f text-generation-interface.yaml\n</code></pre> <p>Inside the YAML file the following settings are used: - <code>NUM_SHARD</code>, this has to be set to 2 because 2 x NVIDIA L4 GPUs are used. In our testing without setting this value it will only use a single GPU. - <code>QUANTIZE</code> is set to <code>nf4</code> which means that the model is loaded in 4 bit instead of 32 bits. This allows us to reduce the amount of GPU memory needed and improves the inference speed, however it can also decrease the model accuracy. If you change this you might need additional GPUs</p> <p>Visit the text-generation-inference docs for more details about these settings.</p> <p>How do you know how many GPUs you need? That depends on the value of <code>QUANTIZE</code>, in our case it is set to <code>bitsandbytes-nf4</code>, which means that the model will be loaded in 4 bits. So a 70 billion parameter model would require a minimum of 70 billion * 4 bits = 35 GB of GPU memory, lets say there is 5GB of overhead, which takes the minimum to 40GB. The L4 GPU has 24GB of GPU memory, so a single L4 GPU wouldn't have enough memory, however 2 x 24 = 48GB GPU memory, so using 2 x L4 GPU is sufficient to run Llama 2 70B on L4 GPUs.</p> <p>Check the logs and make sure there are no errors:</p> <pre><code>kubectl logs -l app=llama-2-70b\n</code></pre> <p>It's time to test it out by sending it some prompts. Setup port forwarding to the inferencing server:</p> <pre><code>kubectl port-forward deployment/llama-2-70b 8080:8080\n</code></pre> <p>Now you can chat with your model through a simple curl:</p> <pre><code>curl 127.0.0.1:8080/generate -X POST \\\n    -H 'Content-Type: application/json' \\\n    --data-binary @- &lt;&lt;EOF\n{\n    \"inputs\": \"[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n&lt;&lt;/SYS&gt;&gt;\\nHow to deploy a container on K8s?[/INST]\",\n    \"parameters\": {\"max_new_tokens\": 400}\n}\nEOF\n</code></pre> <p>There are also API docs available at http://localhost:8080/docs.</p>"},{"location":"gpu-examples/a100-jax/","title":"JAX 'Hello World' on GKE + A100-80GB","text":"<p>This tutorial shows how to run a simple JAX program using NVIDIA GPUs A100-80GB on a GKE cluster</p> <p>Visit https://cloud.google.com/blog/products/containers-kubernetes/machine-learning-with-jax-on-kubernetes-with-nvidia-gpus to follow the tutorial</p>"},{"location":"gpu-examples/online-serving-single-gpu/","title":"Serve a model with a GPU on GKE Autopilot","text":"<p>Please follow the How-to at TO DO: add link to how-to</p>"},{"location":"gpu-examples/training-single-gpu/","title":"Train a model with GPUs on GKE Standard mode","text":"<p>Please follow the Quick Start at https://cloud.google.com/kubernetes-engine/docs/quickstarts/train-model-gpus-standard</p>"},{"location":"hf-tgi/","title":"Index","text":"<ol> <li>Set env vars</li> </ol> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get project)\n</code></pre> <ol> <li>Create cluster</li> </ol> <pre><code>gcloud container clusters create l4-demo --location ${REGION}   \\\n--workload-pool ${PROJECT_ID}.svc.id.goog   --enable-image-streaming \\\n--node-locations=$REGION-a --addons GcsFuseCsiDriver  \\\n --machine-type n2d-standard-4  \\\n --num-nodes 1 --min-nodes 1 --max-nodes 5   \\\n--ephemeral-storage-local-ssd=count=2 --enable-ip-alias\n</code></pre> <pre><code>kubectl config set-cluster l4-demo\n</code></pre> <ol> <li>Create node pool</li> </ol> <pre><code>gcloud container node-pools create g2-standard-24 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\n  --machine-type g2-standard-24 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n --enable-image-streaming \\\n --num-nodes=1 --min-nodes=1 --max-nodes=2 \\\n --node-locations $REGION-a,$REGION-b --region $REGION\n ```\n4. Set the project_id in workloads.tfvars and create the application: `terrafrom apply -var-file=workloads.tfvars` \n5. Make sure app started ok: `kubectl logs -l app=mistral-7b-instruct`\n6. Set up port forward\n</code></pre> <p>kubectl port-forward deployment/mistral-7b-instruct 8080:8080 &amp;</p> <pre><code>7. Try a few prompts:\n</code></pre> <p>export USER_PROMPT=\"How to deploy a container on K8s?\"</p> <pre><code></code></pre> <p>curl 127.0.0.1:8080/generate -X POST \\     -H 'Content-Type: application/json' \\     --data-binary @- &lt;&lt;EOF {     \"inputs\": \"[INST] &lt;&gt;\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n&lt;&gt;\\n$USER_PROMPT[/INST]\",     \"parameters\": {\"max_new_tokens\": 400} } EOF</p> <pre><code>8. Look at `/metrics` endpoint of the service. Go to cloud monitoring and search for one of those metrics. For example, `tgi_request_count` or `tgi_batch_inference_count`. Those metrics should show up if you search for them in PromQL. \n\n9. Clean up the cluster\n</code></pre> <p>gcloud container clusters delete l4-demo --location ${REGION}  ```</p>"},{"location":"inference-servers/checkpoints/","title":"Checkpoint conversion","text":"<p>The <code>checkpoint_entrypoint.sh</code> script overviews how to convert your inference checkpoint for various model servers.</p> <p>Build the checkpoint conversion Dockerfile</p> <pre><code>docker build -t inference-checkpoint .\ndocker tag inference-checkpoint gcr.io/${PROJECT_ID}/inference-checkpoint:latest\ndocker push gcr.io/${PROJECT_ID}/inference-checkpoint:latest\n</code></pre> <p>Now you can use it in a Kubernetes job and pass the following arguments</p>"},{"location":"inference-servers/checkpoints/#jetstream-maxtext","title":"Jetstream + MaxText","text":"<pre><code>- -s=INFERENCE_SERVER\n- -b=BUCKET_NAME\n- -m=MODEL_PATH\n- -v=VERSION (Optional)\n</code></pre>"},{"location":"inference-servers/checkpoints/#jetstream-pytorchxla","title":"Jetstream + Pytorch/XLA","text":"<pre><code>- -s=INFERENCE_SERVER\n- -m=MODEL_PATH\n- -n=MODEL_NAME\n- -q=QUANTIZE_WEIGHTS (Optional) (default=False)\n- -t=QUANTIZE_TYPE (Optional) (default=int8_per_channel)\n- -v=VERSION (Optional) (default=jetstream-v0.2.3)\n- -i=INPUT_DIRECTORY (Optional)\n- -o=OUTPUT_DIRECTORY\n- -h=HUGGINGFACE (Optional) (default=False)\n</code></pre>"},{"location":"inference-servers/checkpoints/#argument-descriptions","title":"Argument descriptions:","text":"<pre><code>b) BUCKET_NAME: (str) GSBucket, without gs://\ns) INFERENCE_SERVER: (str) Inference server, ex. jetstream-maxtext, jetstream-pytorch\nm) MODEL_PATH: (str) Model path, varies depending on inference server and location of base checkpoint\nn) MODEL_NAME: (str) Model name, ex. llama-2, llama-3, gemma\nh) HUGGINGFACE: (bool) Checkpoint is from HuggingFace.\nq) QUANTIZE_WEIGHTS: (str) Whether to quantize weights\nt) QUANTIZE_TYPE: (str) Quantization type, QUANTIZE_WEIGHTS must be set to true. Availabe quantize type: {\"int8\", \"int4\"} x {\"per_channel\", \"blockwise\"},\nv) VERSION: (str) Version of inference server to override, ex. jetstream-v0.2.2, jetstream-v0.2.3\ni) INPUT_DIRECTORY: (str) Input checkpoint directory, likely a GSBucket path\no) OUTPUT_DIRECTORY: (str) Output checkpoint directory, likely a GSBucket path\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/","title":"Serve a LLM using a single-host TPU on GKE with JetStream and MaxText","text":""},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#background","title":"Background","text":"<p>This tutorial shows you how to serve a large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with JetStream and MaxText. </p>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#setup","title":"Setup","text":""},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#set-default-environment-variables","title":"Set default environment variables","text":"<pre><code>gcloud config set project [PROJECT_ID]\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=[COMPUTE_REGION]\nexport ZONE=[ZONE]\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#create-gke-cluster-and-node-pool","title":"Create GKE cluster and node pool","text":"<pre><code># Create zonal cluster with 2 CPU nodes\ngcloud container clusters create jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --project=${PROJECT_ID} \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --release-channel=rapid \\\n    --num-nodes=2\n\n# Create one v5e TPU pool with topology 2x4 (1 TPU node with 8 chips)\ngcloud container node-pools create tpu \\\n    --cluster=jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --num-nodes=2 \\\n    --machine-type=ct5lp-hightpu-8t \\\n    --project=${PROJECT_ID}\n</code></pre> <p>You have created the following resources:</p> <ul> <li>Standard cluster with 2 CPU nodes.</li> <li>One v5e TPU node pool with 2 nodes, each with 8 chips.</li> </ul>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#configure-applications-to-use-workload-identity","title":"Configure Applications to use Workload Identity","text":"<p>Prerequisite: make sure you have the following roles</p> <pre><code>roles/container.admin\nroles/iam.serviceAccountAdmin\n</code></pre> <p>Follow these steps to configure the IAM and Kubernetes service account:</p> <pre><code># Get credentials for your cluster\n$ gcloud container clusters get-credentials jetstream-maxtext \\\n    --zone=${ZONE}\n\n# Create an IAM service account.\n$ gcloud iam service-accounts create jetstream-iam-sa\n\n# Ensure the IAM service account has necessary roles. Here we add roles/storage.objectUser for gcs bucket access.\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.objectUser\n\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.insightsCollectorService\n\n# Allow the Kubernetes default service account to impersonate the IAM service account\n$ gcloud iam service-accounts add-iam-policy-binding jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n\n# Annotate the Kubernetes service account with the email address of the IAM service account.\n$ kubectl annotate serviceaccount default \\\n    iam.gke.io/gcp-service-account=jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#create-a-cloud-storage-bucket-to-store-the-gemma-7b-model-checkpoint","title":"Create a Cloud Storage bucket to store the Gemma-7b model checkpoint","text":"<pre><code>gcloud storage buckets create $BUCKET_NAME\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#get-access-to-the-model","title":"Get access to the model","text":"<p>Access the model consent page and request access with your Kaggle Account. Accept the Terms and Conditions. </p> <p>Obtain a Kaggle API token by going to your Kaggle settings and under the <code>API</code> section, click <code>Create New Token</code>. A <code>kaggle.json</code> file will be downloaded.</p> <p>Create a Secret to store the Kaggle credentials</p> <pre><code>kubectl create secret generic kaggle-secret \\\n    --from-file=kaggle.json\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#convert-the-gemma-7b-checkpoint","title":"Convert the Gemma-7b checkpoint","text":"<p>To convert the Gemma-7b checkpoint, we have created a job <code>checkpoint-job.yaml</code> that does the following: 1. Download the base orbax checkpoint from kaggle 2. Upload the checkpoint to a Cloud Storage bucket 3. Convert the checkpoint to a MaxText compatible checkpoint 4. Unscan the checkpoint to be used for inference</p> <p>In the manifest, ensure the value of the BUCKET_NAME environment variable is the name of the Cloud Storage bucket you created above. Do not include the <code>gs://</code> prefix.</p> <p>Apply the manifest:</p> <pre><code>kubectl apply -f checkpoint-job.yaml\n</code></pre> <p>Observe the logs:</p> <pre><code>kubectl logs -f jobs/data-loader-7b\n</code></pre> <p>You should see the following output once the job has completed. This will take around 10 minutes:</p> <pre><code>Successfully generated decode checkpoint at: gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items\n+ echo -e '\\nCompleted unscanning checkpoint to gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items'\n\nCompleted unscanning checkpoint to gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#deploy-maxengine-server-and-http-server","title":"Deploy Maxengine Server and HTTP Server","text":"<p>Next, deploy a Maxengine server hosting the Gemma-7b model. You can use the provided Maxengine server and HTTP server images or build your own. Depending on your needs and constraints you can elect to deploy either via Terraform or via Kubectl.</p>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#deploy-via-kubectl","title":"Deploy via Kubectl","text":"<p>See the Jetstream component README for start to finish instructions on how to deploy jetstream to your cluster, assure the value of the PARAMETERS_PATH is the path where the checkpoint-converter job uploaded the converted checkpoints to, in this case it should be <code>gs://$BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items</code> where $BUCKET_NAME is the same as above.</p> <p>This README also includes instructions for setting up autoscaling. Follow those instructions to install the required components for autoscaling and configuring your HPAs appropriately.</p>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#deploy-via-terraform","title":"Deploy via Terraform","text":"<p>Navigate to the <code>./terraform</code> directory and run <code>terraform init</code>. The deployment requires some inputs, an example <code>sample-terraform.tfvars</code> is provided as a starting point, run <code>cp sample-terraform.tfvars terraform.tfvars</code> and modify the resulting <code>terraform.tfvars</code> as needed. Since we're using gemma-7b the <code>maxengine_deployment_settings.parameters_path</code> terraform variable should be set to the following: <code>gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items</code>. Finally run <code>terraform apply</code> to apply these resources to your cluster.</p> <p>For deploying autoscaling components via terraform, a few more variables to be set, doing so and rerunning the prior step with these set will deploy the components. The following variables should be set:</p> <pre><code>maxengine_deployment_settings = {\n  metrics = {\n    port: &lt;same as above&gt;   # which port will we scrape server metrics from\n    scrape_interval: 5s     # how often do we scrape\n  }\n}\n\nhpa_config = {\n  metrics_adapter = &lt;either 'prometheus-adapter` (recommended) or 'custom-metrics-stackdriver-adapter' &gt;\n  max_replicas\n  min_replicas\n  rules = [{\n    target_query = &lt;see [jetstream-maxtext-module README](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/modules//jetstream-maxtext-deployment/README.md) for a list of valid values&gt;\n    average_value_target\n  }]\n}\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#verify-the-deployment","title":"Verify the deployment","text":"<p>Wait for the containers to finish creating:</p> <pre><code>kubectl get deployment\n\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nmaxengine-server   2/2     2            2           ##s\n</code></pre> <p>Check the Maxengine pod\u2019s logs, and verify the compilation is done. You will see similar logs of the following:</p> <pre><code>kubectl logs deploy/maxengine-server -f -c maxengine-server\n\n2024-03-29 17:09:08,047 - jax._src.dispatch - DEBUG - Finished XLA compilation of jit(initialize) in 0.26236414909362793 sec\n2024-03-29 17:09:08,150 - root - INFO - ---------Generate params 0 loaded.---------\n</code></pre> <p>Check http server logs, this can take a couple minutes:</p> <pre><code>kubectl logs deploy/maxengine-server -f -c jetstream-http\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#send-sample-requests","title":"Send sample requests","text":"<p>Run the following command to set up port forwarding to the http server:</p> <pre><code>kubectl port-forward svc/jetstream-svc 8000:8000\n</code></pre> <p>In a new terminal, send a request to the server:</p> <pre><code>curl --request POST --header \"Content-type: application/json\" -s localhost:8000/generate --data '{\n    \"prompt\": \"What are the top 5 programming languages\",\n    \"max_tokens\": 200\n}'\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>{\n    \"response\": \" in 2021?\\n\\nThe answer to this question is not as simple as it may seem. There are many factors that go into determining the most popular programming languages, and they can change from year to year.\\n\\nIn this blog post, we will discuss the top 5 programming languages in 2021 and why they are so popular.\\n\\n&lt;h2&gt;&lt;strong&gt;1. Python&lt;/strong&gt;&lt;/h2&gt;\\n\\nPython is a high-level programming language that is used for web development, data analysis, and machine learning. It is one of the most popular languages in the world and is used by many companies such as Google, Facebook, and Instagram.\\n\\nPython is easy to learn and has a large community of developers who are always willing to help out.\\n\\n&lt;h2&gt;&lt;strong&gt;2. Java&lt;/strong&gt;&lt;/h2&gt;\\n\\nJava is a general-purpose programming language that is used for web development, mobile development, and game development. It is one of the most popular languages in the\"\n}\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#other-optional-steps","title":"Other optional steps","text":""},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#build-and-upload-maxengine-server-image","title":"Build and upload Maxengine Server image","text":"<p>Build the Maxengine Server from here and upload to your project</p> <pre><code>docker build -t maxengine-server .\ndocker tag maxengine-server gcr.io/${PROJECT_ID}/jetstream/maxtext/maxengine-server:latest\ndocker push gcr.io/${PROJECT_ID}/jetstream/maxtext/maxengine-server:latest\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#build-and-upload-http-server-image","title":"Build and upload HTTP Server image","text":"<p>Build the HTTP Server Dockerfile from here and upload to your project</p> <pre><code>docker build -t jetstream-http .\ndocker tag jetstream-http gcr.io/${PROJECT_ID}/jetstream/maxtext/jetstream-http:latest\ndocker push gcr.io/${PROJECT_ID}/jetstream/maxtext/jetstream-http:latest\n</code></pre>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#interact-with-the-maxengine-server-directly-using-grpc","title":"Interact with the Maxengine server directly using gRPC","text":"<p>The Jetstream HTTP server is great for initial testing and validating end-to-end requests and responses. If you would like to interact directly with the Maxengine server directly for use cases such as benchmarking, you can do so by following the Jetstream benchmarking setup and applying the <code>deployment.yaml</code> manifest file and interacting with the Jetstream gRPC server at port 9000.</p> <pre><code>kubectl apply -f kubectl/deployment.yaml\n\nkubectl port-forward svc/jetstream-svc 9000:9000\n</code></pre> <p>To run benchmarking, pass in the flag <code>--server 127.0.0.1</code> when running the benchmarking script.</p>"},{"location":"inference-servers/jetstream/maxtext/single-host-inference/#observe-custom-metrics","title":"Observe custom metrics","text":"<p>This step assumes you specified a metrics port to your jetstream deployment via <code>prometheus_port</code>. If you would like to probe the metrics manually, <code>cURL</code> your maxengine-server container on the metrics port you set and you should see something similar to the following:</p> <pre><code># HELP jetstream_prefill_backlog_size Size of prefill queue\n# TYPE jetstream_prefill_backlog_size gauge\njetstream_prefill_backlog_size{id=\"SOME-HOSTNAME-HERE&gt;\"} 0.0\n# HELP jetstream_slots_used_percentage The percentage of decode slots currently being used\n# TYPE jetstream_slots_used_percentage gauge\njetstream_slots_used_percentage{id=\"&lt;SOME-HOSTNAME-HERE&gt;\",idx=\"0\"} 0.04166666666666663\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/","title":"Serve a LLM using a single-host TPU on GKE with JetStream and PyTorch/XLA","text":""},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#background","title":"Background","text":"<p>This tutorial shows you how to serve a large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with JetStream and Jetstream-Pytorch.</p>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#set-default-environment-variables","title":"Set default environment variables","text":"<pre><code>gcloud config set project [PROJECT_ID]\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=[COMPUTE_REGION]\nexport ZONE=[ZONE]\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#create-gke-cluster-and-node-pool","title":"Create GKE cluster and node pool","text":"<pre><code># Create zonal cluster with 2 CPU nodes\ngcloud container clusters create jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --project=${PROJECT_ID} \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --release-channel=rapid \\\n    --addons GcsFuseCsiDriver\n    --num-nodes=2\n# Create one v5e TPU pool with topology 2x4 (1 TPU node with 8 chips)\ngcloud container node-pools create tpu \\\n    --cluster=jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --num-nodes=2 \\\n    --machine-type=ct5lp-hightpu-8t \\\n    --project=${PROJECT_ID}\n</code></pre> <p>You have created the following resources:</p> <ul> <li>Standard cluster with 2 CPU nodes.</li> <li>One v5e TPU node pool with 2 nodes, each with 8 chips.</li> </ul>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#configure-applications-to-use-workload-identity","title":"Configure Applications to use Workload Identity","text":"<p>Prerequisite: make sure you have the following roles</p> <pre><code>roles/container.admin\nroles/iam.serviceAccountAdmin\n</code></pre> <p>Follow these steps to configure the IAM and Kubernetes service account:</p> <pre><code># Get credentials for your cluster\n$ gcloud container clusters get-credentials jetstream-maxtext \\\n    --zone=${ZONE}\n# Create an IAM service account.\n$ gcloud iam service-accounts create jetstream-iam-sa\n# Ensure the IAM service account has necessary roles. Here we add roles/storage.objectUser for gcs bucket access.\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.objectUser\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.insightsCollectorService\n# Allow the Kubernetes default service account to impersonate the IAM service account\n$ gcloud iam service-accounts add-iam-policy-binding jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n# Annotate the Kubernetes service account with the email address of the IAM service account.\n$ kubectl annotate serviceaccount default \\\n    iam.gke.io/gcp-service-account=jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#create-a-cloud-storage-bucket-to-store-your-model-checkpoint","title":"Create a Cloud Storage bucket to store your model checkpoint","text":"<pre><code>BUCKET_NAME=&lt;your desired gsbucket name&gt;\ngcloud storage buckets create $BUCKET_NAME\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#checkpoint-conversion","title":"Checkpoint conversion","text":""},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#option-1-download-weights-from-github","title":"[Option #1] Download weights from GitHub","text":"<p>Follow the instructions here to download the llama-2-7b weights: https://github.com/meta-llama/llama#download </p> <pre><code>ls llama\n\nllama-2-7b tokenizer.model ..\n</code></pre> <p>Upload your weights and tokenizer to your GSBucket</p> <pre><code>gcloud storage cp -r llama-2-7b/* gs://BUCKET_NAME/llama-2-7b/base/\ngcloud storage cp tokenizer.model gs://BUCKET_NAME/llama-2-7b/base/\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#option-2-download-weights-from-huggingface","title":"[Option #2] Download weights from HuggingFace","text":"<p>Accept the terms and conditions from https://huggingface.co/meta-llama/Llama-2-7b-hf.</p> <p>For llama-3-8b: https://huggingface.co/meta-llama/Meta-Llama-3-8B.</p> <p>For gemma-2b: https://huggingface.co/google/gemma-2b-pytorch.</p> <p>Obtain a HuggingFace CLI token by going to your HuggingFace settings and under the <code>Access Tokens</code>, generate a <code>New token</code>. Edit permissions to your access token to have read access to your respective checkpoint repository.</p> <p>Copy your access token and create a Secret to store the HuggingFace token</p> <pre><code>kubectl create secret generic huggingface-secret \\\n  --from-literal=HUGGINGFACE_TOKEN=&lt;access_token&gt;\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#apply-the-checkpoint-conversion-job","title":"Apply the checkpoint conversion job","text":"<p>For the following models, replace the following arguments in <code>checkpoint-job.yaml</code></p>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#llama-2-7b-hf","title":"Llama-2-7b-hf","text":"<pre><code>- -s=jetstream-pytorch\n- -m=meta-llama/Llama-2-7b-hf\n- -o=gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/\n- -n=llama-2\n- -q=False\n- -h=True\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#llama-3-8b","title":"Llama-3-8b","text":"<pre><code>- -s=jetstream-pytorch\n- -m=meta-llama/Meta-Llama-3-8B\n- -o=gs://BUCKET_NAME/pytorch/llama-3-8b/final/bf16/\n- -n=llama-3\n- -q=False\n- -h=True\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#gemma-2b","title":"Gemma-2b","text":"<pre><code>- -s=jetstream-pytorch\n- -m=google/gemma-2b-pytorch\n- -o=gs://BUCKET_NAME/pytorch/gemma-2b/final/bf16/\n- -n=gemma\n- -q=False\n- -h=True\n</code></pre> <p>Run the checkpoint conversion job. This will use the checkpoint conversion script from Jetstream-pytorch to create a compatible Pytorch checkpoint</p> <p>Please make sure you edit <code>checkpoint-job</code> and replace all occurrences of <code>BUCKET_NAME</code> with the <code>BUCKET_NAME</code> that you have set above.</p> <pre><code>kubectl apply -f checkpoint-job.yaml\n</code></pre> <p>Observe your checkpoint</p> <pre><code>kubectl logs -f jobs/checkpoint-converter\n# This can take several minutes\n...\nCompleted uploading converted checkpoint from local path /pt-ckpt/ to GSBucket gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/\"\n</code></pre> <p>Now your converted checkpoint will be located in <code>gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/</code></p>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#deploy-the-jetstream-pytorch-server","title":"Deploy the Jetstream Pytorch server","text":"<p>The following flags are set in the manifest file</p> <pre><code>--size: Size of model\n--model_name: Name of model (llama-2, llama-3, gemma)\n--batch_size: Batch size\n--max_cache_length: Maximum length of kv cache \n--tokenizer_path: Path to model tokenizer file\n--checkpoint_path: Path to checkpoint\nOptional flags to add\n--quantize_weights (Default False): Checkpoint is quantized\n--quantize_kv_cache (Default False): Quantized kv cache\n</code></pre> <p>For llama3-8b, you can use the following arguments:</p> <pre><code>- --size=8b\n- --model_name=llama-3\n- --batch_size=80\n- --max_cache_length=2048\n- --quantize_weights=False\n- --quantize_kv_cache=False\n- --tokenizer_path=/models/pytorch/llama3-8b/final/bf16/tokenizer.model\n- --checkpoint_path=/models/pytorch/llama3-8b/final/bf16/model.safetensors\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#verify-the-deployment","title":"Verify the deployment","text":"<pre><code>kubectl get deployment\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\njetstream-pytorch-server    2/2       2            2         ##s\n</code></pre> <p>View the HTTP server logs to check that the model has been loaded and compiled. It may take the server a few minutes to complete this operation.</p> <pre><code>kubectl logs deploy/jetstream-pytorch-server -f -c jetstream-http\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre> <p>View the Jetstream Pytorch server logs and verify that the compilation is done.</p> <pre><code>kubectl logs deploy/jetstream-pytorch-server -f -c jetstream-pytorch-server\nStarted jetstream_server....\n2024-04-12 04:33:37,128 - root - INFO - ---------Generate params 0 loaded.---------\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#serve-the-model","title":"Serve the model","text":"<pre><code>kubectl port-forward svc/jetstream-svc 8000:8000\n</code></pre> <p>Interact with the model via curl</p> <pre><code>curl --request POST \\\n--header \"Content-type: application/json\" \\\n-s \\\nlocalhost:8000/generate \\\n--data \\\n'{\n    \"prompt\": \"What are the top 5 programming languages\",\n    \"max_tokens\": 200\n}'\n</code></pre> <p>The initial request can take several seconds to complete due to model warmup. The output is similar to the following:</p> <pre><code>{\n    \"response\": \" for 2019?\\nWhat are the top 5 programming languages for 2019? The top 5 programming languages for 2019 are Python, Java, JavaScript, C, and C++.\\nWhat are the top 5 programming languages for 2019? The top 5 programming languages for 2019 are Python, Java, JavaScript, C, and C++. These languages are used in a variety of industries and are popular among developers.\\nPython is a versatile language that can be used for web development, data analysis, and machine learning. It is easy to learn and has a large community of developers.\\nJava is a popular language for enterprise applications and is used by many large companies. It is also used for mobile development and has a large community of developers.\\nJavaScript is a popular language for web development and is used by many websites. It is also used for mobile development and has a\"\n}\n</code></pre>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#optionals","title":"Optionals","text":""},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#interact-with-the-jetstream-pytorch-server-directly-using-grpc","title":"Interact with the Jetstream Pytorch server directly using gRPC","text":"<p>The Jetstream HTTP server is great for initial testing and validating end-to-end requests and responses. In production use case, it's recommended to interact with the JetStream-Pytorch server directly for better throughput/latency and use the streaming decode feature on the JetStream grpc server.</p> <pre><code>kubectl port-forward svc/jetstream-svc 9000:9000\n</code></pre> <p>Now you can interact with the JetStream grpc server directly via port 9000.</p>"},{"location":"inference-servers/jetstream/pytorch/single-host-inference/#use-a-persistent-disk-to-host-your-checkpoint","title":"Use a Persistent Disk to host your checkpoint","text":"<p>Create a GCE CPU VM to do your checkpoint conversion. </p> <pre><code>gcloud compute instances create jetstream-ckpt-converter \\\n  --zone=us-central1-a \\\n  --machine-type=n2-standard-32 \\\n  --scopes=https://www.googleapis.com/auth/cloud-platform \\\n  --image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20230919 \\\n  --boot-disk-size=128GB \\\n  --boot-disk-type=pd-balanced\n</code></pre> <p>SSH into the VM and install python and git</p> <pre><code>gcloud compute ssh jetstream-ckpt-converter --zone=us-central1-a\nsudo apt update\nsudo apt-get install python3-pip\nsudo apt-get install git-all\n</code></pre> <p>In your CPU VM, follow these instructions to do the following: 1. Clone the jetstream-pytorch repository 2. Run the installation script 3. Download and convert llama-2-7b weights</p> <p>After running weight safetensor convert, you should see the following files in the directory you have saved them to:</p> <pre><code>ls &lt;directory where checkpoint is stored&gt;\nmodel.safetensors  params.json\n</code></pre> <p>Create a persistent disk to store your checkpoint</p> <pre><code>gcloud compute disks create jetstream-pytorch-ckpt --zone=us-west4-a --type pd-balanced\nNAME                    ZONE        SIZE_GB  TYPE         STATUS\npytorch-jetstream-ckpt  us-west4-a  100      pd-balanced  READY\n</code></pre> <p>Attach the disk to your VM</p> <pre><code>gcloud compute instances attach-disk jetstream-ckpt-converter \\\n  --disk jetstream-pytorch-ckpt --project $PROJECT_ID --zone us-west4-a\n</code></pre> <p>Identity your disk, it will be similar to the following but may also have a different name:</p> <pre><code>lsblk\nNAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\n...\nsdc       8:32   0   100G  0 disk \n</code></pre> <p>Format your disk and create a directory in its mount folder</p> <pre><code>sudo mkfs.ext4 /dev/sdc\nmkdir /mnt/jetstream-pytorch-ckpt\nsudo mount /dev/sdc /mnt/jetstream-pytorch-ckpt\n</code></pre> <p>Copy your converted checkpoint folder into /mnt/jetstream-pytorch-ckpt</p> <pre><code>cp &lt;path to converterted checkpoint&gt; /mnt/jetstream-pytorch-ckpt\n</code></pre> <p>Unmount and detach your persistent disk</p> <pre><code>sudo umount /mnt/jetstream-pytorch-ckpt\ngcloud compute instances detach-disk jetstream-ckpt-converter \\\n  --disk jetstream-pytorch-ckpt --project $PROJECT_ID --zone us-west4-a\n</code></pre> <p>Apply your storage and deployment manifest file</p> <pre><code>kubectl apply -f storage.yaml\nkubectl apply -f pd-deployment.yaml\n</code></pre>"},{"location":"inference-servers/maxdiffusion/","title":"High-performance diffusion model inference on GKE and TPU using MaxDiffusion","text":""},{"location":"inference-servers/maxdiffusion/#about-maxdiffusion","title":"About MaxDiffusion","text":"<p>Just as LLMs have revolutionized natural language processing, diffusion models are transforming the field of computer vision. To reduce our customers\u2019 costs of deploying these models, Google created MaxDiffusion: a collection of open-source diffusion-model reference implementations. These implementations are written in JAX and are highly performant, scalable, and customizable \u2013 think MaxText for computer vision. </p> <p>MaxDiffusion provides high-performance implementations of core components of diffusion models such as cross attention, convolutions, and high-throughput image data loading. MaxDiffusion is designed to be highly adaptable and customizable: whether you're a researcher pushing the boundaries of image generation or a developer seeking to integrate cutting-edge gen AI capabilities into your applications, MaxDiffusion provides the foundation you need to succeed.</p>"},{"location":"inference-servers/maxdiffusion/#prerequisites","title":"Prerequisites","text":"<p>Access to a Google Cloud project with the TPU v5e available and enough quota in the region you select. In the walk-through, we only choose lower end TPU v5e, single host with 1x1 chips</p> <p>A computer terminal with kubectl and the Google Cloud SDK installed. From the GCP project console you\u2019ll be working with, you may want to use the included Cloud Shell as it already has the required tools installed.</p> <p>MaxDiffsusion will access Huggingface to download Stable Diffusion XL model(stabilityai/stable-diffusion-xl-base-1.0), while no huggingface access token required for access</p>"},{"location":"inference-servers/maxdiffusion/#setup-project-environments","title":"Setup project environments","text":"<p>Set the project and region that have availability for TPU v5e( alternatively, you can choose other regions with different TPU v5e accelerator type available):</p> <pre><code>export PROJECT_ID=&lt;your-project-id&gt;\nexport REGION=us-east1\nexport ZONE_1=${REGION}-c # You may want to change the zone letter based on the region you selected above\n\nexport CLUSTER_NAME=tpu-cluster\ngcloud config set project \"$PROJECT_ID\"\ngcloud config set compute/region \"$REGION\"\ngcloud config set compute/zone \"$ZONE_1\"\n</code></pre> <p>Then, enable the required APIs to create a GK cluster:</p> <pre><code>gcloud services enable compute.googleapis.com container.googleapis.com\n</code></pre> <p>Also, you may go ahead download the source code repo for this exercise, :</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git\ncd tutorials-and-examples/tpu-examples/maxdiffusion\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#create-gke-cluster-and-nodepools","title":"Create GKE Cluster and Nodepools","text":""},{"location":"inference-servers/maxdiffusion/#gke-cluster","title":"GKE Cluster","text":"<p>Now, create a GKE cluster with a minimal default node pool, as you will be adding a node pool with TPU v5e later on:</p> <pre><code>gcloud container clusters create $CLUSTER_NAME --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=$REGION-b \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --addons GcsFuseCsiDriver   \\\n  --no-enable-master-authorized-networks \\\n  --machine-type n2d-standard-4 \\\n  --cluster-version 1.29 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 3 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --scopes=\"gke-default,storage-rw\"\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#nodepool","title":"Nodepool","text":"<p>Create an additional Spot node pool (we use spot to save costs, you can remove spot option depends on different use case) with TPU accelerator:</p> <pre><code>cloud container node-pools create $CLUSTER_NAME-tpu \\\n--location=$REGION --cluster=$CLUSTER_NAME --node-locations=$ZONE_1 \\\n--machine-type=ct5lp-hightpu-1t --num-nodes=0 --spot --node-version=1.29 \\\n--ephemeral-storage-local-ssd=count=0 --enable-image-streaming \\\n--shielded-secure-boot --shielded-integrity-monitoring \\\n--enable-autoscaling --total-min-nodes 0 --total-max-nodes 2 --location-policy=ANY\n</code></pre> <p>Note how easy enabling TPU in GKE nodepool with proper TPU machine type. Please refer to the following page, for details on TPU v5e machine type and configuration sections.</p> <p>After a few minutes, check that the node pool was created correctly:</p> <pre><code>gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION --project $PROJECT_ID\ngcloud container node-pools list --region $REGION --cluster $CLUSTER_NAME\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#explaination-on-maxdiffusion-inference-server-sample-code","title":"Explaination on MaxDiffusion inference server sample code","text":"<p>The sample Stable Diffusion XL inference code template from MaxDiffusion repo is updated with FastAPI and uvicorn libraries to fit for API requests.</p> <p>The updated Stable Diffusion XL Inference Code sample provided here for reference.</p>"},{"location":"inference-servers/maxdiffusion/#add-fastapi-and-uvicorn-libraries","title":"Add FastAPI and Uvicorn libraries","text":""},{"location":"inference-servers/maxdiffusion/#add-logging-health-check","title":"Add logging, health check","text":""},{"location":"inference-servers/maxdiffusion/#expose-generate-as-post-methods-for-rest-api-requests","title":"Expose /generate as Post methods for REST API requests","text":""},{"location":"inference-servers/maxdiffusion/#huggingface-stable-diffusion-xl-model-stabilityaistable-diffusion-xl-base-10","title":"HuggingFace Stable Diffusion XL Model: stabilityai/stable-diffusion-xl-base-1.0","text":"<p>To make the Stable Diffusion XL inference more efficient, we compile the pipeline._generate function, and pass all parameters to the function and tell JAX which are static arguments,</p> <p>default_seed = 33</p> <p>default_guidance_scale = 5.0</p> <p>default_num_steps = 40</p> <p>width = 1024</p> <p>height = 1024</p> <p>The following main exposed SDXL inference method,</p> <pre><code>@app.post(\"/generate\")\nasync def generate(request: Request):\n    LOG.info(\"start generate image\")\n    data = await request.json()\n    prompt = data[\"prompt\"]\n    LOG.info(prompt)\n    prompt_ids, neg_prompt_ids = tokenize_prompt(prompt, default_neg_prompt)\n    prompt_ids, neg_prompt_ids, rng = replicate_all(prompt_ids, neg_prompt_ids, default_seed)\n    g = jnp.array([default_guidance_scale] * prompt_ids.shape[0], dtype=jnp.float32)\n    g = g[:, None]\n    LOG.info(\"call p_generate\")\n    images = p_generate(prompt_ids, p_params, rng, g, None, neg_prompt_ids)\n\n    # convert the images to PIL\n    images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\n    images=pipeline.numpy_to_pil(np.array(images))\n    buffer = io.BytesIO()\n    LOG.info(\"Save image\")\n    for i, image in enumerate(images):\n        if i==0:\n          image.save(buffer, format=\"PNG\")\n    #await images[0].save(buffer, format=\"PNG\")\n\n    # Return the image as a response\n    return Response(content=buffer.getvalue(), media_type=\"image/png\")\n\nif __name__ == \"__main__\":\n   uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False, log_level=\"debug\")\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#build-stable-diffusion-xl-inference-container-image","title":"Build Stable Diffusion XL Inference Container Image","text":"<p>Next, let\u2019s build the Inference server container image with cloud build.</p> <p>Sample Docker file and cloudbuild.yaml already under directory buid/server/ downloaded repo,</p> <p>buid/server/Dockerfile:</p> <pre><code>FROM python:3.11-slim\nWORKDIR /app\nRUN apt-get -y update\nRUN apt-get -y install git\nCOPY requirements.txt ./\nRUN python -m pip install --upgrade pip\nRUN pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\nRUN pip install -r requirements.txt\nRUN pip install git+https://github.com/google/maxdiffusion.git\nCOPY main.py ./\nEXPOSE 8000\nENTRYPOINT [\"python\", \"main.py\"]\n</code></pre> <p>Notes: we may not have maxdiffusion pip package to be downloaded yet, thus, RUN pip install git+https://github.com/google/maxdiffusion.git is used to download MaxDiffusion from source directly.</p> <p>cloudbuild.yaml:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: [ 'build', '-t', 'us-east1-docker.pkg.dev/$PROJECT_ID/gke-llm/max-diffusion:latest', '.' ]\nimages:\n- 'us-east1-docker.pkg.dev/$PROJECT_ID/gke-llm/max-diffusion:latest'\n</code></pre> <p>Note: replace destination of container image as your own environment</p> <p>Run the following commands to kick of container image builds:</p> <pre><code>cd build/server\ngcloud builds submit .\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#deploy-stable-diffusion-xl-inference-server-in-gke","title":"Deploy Stable Diffusion XL Inference Server in GKE","text":"<p>In the downloaded code repo root directory, you can check the following kubernetes deployment resource files,</p> <p>serve_sdxl_v5e.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stable-diffusion-deployment\nspec:\n  selector:\n    matchLabels:\n      app: max-diffusion-server\n  replicas: 1  # number of nodes in node-pool\n  template:\n    metadata:\n      labels:\n        app: max-diffusion-server\n    spec:\n      nodeSelector:\n        cloud.google.com/gke-tpu-topology: 1x1 #  target topology\n        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\n      volumes:\n       - name: dshm\n         emptyDir:\n              medium: Memory\n      containers:\n      - name: serve-stable-diffusion\n        image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/max-diffusion:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: MODEL_NAME\n          value: 'stable_diffusion'\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            google.com/tpu: 1  # TPU chip request\n          limits:\n            google.com/tpu: 1  # TPU chip request\n        volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: max-diffusion-server\n  labels:\n    app: max-diffusion-server\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8000\n      targetPort: http\n      name: http-max-diffusion-server\n  selector:\n    app: max-diffusion-server\n</code></pre> <p>To be noted, in deployment specs settings related to TPU accelerators which has to match v5e machine types (ct5lp-hightpu-1t has 1x1=1 total TPU chips):</p> <pre><code>nodeSelector:\n        cloud.google.com/gke-tpu-topology: 1x1 #  target topology\n        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\n\nresources:\n          requests:\n            google.com/tpu: 1  # TPU chip request\n          limits:\n            google.com/tpu: 1  # TPU chip request\n</code></pre> <p>We use type: ClusterIP to expose inference service availabe to GKE cluster only. Update this file with proper container image name and location,</p> <p>Run the following command to deploy Stable Diffusion Inference server to GKE:</p> <pre><code>gcloud container clusters get-credentials $CLUSTER_NAME $REGION\nkubectl apply -f serve_sdxl_v5e.yaml\nkubectl get svc max-diffusion-server. \n</code></pre> <p>It may take 7\u20138 minutes to wait for the spot nodepool provisioning, model loading and initial pipeline compilation. Note the service IP need to be referenced by later section</p> <p>You may use the following command to validate Stable Diffusion Inference Server setup properly,</p> <pre><code>SERVER_URL=XXXX\nkubectl run -it busybox --image radial/busyboxplus:curl\n\n\ncurl SERVER_URL:8000\n</code></pre> <p>If you have issues such as connection refused in curl command validations, you may use the following command instead to create a service through kubectl command:</p> <pre><code>kubectl expose deployment max-diffusion-deployment max-diffusion-service --port 8000 --protocol tcp --target-port 8000\n\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#deploy-webapp","title":"Deploy WebApp","text":"<p>A simple client webapp provided under build/webapp directory with following files included:</p> <p>app.py ( main python file), Dockerfile , cloudbuild.yaml , requirements.txt</p> <p>You may update cloudbuild.yaml file with your own container image destination accordingly.</p> <p>Run the following command to build testing webapp container image using cloud build:</p> <pre><code>cd build/webapp\ngcloud builds submit .\n</code></pre> <p>Once the webapp image build completed, you may go ahead deploy frontend webapp to test Stable Diffusion XL inference server.</p> <p>serve_sdxl_client.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: max-diffusion-client\nspec:\n  selector:\n    matchLabels:\n      app: max-diffusion-client\n  template:\n    metadata:\n      labels:\n        app: max-diffusion-client\n    spec:\n      containers:\n      - name: webclient\n        image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/max-diffusion-client:latest\n        env:\n          - name: SERVER_URL\n            value: \"http://CLusterIP:8000\"\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 5000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: max-diffusion-client-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: max-diffusion-client\n  ports:\n  - port: 8080\n    targetPort: 5000\n</code></pre> <p>We use type: LoadBalancer to expose webapp to external public. Update this file with proper container image name and SERVER_URL endpoint location from. Run the following command to deploy frontend webapp:</p> <pre><code>kubectl apply -f serve_sdxl_client.yaml\n</code></pre> <p>Once the webapp deployment completed, you may test text to image capabilities from browser: http://webappServiceIP:8080/</p> <p>Image generated after 3\u20135s( displaying only first image from inference), which is quite performance efficient for Cloud TPU v5e based on Single-host serving for one single v5e chips.</p>"},{"location":"inference-servers/maxdiffusion/#cleanups","title":"Cleanups","text":"<p>Don\u2019t forget to clean up the resources created in this article once you\u2019ve finished experimenting with Stable Diffusion inference on GKE and TPU, as keeping the cluster running for a long time can incur in important costs. To clean up, you just need to delete the GKE cluster:</p> <pre><code>gcloud container clusters delete $CLUSTER_NAME - region $REGION\n</code></pre>"},{"location":"inference-servers/maxdiffusion/#conclusion","title":"Conclusion","text":"<p>With the streamlined process showcased in this post, deploying inference servers for open-source image generation/vision models like Stable Diffusion XL on GKE and TPU has never been simpler or more efficient with MaxDiffusion to serve JAX models directly, without need for download and conversion JAX model to Tensorflow compatible model for Stable Diffusion Inference Serving in GKE and TPU</p> <p>Don\u2019t forget to check out other GKE related resources on AI ML infrastructure offered by Google Cloud and check the resources included in the AI/ML orchestration on GKE documentation.</p> <p>The blog published linked to this repo</p>"},{"location":"kserve/","title":"KServe on GKE Autopilot","text":"<p>KServe is a highly scalable, standards-based platform for model inference on Kubernetes. Installing KServe on GKE Autopilot can be challenging due to the security policies enforced by Autopilot. This tutorial will guide you step by step through the process of installing KServe in a GKE Autopilot cluster.</p> <p>Additionally, this tutorial includes an example of serving Gemma2 with vLLM in KServe, demonstrating how to utilize GPU resources in KServe on Google Kubernetes Engine (GKE).</p>"},{"location":"kserve/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Ensure you have a gcp project with billing enabled and enabled the GKE API. </p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> <li>helm</li> </ol>"},{"location":"kserve/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<ol> <li>Set the default environment variables:</li> </ol> <pre><code>export PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport CLUSTER_NAME=kserve-demo\n</code></pre> <ol> <li>Create a GKE Autopilot cluster:</li> </ol> <pre><code>gcloud container clusters create-auto ${CLUSTER_NAME} \\\n    --location=$REGION \\\n    --project=$PROJECT_ID \\\n    --workload-policies=allow-net-admin\n\n# Get credentials\ngcloud container clusters get-credentials ${CLUSTER_NAME} \\\n--region ${REGION} \\\n--project ${PROJECT_ID}\n</code></pre> <p>If you're using an existing cluster, ensure it is updated to allow net admin permissions. This is necessary for the installation of Istio later on:</p> <pre><code>gcloud container clusters update ${CLUSTER_NAME} \\\n--region=${REGION}\n--project=$PROJECT_ID \\\n--workload-policies=allow-net-admin \n</code></pre>"},{"location":"kserve/#install-kserve","title":"Install KServe","text":"<p>KServe relies on Knative and requires a networking layer. In this tutorial, we will use Istio, the networking layer that integrates best with Knative.</p> <ol> <li>Install Knative</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-core.yaml\n</code></pre> <p>Note:  You will see warnings that Autopilot mutated the CRDs during this tutorial. These warnings are safe to ignore.</p> <ol> <li>Install Istio</li> </ol> <pre><code>helm repo add istio https://istio-release.storage.googleapis.com/charts\nhelm repo update\nkubectl create namespace istio-system\nhelm install istio-base istio/base -n istio-system --set defaultRevision=default\nhelm install istiod istio/istiod -n istio-system --wait\nhelm install istio-ingressgateway istio/gateway -n istio-system\n\n# Verify the installation\nkubectl get deployments -n istio-system\n\n# Example Output\nNAME                   READY   UP-TO-DATE   AVAILABLE   AGE\nistio-ingressgateway   1/1     1            1           17h\nistiod                 1/1     1            1           20h\n</code></pre> <ol> <li>Install Knative-Istio</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.1/net-istio.yaml\n\n# Verify the installation\nkubectl get pods -n knative-serving\n\n# Example Output\nNAME                                    READY   STATUS    RESTARTS      AGE\nactivator-749cf94f87-b7p9n              1/1     Running   0             17m\nautoscaler-5c764b5f7d-m8zvk             1/1     Running   1 (14m ago)   17m\ncontroller-5649f5bbb7-wvlmk             1/1     Running   4 (13m ago)   17m\nnet-istio-controller-7f8dfbddb7-d8cmq   1/1     Running   0             18s\nnet-istio-webhook-54ffc96585-cpgfl      2/2     Running   0             18s\nwebhook-64c67b4fc-smdtl                 1/1     Running   3 (13m ago)   17m\n</code></pre> <ol> <li>Install DNS In this tutorial we use Magic DNS. To configure a real DNS, follow the steps here.</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-default-domain.yaml\n</code></pre> <ol> <li>Install Cert Manager, which is required to provision webhook certs for production grade installation.</li> </ol> <pre><code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.15.3 --set crds.enabled=true --set global.leaderElection.namespace=cert-manager\n</code></pre> <ol> <li>Install Kserve and Kserve cluster runtimes</li> </ol> <pre><code>kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.14.0-rc0/kserve.yaml\n\n# Wait until kserve-controller-manager is ready\nkubectl rollout status deployment kserve-controller-manager -n kserve\n\n# Install cluster runtimes\nkubectl apply -f https://github.com/kserve/kserve/releases/download/v0.14.0-rc0/kserve-cluster-resources.yaml\n\n# View these runtimes\nkubectl get ClusterServingRuntimes -n kserve\n</code></pre> <ol> <li>To request accelerators (GPUs) for your Google Kubernetes Engine (GKE) Autopilot workloads, nodeSelector is used in the manifest. Therefore, we will enable nodeSelector in Knative, which is disabled by default.</li> </ol> <pre><code>kubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-nodeselector\":\"enabled\", \"kubernetes.podspec-tolerations\":\"enabled\"}}'\n\n# restart knative webhook to consume the config, for example\nkubectl get pods -n knative-serving\n# Find the webhook pod and delete it to restart the pod.\nkubeclt delete pod webhook-64c67b4fc-nmzwt -n knative-serving\n</code></pre> <p>After successfully installing KServe, you can now explore various examples such as, first inference service, canary rollout, inference batcher and auto-scaling. In the next step, we'll demonstrate how to deploy Gemma2 using vLLM in KServe with GKE Autopilot.</p>"},{"location":"kserve/#deploy-gemma2-served-with-vllm","title":"Deploy Gemma2 served with vllm.","text":"<ol> <li> <p>Generate a hugging face access token follow these steps. Specify a Name of your choice and a Role of at least Read. </p> </li> <li> <p>Make sure you accepted the term to use gemma2 in hugging face.</p> </li> <li> <p>Create the hugging face token</p> </li> </ol> <pre><code>kubectl create namespace kserve-test\n\n# Specify your hugging face token.\nexport HF_TOKEN = XXX\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n    name: hf-secret\n    namespace: kserve-test\ntype: Opaque\nstringData:\n    hf_api_token: ${HF_TOKEN}\nEOF\n\n</code></pre> <ol> <li>Create the inference service</li> </ol> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-gemma2\n  namespace: kserve-test\nspec:\n  predictor:\n    nodeSelector:\n      cloud.google.com/gke-accelerator: nvidia-l4\n      cloud.google.com/gke-accelerator-count: \"1\"\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n        - --enable_docs_url=True\n        - --model_name=gemma2\n        - --model_id=google/gemma-2-2b\n      env:\n      - name: HF_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: hf-secret\n            key: hf_api_token\n      resources:\n        limits:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\nEOF\n</code></pre> <p>Wait for the service to be ready:</p> <pre><code>kubectl get inferenceservice huggingface-gemma2 -n kserve-test\nkubectl get pods -n kserve-test\n\n# Replace pod_name with the correct pod name.\nkubectl events --for pod/POD_NAME -n kserve-test --watch\n</code></pre>"},{"location":"kserve/#test-the-inference-service","title":"Test the Inference Service","text":"<ol> <li>Find the URL returned in kubectl get inferenceservice</li> </ol> <pre><code>URL=$(kubectl get inferenceservice huggingface-gemma2 -o jsonpath='{.status.url}')\n\n# URL should look like this:\nhttp://huggingface-gemma2.kserve-test.34.121.87.225.sslip.io\n</code></pre> <ol> <li> <p>Open the swagger UI at $URL/docs</p> </li> <li> <p>Play with the openai chat API with the example input below. Click execute and you can see the response.</p> </li> </ol> <pre><code>{\n    \"model\": \"gemma2\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an assistant that speaks like Shakespeare.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a poem about colors\"\n        }\n    ],\n    \"max_tokens\": 30,\n    \"stream\": false\n}\n</code></pre>"},{"location":"kserve/#clean-up","title":"Clean up","text":"<p>Delete the GKE cluster.</p> <pre><code>gcloud container clusters delete ${CLUSTER_NAME} \\\n    --location=$REGION \\\n    --project=$PROJECT_ID \\\n</code></pre>"},{"location":"models-as-oci/","title":"Package and Deploy from Hugging Face to Artifact Registry and GKE","text":"<p>This repository contains a Google Cloud Build configuration for building and pushing Docker images of Hugging Face models to Google Artifact Registry.</p>"},{"location":"models-as-oci/#overview","title":"Overview","text":"<p>This project allows you to download a Hugging Face model and package it as a Docker image. The Docker image can then be pushed to Google Artifact Registry for deployment or distribution. Build time can be significant for large models, it is recommended to not exceed models above 10 billion parameters. For reference 8b model roughly takes 35 minutes to build and push with this cloudbuild config.</p>"},{"location":"models-as-oci/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Google Cloud project with billing enabled.</li> <li>Google Cloud SDK installed and authenticated.</li> <li>Access to Google Cloud Build and Artifact Registry.</li> <li>A Hugging Face account with an access token.</li> </ul>"},{"location":"models-as-oci/#setupcreate-a-secret-for-hugging-face-token","title":"SetupCreate a Secret for Hugging Face Token","text":"<ol> <li>Clone the Repository</li> </ol> <p><code>bash    git clone https://github.com/your-username/your-repo-name.git    cd your-repo-name 2. **Create a Secret for Hugging Face Token**</code>bash     echo \"your_hugging_face_token\" | gcloud secrets create huggingface-token --data-file=-</p>"},{"location":"models-as-oci/#configuration","title":"Configuration","text":""},{"location":"models-as-oci/#substitutions","title":"Substitutions","text":"<p>The following substitutions are defined in the <code>cloudbuild.yaml</code> file, they can be changed by passing <code>--substitutions SUBSTITUTION_NAME=SUBSTITUTION_VALUE</code> to <code>gcloud builds submit</code>:</p> <ul> <li><code>_MODEL_NAME</code>: The name of the Hugging Face model to download (default: <code>huggingfaceh4/zephyr-7b-beta</code>).</li> <li><code>_REGISTRY</code>: The URL for the Docker registry (default: <code>us-docker.pkg.dev</code>).</li> <li><code>_REPO</code>: The name of the Artifact Registry repository (default: <code>cloud-blog-oci-models</code>).</li> <li><code>_IMAGE_NAME</code>: The name of the Docker image to be created (default: <code>zephyr-7b-beta</code>).</li> <li><code>_CLOUD_SECRET_NAME</code>: The name of the secret storing the Hugging Face token (default: <code>huggingface-token</code>).</li> </ul>"},{"location":"models-as-oci/#options","title":"Options","text":"<p>The following options are configured in the <code>cloudbuild.yaml</code> file:</p> <ul> <li><code>diskSizeGb</code>: The size of the disk for the build, specified in gigabytes (default: <code>100</code>). can be changed by passing <code>--disk-size=DISK_SIZE</code> to <code>gcloud builds submit</code></li> <li><code>machineType</code>: The machine type can be set by passing <code>--machine-type=</code> in <code>gcloud builds submit</code></li> </ul>"},{"location":"models-as-oci/#usage","title":"Usage","text":"<p>To trigger the Cloud Build and create the Docker image, run the following command:</p> <pre><code>gcloud builds submit --config cloudbuild.yaml --substitutions _MODEL_NAME=\"your_model_name\",_IMAGE_NAME=\"LOCATION-docker.pkg.dev/[YOUR_PROJECT_ID]/[REPOSITORY_NAME]/[IMAGE_NAME]\"\n</code></pre>"},{"location":"models-as-oci/#usage_1","title":"Usage","text":""},{"location":"models-as-oci/#inside-an-inference-deployment-dockerfile","title":"Inside an Inference Deployment Dockerfile","text":""},{"location":"models-as-oci/#example","title":"Example","text":"<pre><code># Start from the PyTorch base image with CUDA and cuDNN support\nFROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-devel\n\n# Set the working directory\nWORKDIR /srv\n\n# Install vllm (version 0.3.3)\nRUN pip install vllm==0.3.3 --no-cache-dir\n\n# Import the model from the 'model-as-image'\nFROM model-as-image as model\n\n# Copy the model files from 'model-as-image' into the inference container\nCOPY --from=model /model/ /srv/models/$MODEL_DIR/\n\n# Define the entrypoint to run the VLLM OpenAI API server\nENTRYPOINT [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n            \"--host\", \"0.0.0.0\", \"--port\", \"80\", \\\n            \"--model\", \"/srv/models/$MODEL_DIR\", \\\n            \"--dtype=half\"]\n</code></pre>"},{"location":"models-as-oci/#mount-the-image-as-to-your-inference-deployment","title":"Mount the image as to your inference deployment","text":"<p>You can mount the image to a shared volume in your inference deployment via a sidecar </p>"},{"location":"models-as-oci/#example_1","title":"example","text":"<pre><code>initContainers:\n  - name: model\n    image: model-as-image\n    restartPolicy: Always\n    args:\n    - \"sh\"\n    - \"-c\"\n    - \"ln -s /model /mnt/model &amp;&amp; sleep infinity\"\n    volumeMounts:\n    - mountPath: /mnt/model\n      name: model-image-mount\n      readOnly: False\nvolumes:\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: llama3-model\n    emptyDir: {}\n</code></pre> <p>Mount the same volume to your inference container and consume it there.  Pulling images can be optimized in Google Kubernetes Engine with image streaming and secondary boot disk. These method can be used for packaging and mass distributing small/medium size models and low rank adapters of foundational models. </p>"},{"location":"nvidia-nim/","title":"NVIDIA NIM on GKE","text":""},{"location":"nvidia-nim/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Get access to NVIDIA NIMs</p> <p>[!IMPORTANT] Before you proceed further, ensure you have the NVIDIA AI Enterprise License (NVAIE) to access the NIMs.  To get started, go to build.nvidia.com and provide your company email address</p> </li> <li> <p>In the Google Cloud console, on the project selector page, select or create a new project with billing enabled</p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> <li>git</li> <li>jq</li> <li> <p>ngc</p> </li> <li> <p>Enable the required APIs</p> </li> </ol> <pre><code>gcloud services enable \\\n  container.googleapis.com \\\n  file.googleapis.com\n</code></pre>"},{"location":"nvidia-nim/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<ol> <li>Choose your region and set your project and machine variables:</li> </ol> <pre><code>export PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport ZONE=${REGION?}-b\nexport MACH=a2-highgpu-1g\nexport GPU_TYPE=nvidia-tesla-a100\nexport GPU_COUNT=1\n</code></pre> <ol> <li>Create a GKE cluster:</li> </ol> <pre><code>gcloud container clusters create nim-demo --location ${REGION?} \\\n  --workload-pool ${PROJECT_ID?}.svc.id.goog \\\n  --enable-image-streaming \\\n  --enable-ip-alias \\\n  --node-locations ${ZONE?} \\\n  --workload-pool=${PROJECT_ID?}.svc.id.goog \\\n  --addons=GcpFilestoreCsiDriver  \\\n  --machine-type n2d-standard-4 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 5 \\\n  --ephemeral-storage-local-ssd=count=2\n</code></pre> <ol> <li>Create a nodepool</li> </ol> <pre><code>gcloud container node-pools create ${MACH?}-node-pool --cluster nim-demo \\\n   --accelerator type=${GPU_TYPE?},count=${GPU_COUNT?},gpu-driver-version=latest \\\n  --machine-type ${MACH?} \\\n  --ephemeral-storage-local-ssd=count=${GPU_COUNT?} \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=1 --min-nodes=1 --max-nodes=3 \\\n  --node-locations ${ZONE?} \\\n  --region ${REGION?} \\\n  --spot\n</code></pre>"},{"location":"nvidia-nim/#set-up-access-to-nvidia-nims-and-prepare-environment","title":"Set Up Access to NVIDIA NIMs and prepare environment","text":"<ol> <li>Get your NGC_API_KEY from NGC</li> </ol> <pre><code>export NGC_CLI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n</code></pre> <p>[!NOTE] If you have not set up NGC, see NGC Setup to get your access key and begin using NGC.</p> <ol> <li>As a part of the NGC setup, set your configs</li> </ol> <pre><code>ngc config set\n</code></pre> <ol> <li>Ensure you have access to the repository by listing the models</li> </ol> <pre><code>ngc registry model list\n</code></pre> <ol> <li>Create a Kuberntes namespace</li> </ol> <pre><code>kubectl create namespace nim\n</code></pre>"},{"location":"nvidia-nim/#deploy-a-pvc-to-persist-the-model","title":"Deploy a PVC to persist the model","text":"<ol> <li>Create a PVC to persist the model weights - recommended for deployments with more than one (1) replica.  Save the following yaml as <code>pvc.yaml</code>.</li> </ol> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: model-store-pvc\n  namespace: nim\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 30Gi\n  storageClassName: standard-rwx\n</code></pre> <ol> <li>Apply PVC</li> </ol> <pre><code>kubectl apply -f pvc.yaml\n</code></pre> <p>[!NOTE] This PVC will dynamically provision a PV with the necessary storage to persist model weights across replicas of your pods.</p>"},{"location":"nvidia-nim/#deploy-the-nim-with-the-generated-engine-using-a-helm-chart","title":"Deploy the NIM with the generated engine using a Helm chart","text":"<ol> <li>Clone the nim-deploy repository</li> </ol> <pre><code>git clone https://github.com/NVIDIA/nim-deploy.git\ncd nim-deploy/helm\n</code></pre> <ol> <li>Deploy chart with minimal configurations</li> </ol> <pre><code>helm --namespace nim install demo-nim nim-llm/ --set model.ngcAPIKey=$NGC_CLI_API_KEY --set persistence.enabled=true --set persistence.existingClaim=model-store-pvc\n</code></pre>"},{"location":"nvidia-nim/#test-the-nim","title":"Test the NIM","text":"<ol> <li>Expose the service</li> </ol> <pre><code>kubectl port-forward --namespace nim services/demo-nim-nim-llm 8000\n</code></pre> <ol> <li>Send a test prompt - A100</li> </ol> <pre><code>curl -X 'POST' \\\n  'http://localhost:8000/v1/chat/completions' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"messages\": [\n    {\n      \"content\": \"You are a polite and respectful poet.\",\n      \"role\": \"system\"\n    },\n    {\n      \"content\": \"Write a limerick about the wonders of GPUs and Kubernetes?\",\n      \"role\": \"user\"\n    }\n  ],\n  \"model\": \"meta/llama3-8b-instruct\",\n  \"max_tokens\": 256,\n  \"top_p\": 1,\n  \"n\": 1,\n  \"stream\": false,\n  \"frequency_penalty\": 0.0\n}' | jq '.choices[0].message.content' -\n</code></pre> <ol> <li>Browse the API by navigating to http://localhost:8000/docs</li> </ol>"},{"location":"skypilot/","title":"GKE cross region capacity chasing with SkyPilot","text":"<p>Due to the limited availability of accelerator resources, customers face significant challenges in securing sufficient capacity to run their AI/ML workloads. They often require:</p> <ul> <li>Preferences for VM families and accelerators, with the ability to automatically fail over to alternative configurations if their preferred resources are unavailable.</li> <li>Automatic capacity acquisition across regions to address scenarios where a specific region lacks sufficient resources.</li> </ul> <p>In this tutorial, we will demonstrate how to leverage the open-source software SkyPilot to help GKE customers efficiently obtain accelerators across regions, ensuring workload continuity and optimized resource utilization.</p> <p>SkyPilot is a framework for running AI and batch workloads on any infra, offering unified execution, high cost savings, and high GPU availability. By combining SkyPilot with GKE's solutions (such as Kueue + Dynamic Workload Scheduler, Custom compute class, GCS FUSE), users can effectively address capacity challenges while optimizing costs.</p>"},{"location":"skypilot/#the-overview","title":"The overview.","text":"<p>In this tutorial, our persona is an ML scientist planning to run a batch workload for hyperparameter tuning. This workload involves two experiments, with each experiment requiring 4 GPUs to execute. </p> <p>We have two GKE clusters in different regions: one in us-central1 with 4A100 and another in us-west1 with 4L4.</p> <p>By the end of this tutorial, our goal is to have one experiment running in the us-central cluster and the other in the us-west cluster, demonstrating efficient resource distribution across regions.</p> <p>SkyPilot supports GKE's cluster autoscaling for dynamic resource management. However, to keep this tutorial straightforward, we will demonstrate the use of a static node pool instead.</p>"},{"location":"skypilot/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Ensure you have a gcp project with billing enabled and enabled the GKE API. </p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> </ol>"},{"location":"skypilot/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<p>Create two clusters, you can create  the clusters in parrallel to reduce time. 1. Set the default environment variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get project)\n</code></pre> <ol> <li>Create a GKE cluster in us-central1-c with 4*A100</li> </ol> <pre><code>gcloud container clusters create demo-us-central1 \\\n    --location=us-central1-c \\\n    --project=$PROJECT_ID \n</code></pre> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-a100,count=4 \\\n  --machine-type a2-highgpu-4g \\\n  --region us-central1-c \\\n  --cluster=demo-us-central1 \\\n  --num-nodes=1\n</code></pre> <pre><code>gcloud container clusters get-credentials demo-us-central1 \\\n--region us-central1-c \\\n--project ${PROJECT_ID}\n</code></pre> <ol> <li>Create a GKE cluster in us-west1-c with 4*L4</li> </ol> <pre><code>gcloud container clusters create demo-us-west1 \\\n    --location=us-west1-c \\\n    --project=$PROJECT_ID \n</code></pre> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4,count=4 \\\n  --machine-type g2-standard-48 \\\n  --region us-west1-c \\\n  --cluster=demo-us-west1 \\\n  --num-nodes=1\n</code></pre> <pre><code>gcloud container clusters get-credentials demo-us-west1 \\\n--region us-west1-c \\\n--project ${PROJECT_ID}\n</code></pre>"},{"location":"skypilot/#install-skypilot","title":"Install SkyPilot","text":"<ol> <li>Create a virtual environment.</li> </ol> <pre><code>cd ~\ngit clone https://github.com/GoogleCloudPlatform/ai-on-gke.git\ncd ai-on-gke/tutorials-and-examples/skypilot\npython3 -m venv ~/ai-on-gke/tutorials-and-examples/skypilot\nsource bin/activate \n</code></pre> <ol> <li>Install SkyPilot</li> </ol> <pre><code>pip install -U \"skypilot[kubernetes,gcp]\"\n</code></pre> <pre><code>sky check\n\nsky show-gpus\n</code></pre> <ol> <li>Find the context names</li> </ol> <pre><code>kubectl config get-contexts\n\n# Find the context name, for example: \ngke_${PROJECT_NAME}_us-central1-c_demo-us-central1\ngke_${PROJECT_NAME}_us-west1-c_demo-us-west1\n</code></pre> <ol> <li>Copy the following yaml to ~/.sky/config.yaml with context name replaced. SkyPilot will evaludate the contexts by the order specified until it finds a cluster that provides enough capacity to deploy the workload.</li> </ol> <pre><code>allowed_clouds:\n  - gcp\n  - kubernetes\nkubernetes:\n  # Use the context's name\n  allowed_contexts:\n    - gke_${PROJECT_NAME}_us-central1-c_demo-us-central1\n    - gke_${PROJECT_NAME}_us-west1-c_demo-us-west1\n  provision_timeout: 30\n</code></pre>"},{"location":"skypilot/#launch-the-jobs","title":"Launch the jobs","text":"<p>Under <code>~/ai-on-gke/tutorials-and-examples/skypilot</code>, you\u2019ll find a file named <code>train.yaml</code>, which uses SkyPilot's syntax to define a job. The job will ask for 4* A100 first. If no capacity is found, it failovers to L4. </p> <pre><code>resources:\n  cloud: kubernetes\n  # list has orders\n  accelerators: [ A100:4, L4:4 ]\n</code></pre> <p>The <code>launch.py</code> a Python program that initiates a hyperparameter tuning process with two candidates for the learning rate (LR) parameter. In production environments, such experiments are typically tracked using open-source frameworks like MLFlow.</p> <p>Start the trainig:</p> <pre><code>python launch.py\n</code></pre> <p>SkyPilot will first select the demo-us-central1 cluster, which has 4 A100 GPUs available. For the second job, it will launch in the demo-us-west1 cluster using L4 GPUs, as no additional clusters with 4 A100 GPUs were available.</p> <p>You also can check SkyPilot's status using: </p> <pre><code>sky status\n</code></pre> <p>You can SSH into the pod in GKE using the cluster's name. Once inside, you'll find the local source code synced to the pod under <code>~/sky_workdir</code>. This setup makes it convenient for developers to debug and iterate on their AI/ML code efficiently.</p> <pre><code>ssh train-cluster1\n</code></pre>"},{"location":"skypilot/#clean-up","title":"Clean up","text":"<p>Delete the GKE clusters.</p> <pre><code>gcloud container clusters delete demo-us-central1 \\\n    --location=us-central1-c \\\n    --project=$PROJECT_ID\n</code></pre> <pre><code>gcloud container clusters delete demo-us-west1 \\\n    --location=us-west1-c \\\n    --project=$PROJECT_ID\n</code></pre>"},{"location":"skypilot/text-classification/","title":"Index","text":""},{"location":"skypilot/text-classification/#text-classification-examples","title":"Text classification examples","text":""},{"location":"skypilot/text-classification/#glue-tasks","title":"GLUE tasks","text":"<p>Based on the script <code>run_glue.py</code>.</p> <p>Fine-tuning the library models for sequence classification on the GLUE benchmark: General Language Understanding Evaluation. This script can fine-tune any of the models on the hub and can also be used for a dataset hosted on our hub or your own data in a csv or a JSON file (the script might need some tweaks in that case, refer to the comments inside for help).</p> <p>GLUE is made up of a total of 9 different tasks. Here is how to run the script on one of them:</p> <pre><code>export TASK_NAME=mrpc\n\npython run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>where task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.</p> <p>We get the following results on the dev set of the benchmark with the previous commands (with an exception for MRPC and WNLI which are tiny and where we used 5 epochs instead of 3). Trainings are seeded so you should obtain the same results with PyTorch 1.6.0 (and close results with different versions), training times are given for information (a single Titan RTX was used):</p> Task Metric Result Training time CoLA Matthews corr 56.53 3:17 SST-2 Accuracy 92.32 26:06 MRPC F1/Accuracy 88.85/84.07 2:21 STS-B Pearson/Spearman corr. 88.64/88.48 2:13 QQP Accuracy/F1 90.71/87.49 2:22:26 MNLI Matched acc./Mismatched acc. 83.91/84.10 2:35:23 QNLI Accuracy 90.66 40:57 RTE Accuracy 65.70 57 WNLI Accuracy 56.34 24 <p>Some of these results are significantly different from the ones reported on the test set of GLUE benchmark on the website. For QQP and WNLI, please refer to FAQ #12 on the website.</p> <p>The following example fine-tunes BERT on the <code>imdb</code> dataset hosted on our hub:</p> <pre><code>python run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --dataset_name imdb  \\\n  --do_train \\\n  --do_predict \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/imdb/\n</code></pre> <p>If your model classification head dimensions do not fit the number of labels in the dataset, you can specify <code>--ignore_mismatched_sizes</code> to adapt it.</p>"},{"location":"skypilot/text-classification/#text-classification","title":"Text classification","text":"<p>As an alternative, we can use the script <code>run_classification.py</code> to fine-tune models on a single/multi-label classification task. </p> <p>The following example fine-tunes BERT on the <code>en</code> subset of  <code>amazon_reviews_multi</code> dataset. We can specify the metric, the label column and aso choose which text columns to use jointly for classification. </p> <pre><code>dataset=\"amazon_reviews_multi\"\nsubset=\"en\"\npython run_classification.py \\\n    --model_name_or_path  google-bert/bert-base-uncased \\\n    --dataset_name ${dataset} \\\n    --dataset_config_name ${subset} \\\n    --shuffle_train_dataset \\\n    --metric_name accuracy \\\n    --text_column_name \"review_title,review_body,product_category\" \\\n    --text_column_delimiter \"\\n\" \\\n    --label_column_name stars \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 512 \\\n    --per_device_train_batch_size 32 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 1 \\\n    --output_dir /tmp/${dataset}_${subset}/\n</code></pre> <p>Training for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+body+category.</p> <p>The following is a multi-label classification example. It fine-tunes BERT on the <code>reuters21578</code> dataset hosted on our hub:</p> <pre><code>dataset=\"reuters21578\"\nsubset=\"ModApte\"\npython run_classification.py \\\n    --model_name_or_path google-bert/bert-base-uncased \\\n    --dataset_name ${dataset} \\\n    --dataset_config_name ${subset} \\\n    --shuffle_train_dataset \\\n    --remove_splits \"unused\" \\\n    --metric_name f1 \\\n    --text_column_name text \\\n    --label_column_name topics \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 512 \\\n    --per_device_train_batch_size 32 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 15 \\\n    --output_dir /tmp/${dataset}_${subset}/ \n</code></pre> <p>It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that you have to explicitly remove the \"unused\" split from the dataset, since it is not used for classification.</p>"},{"location":"skypilot/text-classification/#mixed-precision-training","title":"Mixed precision training","text":"<p>If you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision training with PyTorch 1.6.0 or latest, or by installing the Apex library for previous versions. Just add the flag <code>--fp16</code> to your command launching one of the scripts mentioned above!</p> <p>Using mixed precision training usually results in 2x-speedup for training with the same final results:</p> Task Metric Result Training time Result (FP16) Training time (FP16) CoLA Matthews corr 56.53 3:17 56.78 1:41 SST-2 Accuracy 92.32 26:06 91.74 13:11 MRPC F1/Accuracy 88.85/84.07 2:21 88.12/83.58 1:10 STS-B Pearson/Spearman corr. 88.64/88.48 2:13 88.71/88.55 1:08 QQP Accuracy/F1 90.71/87.49 2:22:26 90.67/87.43 1:11:54 MNLI Matched acc./Mismatched acc. 83.91/84.10 2:35:23 84.04/84.06 1:17:06 QNLI Accuracy 90.66 40:57 90.96 20:16 RTE Accuracy 65.70 57 65.34 29 WNLI Accuracy 56.34 24 56.34 12"},{"location":"skypilot/text-classification/#pytorch-version-no-trainer","title":"PyTorch version, no Trainer","text":"<p>Based on the script <code>run_glue_no_trainer.py</code>.</p> <p>Like <code>run_glue.py</code>, this script allows you to fine-tune any of the models on the hub on a text classification task, either a GLUE task or your own data in a csv or a JSON file. The main difference is that this script exposes the bare training loop, to allow you to quickly experiment and add any customization you would like.</p> <p>It offers less options than the script with <code>Trainer</code> (for instance you can easily change the options for the optimizer or the dataloaders directly in the script) but still run in a distributed setup, on TPU and supports mixed precision by the mean of the \ud83e\udd17 <code>Accelerate</code> library. You can use the script normally after installing it:</p> <pre><code>pip install git+https://github.com/huggingface/accelerate\n</code></pre> <p>then</p> <pre><code>export TASK_NAME=mrpc\n\npython run_glue_no_trainer.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --max_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>You can then use your usual launchers to run in it in a distributed environment, but the easiest way is to run</p> <pre><code>accelerate config\n</code></pre> <p>and reply to the questions asked. Then</p> <pre><code>accelerate test\n</code></pre> <p>that will check everything is ready for training. Finally, you can launch training with</p> <pre><code>export TASK_NAME=mrpc\n\naccelerate launch run_glue_no_trainer.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --max_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>This command is the same and will work for:</p> <ul> <li>a CPU-only setup</li> <li>a setup with one GPU</li> <li>a distributed training with several GPUs (single or multi node)</li> <li>a training on TPUs</li> </ul> <p>Note that this library is in alpha release so your feedback is more than welcome if you encounter any problem using it.</p>"},{"location":"skypilot/text-classification/#xnli","title":"XNLI","text":"<p>Based on the script <code>run_xnli.py</code>.</p> <p>XNLI is a crowd-sourced dataset based on MultiNLI. It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-resource language such as English and low-resource languages such as Swahili).</p>"},{"location":"skypilot/text-classification/#fine-tuning-on-xnli","title":"Fine-tuning on XNLI","text":"<p>This example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins on a single tesla V100 16GB.</p> <pre><code>python run_xnli.py \\\n  --model_name_or_path google-bert/bert-base-multilingual-cased \\\n  --language de \\\n  --train_language en \\\n  --do_train \\\n  --do_eval \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 5e-5 \\\n  --num_train_epochs 2.0 \\\n  --max_seq_length 128 \\\n  --output_dir /tmp/debug_xnli/ \\\n  --save_steps -1\n</code></pre> <p>Training with the previously defined hyper-parameters yields the following results on the test set:</p> <pre><code>acc = 0.7093812375249501\n</code></pre>"},{"location":"storage/hyperdisk-ml/","title":"Index","text":""},{"location":"storage/hyperdisk-ml/#populate-a-hyperdisk-ml-disk-from-google-cloud-storage","title":"Populate a Hyperdisk ML Disk from Google Cloud Storage","text":""},{"location":"storage/hyperdisk-ml/#this-guide-uses-the-google-cloud-api-to-create-a-hyperdisk-ml-disk-from-data-in-cloud-storage-and-then-use-it-in-a-gke-cluster-refer-to-this-documentation-for-instructions-all-in-the-gke-api","title":"This guide uses the Google Cloud API to create a Hyperdisk ML disk from data in Cloud Storage and then use it in a GKE cluster. Refer to this documentation for instructions all in the GKE API","text":"<ol> <li>Create a new GCE instance that you will use to hydrate the new Hyperdisk ML disk with data. Note a c3-standard-44 instance is used to provide the max throughput while populating the hyperdisk(Instance to throughput rates).</li> </ol> <pre><code>VM_NAME=hydrator\nMACHINE_TYPE=c3-standard-44\nIMAGE_FAMILY=debian-11\nIMAGE_PROJECT=debian-cloud\nZONE=us-central1-a\nSNAP_SHOT_NAME=hdmlsnapshot\nPROJECT_ID=myproject\nDISK_NAME=model1\n\ngcloud compute instances create $VM_NAME \\\n    --image-family=$IMAGE_FAMILY \\\n    --image-project=$IMAGE_PROJECT \\\n    --zone=$ZONE \\\n    --machine-type=$MACHINE_TYPE\n\ngcloud compute ssh $VM_NAME\n\n</code></pre> <p>Update and authenticate the instance</p> <pre><code>sudo apt-get update\nsudo apt-get install google-cloud-cli\ngcloud init\ngcloud auth login\n\n</code></pre> <ol> <li>Create and attach the disk to the new GCE VM.</li> </ol> <pre><code>SIZE=140\nTHROUGHPUT=2400\n\ngcloud compute disks create $DISK_NAME --type=hyperdisk-ml \\\n--size=$SIZE --provisioned-throughput=$THROUGHPUT  \\\n--zone $ZONE\n\ngcloud compute instances attach-disk $VM_NAME --disk=$DISK_NAME --zone=$ZONE \n</code></pre> <ol> <li>Log into the hydrator VM, format the volume, initiate transfer, and dismount the volume.</li> </ol> <pre><code>gcloud compute ssh $VM_NAME\n</code></pre> <p>Identify the device name (eg: /dev/nvme0n2) by looking at the output of lsblk. This should correspond to the disk that was attached in the previous step. </p> <pre><code>lsblk\n</code></pre> <p>Save device name given by lsblk(example nvme0n2)</p> <pre><code>DEVICE=/dev/nvme0n2\n</code></pre> <pre><code>GCS_DIR=gs://vertex-model-garden-public-us-central1/llama2/llama2-70b-hf \nsudo /sbin/mkfs -t ext4 -E lazy_itable_init=0,lazy_journal_init=0,discard $DEVICE\n\nsudo mount $DEVICE /mnt\ngcloud storage cp -r $GCS_DIR /mnt\nsudo umount /mnt\n</code></pre> <ol> <li>Detach disk from the hydrator and switch to READ_ONLY_MANY access mode.</li> </ol> <pre><code>gcloud compute instances detach-disk $VM_NAME --disk=$DISK_NAME --zone=$ZONE\ngcloud compute disks update $DISK_NAME --access-mode=READ_ONLY_MANY  --zone=$ZONE\n</code></pre> <ol> <li>Create a snapshot from the disk to use as a template.</li> </ol> <pre><code>gcloud compute snapshots create $SNAP_SHOT_NAME \\\n    --source-disk-zone=$ZONE \\\n    --source-disk=$DISK_NAME \\\n    --project=$PROJECT_ID\n</code></pre> <ol> <li>You now have a hyperdisk ML snapshot populated with your data from Google Cloud Storage. You can delete the hydrator GCE instance and the original disk.</li> </ol> <pre><code>gcloud compute instances delete $VM_NAME --zone=$ZONE\ngcloud compute disks delete $DISK_NAME --project $PROJECT_ID --zone $ZONE\n</code></pre> <ol> <li>In your GKE cluster create your Hypedisk ML multi zone and Hyperdisk ML storage classes. Hyperdisk ML disks are zonal and the Hyperdisk-ml-multi-zone storage class automatically provisions disks in zones where the pods using them are.  Replace the zones in this class with the zones you want to allow the Hyperdisk ML snapshot to create disks in. </li> </ol> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: hyperdisk-ml-multi-zone\nparameters:\n  type: hyperdisk-ml\n  provisioned-throughput-on-create: \"2400Mi\"\n  enable-multi-zone-provisioning: \"true\"\nprovisioner: pd.csi.storage.gke.io\nallowVolumeExpansion: false\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowedTopologies:\n- matchLabelExpressions:\n  - key: topology.gke.io/zone\n    values:\n    - us-central1-a\n    - us-central1-c\n--- \napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n    name: hyperdisk-ml\nparameters:\n    type: hyperdisk-ml\nprovisioner: pd.csi.storage.gke.io\nallowVolumeExpansion: false\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <ol> <li>Create a volumeSnapshotClass and VolumeSnapshotContent config to use your snapshot. Replace the VolumeSnapshotContent.spec.source.snapshotHandle with the path to your snapshot. </li> </ol> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: my-snapshotclass\ndriver: pd.csi.storage.gke.io\ndeletionPolicy: Delete\n---\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: restored-snapshot\nspec:\n  volumeSnapshotClassName: my-snapshotclass\n  source:\n    volumeSnapshotContentName: restored-snapshot-content\n---\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: restored-snapshot-content\nspec:\n  deletionPolicy: Retain\n  driver: pd.csi.storage.gke.io\n  source:\n    snapshotHandle: projects/[project_ID]/global/snapshots/[snapshotname]\n  volumeSnapshotRef:\n    kind: VolumeSnapshot\n    name: restored-snapshot\n    namespace: default\n\n</code></pre> <ol> <li>Reference your snapshot in the persistent volume claim. Be sure to adjust the spec.dataSource.name and spec.resources.requests.storage to your snapshot name and size.</li> </ol> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: hdml-consumer-pvc\nspec:\n  dataSource:\n    name: restored-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n  - ReadOnlyMany\n  storageClassName: hyperdisk-ml-multi-zone\n  resources:\n    requests:\n      storage: 140Gi\n</code></pre> <ol> <li>Add a reference to this PVC in your deployment spec.template.spec.volume.persistentVolumeClaim.claimName parameter. </li> </ol> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: busybox\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - image: busybox:latest\n        name: busybox\n        command:\n          - \"sleep\"\n          - \"infinity\"\n        volumeMounts:\n        - name: busybox-persistent-storage\n          mountPath: /var/www/html\n      volumes:\n      - name: busybox-persistent-storage\n        persistentVolumeClaim:\n          claimName: hdml-consumer-pvc\n</code></pre>"},{"location":"tpu-examples/single-host-inference/jax/stable-diffusion/","title":"Index","text":""},{"location":"tpu-examples/single-host-inference/jax/stable-diffusion/#serve-online-inference-a-model-using-a-single-tpu-and-gke","title":"Serve (online inference) a model using a single TPU and GKE","text":"<p>To better understand how TPUs work on GKE, please read the doc TPUs in GKE introduction.</p> <p>This directory contains files for JAX Model inference and serving. You can find step-by-step instructions in the quickstart guide.</p>"},{"location":"tpu-examples/training/mnist-single-tpu/","title":"Train a model with TPUs on GKE Standard mode","text":"<p>Please follow the Quick Start at https://cloud.google.com/kubernetes-engine/docs/quickstarts/train-model-tpus-standard</p>"},{"location":"vector-databases/readme/","title":"Vector Database Repo","text":""},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/","title":"From RAG to autonomous apps with Weaviate and Gemini on Google Kubernetes Engine","text":""},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#video-of-this-tutorial","title":"Video of this tutorial","text":"<p>This demo application creates a product catalog that is stored in a Weaviate vector database and vectorized for semantic search. It is also integrated with the Gemini Pro Vision API to automatically create product descriptions. The App is built with the Next.js framework. </p> <p>There is also a notebook, notebook.ipynb, asset that allows you to work directly with the Weaviate database and Gemini API to create advanced product descriptions based on user personas as well as demonstrate the concept of \"Generative Feedback Loops\" as done in the original session. </p> <p>Our application architecture consists of: - Cloud Build to build our NEXTJS website into a container image and deploy it to Cloud Run - Cloud Run and host our containerized website - Google Cloud Storage to store and host our product images - Google Gemini/Vertex AI to generate our embeddings and product descriptions - Weaviate Vector Database for product and embedding storage and search - GKE to run our database</p> <p>Product Catalog</p> <p></p> <p>Product Description Generation and Semantic Search</p> <p>You can create/edit a product and use Gemini to create the product description based on the image and title</p> <p></p> <p>System Architecture</p> <p></p>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#to-deploy-this-demo","title":"To Deploy this demo...","text":"<p>clone this git repo</p> <pre><code>git clone https://github.com/bkauf/next-store.git\ncd next-store\n</code></pre>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#setup-the-weaviate-vector-database","title":"Setup the Weaviate Vector Database","text":""},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#get-your-gemini-api-key","title":"Get your GEMINI API key","text":"<ul> <li>Go to https://developers.generativeai.google/ to create a GEMINI API key. This is necessary to be able to run the demo.</li> <li>Set this API key as an environment variable</li> </ul> <pre><code>export GEMINI_API_KEY=&lt;your Gemini API key&gt;\n</code></pre>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#enable-the-necessary-google-cloud-apis","title":"Enable the necessary Google Cloud APIs","text":"<p>Set your project id</p> <pre><code>export PROJECT_ID=&lt;your project id&gt;\ngcloud config set core/project $PROJECT_ID\n</code></pre> <p>Enable the necessary Google cloud APIs for your project. These APIs are necessary for installing the Weaviate database and to run the Demo app.</p> <pre><code>\ngcloud services enable \\\ncloudbuild.googleapis.com \\\nstorage.googleapis.com \\\nserviceusage.googleapis.com \\\ncloudresourcemanager.googleapis.com \\\ncompute.googleapis.com \\\niam.googleapis.com \\\ncontainer.googleapis.com \\\nartifactregistry.googleapis.com \\\nclouddeploy.googleapis.com \\\nrun.googleapis.com\n</code></pre>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#deploy-the-gke-cluster","title":"Deploy the GKE Cluster","text":"<ol> <li> <p>Set environment varia1bles used in later steps.</p> <p><code>sh export CLUSTER_NAME=demo-cluster # A name for your cluster, you can change this if you want. export LOCATION=us-central1 # Google cloud region to host your infrastucture, you can change this if you want.</code></p> </li> <li> <p>We will use the default VPC to deploy the GKE cluster into. In some environments, the autocreation of the default VPC is disabled by the administrator and default network will not be present. If your project doesn't have it, simply create a new VPC network named default and add the add the firewall rules mentioned here to the VPC. You can check if the default network exists by running this command.</p> <p><code>sh gcloud compute networks list</code></p> </li> <li> <p>Deploy a small regional GKE Cluster. This step can take several minutes to finish.</p> <p>```sh  gcloud container clusters create $CLUSTER_NAME \\ --project=$PROJECT_ID \\ --region=$LOCATION \\ --enable-ip-alias \\ --num-nodes=1 \\ --scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/cloud-platform</p> <p>```</p> </li> </ol>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#install-weaviate","title":"Install Weaviate","text":"<ol> <li> <p>Connect to the cluster     <code>sh     gcloud container clusters get-credentials $CLUSTER_NAME --region $LOCATION --project $PROJECT_ID</code></p> </li> <li> <p>We are going to use the regional persistant disk storage class for weaviate, so we'll install that storage class in the cluster.</p> <p><code>sh kubectl apply -f weaviate/storage-class.yaml</code></p> </li> <li> <p>Let's create a secret API KEY for weaviate so we don't allow unauthenticated access.</p> <p><code>sh export WEAVIATE_API_KEY=\"next-demo349834\" # you can choose another random string as the key.</code> 1. Store the key as a kubernetes secret.</p> <p>```sh kubectl create namespace weaviate</p> <p>kubectl create secret generic user-keys \\ --from-literal=AUTHENTICATION_APIKEY_ALLOWED_KEYS=$WEAVIATE_API_KEY \\ -n weaviate ``` 1. Install Weaviate using a helm chart.</p> <p>```sh helm repo add weaviate https://weaviate.github.io/weaviate-helm</p> <p>helm upgrade --install weaviate weaviate/weaviate \\ -f weaviate/demo-values.yaml \\ --set modules.text2vec-palm.apiKey=$GEMINI_API_KEY \\ --namespace weaviate ```</p> </li> </ol>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#get-weaviate-server-ips","title":"Get Weaviate Server IPs","text":"<ol> <li> <p>Get the HTTP IP that the web server will use</p> <p>```sh export WEAVIATE_SERVER=\"\" while [[ -z \"$WEAVIATE_SERVER\" ]]; do WEAVIATE_SERVER=$(kubectl get service weaviate -n weaviate -o jsonpath='{.status.loadBalancer.ingress[0].ip}') sleep 2  done echo \"External HTTP IP obtained: $WEAVIATE_SERVER\"</p> <p>``` 1. Get the IP of the GRPC service that the database creation script will use</p> <p><code>sh export WEAVIATE_SERVER_GRPC=\"\" while [[ -z \"$WEAVIATE_SERVER_GRPC\" ]]; do WEAVIATE_SERVER_GRPC=$(kubectl get service weaviate-grpc -n weaviate -o jsonpath='{.status.loadBalancer.ingress[0].ip}') sleep 2  done echo \"External GRPC IP obtained: $WEAVIATE_SERVER_GRPC\"</code></p> </li> </ol>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#configure-the-database","title":"Configure the Database","text":"<ol> <li> <p>Install the Weaviate client</p> <p><code>sh pip3 install -U weaviate-client  # For beta versions: `pip install --pre -U \"weaviate-client==4.*\"`</code></p> </li> <li> <p>Create the database schema and load the sample files     by running the following script from the root directory of the repo where the first_99_objects.json file is located.</p> <p><code>sh python3 createdb.py</code> You should get an output similar to the below if the database was created successfully: <code>Items added to the database: 99</code></p> </li> </ol>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#setup-the-demo-application","title":"Setup the Demo application","text":"<p>The following steps will walk through adding the nessessary variables to the demo application, creating a container for it, then running it on Google Cloud Run.</p> <ol> <li> <p>Create your storage bucket and allow public access to it.</p> <p>```sh export GCS_BUCKET=\"$PROJECT_ID-next-demo\"</p> <p>gcloud storage buckets create gs://$GCS_BUCKET --location=$LOCATION \\ --no-public-access-prevention ```</p> <p>Allow public access to the bucket</p> <p>```sh</p> <p>gcloud storage buckets add-iam-policy-binding gs://$GCS_BUCKET --member=allUsers --role=roles/storage.objectViewer ```</p> </li> <li> <p>Create a .env file for the demo application.</p> <p><code>sh cd demo-website/ touch .env</code></p> <p>Create a .env file in the demo-website directory for the NextJS build process and replace the variables below with your own. If you would like to run this demo app locally with npm run dev you will also need a service account that has GCS object admin permissions. See optional variable below. If you would like to run this on Cloud Run you do not need a local service account.</p> <p>.env file  ```sh GEMINI_API_KEY=\"The GEMINI api key you retrieved earlier\" GCS_BUCKET=\"The name of the bucket you created earlier\" WEAVIATE_SERVER=\"from weaviate install steps\" WEAVIATE_API_KEY=\"next-demo349834\" </p> </li> <li> <p>Create a artifact registry repo for your container.</p> <p>```sh export REPO_NAME=\"next-demo\"</p> <p>gcloud artifacts repositories create $REPO_NAME --repository-format=docker \\     --location=$LOCATION --description=\"Docker repository\" \\     --project=$PROJECT_ID ```</p> </li> <li> <p>Create a container image to store in the image repo.</p> <p><code>sh gcloud builds submit --tag $LOCATION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/next-demo:1.0</code></p> </li> <li> <p>Create a service account for Cloud Run to use to connect to GCS for image uploads.</p> <p>```sh export SERVICE_ACCOUNT_NAME=\"next-demo\"</p> <p>gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \\ --display-name=\"Next Demo\"</p> <p>gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\"serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/storage.objectAdmin\" ```</p> </li> <li> <p>Deploy the container image to Cloud Run.</p> <p>The following commands set your envorinemnt varaibles for Cloud Run and also the service account that allows uploads to your public Google Cloud Storage bucket.</p> <p>```sh export CLOUD_RUN_NAME=\"next-store\"</p> <p>gcloud run deploy $CLOUD_RUN_NAME \\     --image $LOCATION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/next-demo:1.0 \\     --port=3000 \\     --allow-unauthenticated \\     --region $LOCATION \\ --set-env-vars=GEMINI_API_KEY=$GEMINI_API_KEY, \\ --set-env-vars=GCS_BUCKET=$GCS_BUCKET, \\ --set-env-vars=WEAVIATE_SERVER=http://$WEAVIATE_SERVER, \\ --set-env-vars=WEAVIATE_API_KEY=$WEAVIATE_API_KEY \\ --service-account=$SERVICE_ACCOUNT_NAME ```</p> <p>Navigate to the demo application via the service URL returned. You can use the data below to create a new item and search for it:</p> <ul> <li>Title: Project Sushi Tshirt</li> <li>Category: tees Tshirts</li> <li>Image: https://shop.googlemerchandisestore.com/store/20190522377/assets/items/images/GGCPGXXX1338.jpg</li> </ul> </li> </ol>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#if-you-plan-to-run-this-locally-use-a-sevice-account-file-with-gcs-object-admin-permissions","title":"If you plan to run this locally use a sevice account file with GCS object admin permissions","text":""},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/#google_application_credentialssajson","title":"GOOGLE_APPLICATION_CREDENTIALS=\"sa.json\"","text":"<p>```</p>"},{"location":"vector-databases/NEXT-2024-Weaviate-Demo/demo-website/","title":"Demo website for Generative Feedback Loops","text":"<p>Build the IAM account for cloud run</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"SERVICE_ACCOUNT_EMAIL\" \\\n    --member \"PRINCIPAL\" \\\n    --role \"roles/iam.serviceAccountUser\"\n</code></pre> <p>Build  the container using cloud build</p> <pre><code>gcloud builds submit --tag us-central1-docker.pkg.dev/[Project ID]/Repo]/[Name]\n</code></pre> <p>Submit the container to cloud run</p> <p>Sample product to upload</p> <pre><code>    {\n        \"id\": \"id_22\",\n        \"product_id\": \"GGCPGAAJ133812\",\n        \"title\": \"Project Sushi Tshirt\",\n        \"category\": \"Clothing  accessories Tops  tees Tshirts\",\n        \"link\": \"https://shop.googlemerchandisestore.com/store/20190522377/assets/items/images/GGCPGXXX1338.jpg\",\n        \"description\": \"This is a set of two tshirts The first shirt is a heather gray shirt with a black N3Q logo on the chest The second shirt is a heather gray shirt with a black and red sushi graphic on the chest Both shirts are made of 100 cotton and are machine washable\",\n        \"color\": \"['gray', 'black', 'red']\",\n        \"gender\": \"unisex\",\n        \"brand\": \"Google\"\n    },\n</code></pre>"},{"location":"workflow-orchestration/dws-examples/","title":"Dynamic Workload Scheduler examples","text":"<p>The repository contains examples on how to use DWS in GKE. More information about DWS is available here.</p>"},{"location":"workflow-orchestration/dws-examples/#prerequisites","title":"Prerequisites","text":""},{"location":"workflow-orchestration/dws-examples/#kueue","title":"Kueue","text":"<p>To install a released version of Kueue in your cluster, run the following command:</p> <pre><code>VERSION=v0.7.0\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n</code></pre> <p>For more configuration options visit Kueue's installation guide.</p>"},{"location":"workflow-orchestration/dws-examples/#files-included","title":"Files included","text":"<ul> <li><code>dws-queue.yaml</code> - Kueue's Cluster and Local queues with ProvisioningRequest and DWS support enabled.</li> <li><code>job.yaml</code> - Sample job that requires GPU and uses DWS-enabled queue. Contains optional annotation <code>provreq.kueue.x-k8s.io/maxRunDurationSeconds</code> which sets <code>maxRunDurationSeconds</code> for the created ProvisioningRequest</li> </ul>"},{"location":"workflow-orchestration/indexed-job/","title":"Running distributed ML training workloads on GKE using Indexed Jobs","text":"<p>In this guide you will run a distributed ML training workload on GKE using an Indexed Job.</p> <p>Specifically, you will train a handwritten digit image classifier on the classic MNIST dataset using PyTorch. The training computation will be distributed across 4 GPU nodes in a GKE cluster.</p>"},{"location":"workflow-orchestration/indexed-job/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud account set up.</li> <li>gcloud command line tool installed and configured to use your GCP project.</li> <li>kubectl command line utility is installed.</li> <li>docker is installed.</li> </ul>"},{"location":"workflow-orchestration/indexed-job/#1-create-a-standard-gke-cluster","title":"1. Create a standard GKE cluster","text":"<p>Run the command: </p> <pre><code>gcloud container clusters create demo --zone us-central1-c\n</code></pre> <p>You should see output indicating the cluster is being created (this can take ~10 minutes or so).</p>"},{"location":"workflow-orchestration/indexed-job/#2-create-a-gpu-node-pool","title":"2. Create a GPU node pool.","text":"<p>You can choose any supported GPU type you wish, using a supported machine type. See the docs for more details. In this example, we are using NVIDIA Tesla T4s with the N1 machine family.</p> <pre><code>gcloud container node-pools create gpu-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1,gpu-driver-version=LATEST \\\n  --machine-type n1-standard-4 \\\n  --zone us-central1-c --cluster demo \\\n  --node-locations us-central1-c \\\n  --num-nodes 4\n</code></pre> <p>Creating this GPU node pool will take a few minutes.</p>"},{"location":"workflow-orchestration/indexed-job/#3-build-and-push-the-docker-image-to-gcr","title":"3. Build and push the Docker image to GCR","text":"<p>Make a local copy of the mnist.py file which defines a traditional convolutional neural network, as the training logic which trains the model on the classic MNIST dataset.</p> <p>Next, make a local copy of the Dockerfile and run the following commands to build the container image and push it to your GCR repository:</p> <pre><code>export PROJECT_ID=&lt;your GCP project ID&gt;\ndocker build -t pytorch-mnist-gpu -f Dockerfile .\ndocker tag pytorch-mnist-gpu gcr.io/$PROJECT_ID/pytorch-mnist-gpu:latest\ndocker push gcr.io/$PROJECT_ID/pytorch-mnist-gpu:latest\n</code></pre>"},{"location":"workflow-orchestration/indexed-job/#4-define-an-indexed-job-and-headless-service","title":"4. Define an Indexed Job and Headless Service","text":"<p>In the yaml below, we configure an Indexed Job to run 4 pods, 1 for each GPU node, and use torchrun to kick off a distributed training job for the CNN model on the MNIST dataset. This training job will utilize 1 T4 GPU chip on each node in the node pool.</p> <p>We also define a headless service which selects the pods owned by this Indexed Job. This will trigger the creation of the DNS records needed for the pods to communicate with eachother over the network via hostnames.</p> <p>Copy the yaml below into a local file <code>mnist.yaml</code> and be sure to replace <code>&lt;PROJECT_ID&gt;</code> with your GCP project ID in the container image.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: headless-svc\nspec:\n  clusterIP: None \n  selector:\n    job-name: pytorchworker\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pytorchworker\nspec:\n  backoffLimit: 0\n  completions: 4\n  parallelism: 4\n  completionMode: Indexed\n  template:\n    spec:\n      subdomain: headless-svc\n      restartPolicy: Never\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-tesla-t4\n      tolerations:\n      - operator: \"Exists\"\n        key: nvidia.com/gpu\n      containers:\n      - name: pytorch\n        image: gcr.io/&lt;PROJECT_ID&gt;/pytorch-mnist-gpu:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3389\n        env:\n        - name: MASTER_ADDR\n          value: pytorchworker-0.headless-svc\n        - name: MASTER_PORT\n          value: \"3389\"\n        - name: PYTHONBUFFERED\n          value: \"0\"\n        - name: LOGLEVEL\n          value: \"INFO\"\n        - name: RANK\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n        command:\n        - bash\n        - -xc\n        - |\n          printenv\n          torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT --node_rank=$RANK mnist.py --epochs=1 --log-interval=1 \n</code></pre>"},{"location":"workflow-orchestration/indexed-job/#5-run-the-training-job","title":"5. Run the training job","text":"<p>Run the following command to create the Kubernetes resources we defined above and run the training job:</p> <pre><code>kubectl apply -f mnist.yaml\n</code></pre> <p>You should see 4 pods created (note the container image is large and may take a few minutes to pull before the container starts running):</p> <pre><code>$ kubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\npytorchworker-0-bbsmk   0/1     ContainerCreating   0          15s\npytorchworker-1-92tbl   0/1     ContainerCreating   0          15s\npytorchworker-2-nbrgf   0/1     ContainerCreating   0          15s\npytorchworker-3-rsrdf   0/1     ContainerCreating   0          15s\n</code></pre>"},{"location":"workflow-orchestration/indexed-job/#4-observe-training-logs","title":"4. Observe training logs","text":"<p>Once the pods transition from the <code>ContainerCreating</code> status to the <code>Running</code> status, you can observe the training logs by examining the pod logs.</p> <pre><code>$ kubectl logs -f pytorchworker-1\n\n+ torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=pytorchworker-0.headless-svc --master_port=3389 --node_rank=1 mnist.py --epochs=1 --log-interval=1\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 90162259.46it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 33279036.76it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 23474415.33it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 19165521.90it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.297087\nTrain Epoch: 1 [64/60000 (0%)]  Loss: 2.550339\nTrain Epoch: 1 [128/60000 (1%)] Loss: 2.361300\n...\n\nTrain Epoch: 1 [14912/60000 (99%)]      Loss: 0.051500\nTrain Epoch: 1 [5616/60000 (100%)]      Loss: 0.209231\n235it [00:36,  6.51it/s]\n\nTest set: Average loss: 0.0825, Accuracy: 9720/10000 (97%)\n\nINFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\nINFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish\nINFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0015289783477783203 seconds\n</code></pre>"},{"location":"workflow-orchestration/jobset/pytorch/","title":"Running distributed ML training workloads on GKE using the JobSet API","text":"<p>In this guide you will run a distributed ML training workload on GKE using the JobSet API. Specifically, you will train a handwritten digit image classifier on the classic MNIST dataset using PyTorch. The training computation will be distributed across 4 nodes in a GKE cluster.</p>"},{"location":"workflow-orchestration/jobset/pytorch/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud account set up.</li> <li>gcloud command line tool installed and configured to use your GCP project.</li> <li>kubectl command line utility is installed.</li> </ul>"},{"location":"workflow-orchestration/jobset/pytorch/#1-create-a-gke-cluster-with-4-nodes","title":"1. Create a GKE cluster with 4 nodes","text":"<p>Run the command: </p> <p><code>gcloud container clusters create jobset-cluster --zone us-central1-c --num_nodes=4</code></p> <p>You should see output indicating the cluster is being created (this can take ~10 minutes or so).</p>"},{"location":"workflow-orchestration/jobset/pytorch/#2-install-the-jobset-crd-on-your-cluster","title":"2. Install the JobSet CRD on your cluster","text":"<p>Follow the JobSet installation guide.</p>"},{"location":"workflow-orchestration/jobset/pytorch/#3-apply-the-pytorch-mnist-example-jobset","title":"3. Apply the PyTorch MNIST example JobSet","text":"<p>Run the command: </p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/jobset/main/examples/pytorch/cnn-mnist/mnist.yaml\n\njobset.jobset.x-k8s.io/pytorch created\n</code></pre> <p>You should see 4 pods created (note the container image is large and may take a few minutes to pull before the container starts running):</p> <pre><code>$ kubectl get pods\n\nNAME                        READY   STATUS              RESTARTS   AGE\npytorch-workers-0-0-ph645   0/1     ContainerCreating   0          6s\npytorch-workers-0-1-mddhj   0/1     ContainerCreating   0          6s\npytorch-workers-0-2-z9ffc   0/1     ContainerCreating   0          6s\npytorch-workers-0-3-f9ps4   0/1     ContainerCreating   0          6s\n</code></pre>"},{"location":"workflow-orchestration/jobset/pytorch/#4-observe-training-logs","title":"4. Observe training logs","text":"<p>You can observe the training logs by examining the pod logs.</p> <pre><code>$ kubectl logs -f pytorch-workers-0-1-drvk6 \n\n+ torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=pytorch-workers-0-0.pytorch --master_port=3389 --node_rank=1 mnist.py --epochs=1 --log-interval=1\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 90162259.46it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 33279036.76it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 23474415.33it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 19165521.90it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.297087\nTrain Epoch: 1 [64/60000 (0%)]  Loss: 2.550339\nTrain Epoch: 1 [128/60000 (1%)] Loss: 2.361300\n...\n</code></pre>"}]}