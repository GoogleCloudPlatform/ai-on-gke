{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI on GKE Assets","text":"<p>This repository contains assets related to AI/ML workloads on Google Kubernetes Engine (GKE).</p>"},{"location":"#overview","title":"Overview","text":"<p>Run optimized AI/ML workloads with Google Kubernetes Engine (GKE) platform orchestration capabilities. A robust AI/ML platform considers the following layers:</p> <ul> <li>Infrastructure orchestration that support GPUs and TPUs for training and serving workloads at scale</li> <li>Flexible integration with distributed computing and data processing frameworks</li> <li>Support for multiple teams on the same infrastructure to maximize utilization of resources</li> </ul>"},{"location":"#infrastructure","title":"Infrastructure","text":"<p>The AI-on-GKE application modules assumes you already have a functional GKE cluster. If not, follow the instructions under infrastructure/README.md to install a Standard or Autopilot GKE cluster.</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 infrastructure\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 platform.tfvars\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 versions.tf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u2514\u2500\u2500 tutorial.md\n</code></pre> <p>To deploy new GKE cluster update the <code>platform.tfvars</code> file with the appropriate values and then execute below terraform commands:</p> <pre><code>terraform init\nterraform apply -var-file platform.tfvars\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>The repo structure looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 applications\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u2514\u2500\u2500 ray\n\u251c\u2500\u2500 contributing.md\n\u251c\u2500\u2500 dcgm-on-gke\n\u2502   \u251c\u2500\u2500 grafana\n\u2502   \u2514\u2500\u2500 quickstart\n\u251c\u2500\u2500 gke-a100-jax\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 build_push_container.sh\n\u2502   \u251c\u2500\u2500 kubernetes\n\u2502   \u2514\u2500\u2500 train.py\n\u251c\u2500\u2500 gke-batch-refarch\n\u2502   \u251c\u2500\u2500 01_gke\n\u2502   \u251c\u2500\u2500 02_platform\n\u2502   \u251c\u2500\u2500 03_low_priority\n\u2502   \u251c\u2500\u2500 04_high_priority\n\u2502   \u251c\u2500\u2500 05_compact_placement\n\u2502   \u251c\u2500\u2500 06_jobset\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cloudbuild-create.yaml\n\u2502   \u251c\u2500\u2500 cloudbuild-destroy.yaml\n\u2502   \u251c\u2500\u2500 create-platform.sh\n\u2502   \u251c\u2500\u2500 destroy-platform.sh\n\u2502   \u2514\u2500\u2500 images\n\u251c\u2500\u2500 gke-disk-image-builder\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cli\n\u2502   \u251c\u2500\u2500 go.mod\n\u2502   \u251c\u2500\u2500 go.sum\n\u2502   \u251c\u2500\u2500 imager.go\n\u2502   \u2514\u2500\u2500 script\n\u251c\u2500\u2500 gke-dws-examples\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 dws-queues.yaml\n\u2502   \u251c\u2500\u2500 job.yaml\n\u2502   \u2514\u2500\u2500 kueue-manifests.yaml\n\u251c\u2500\u2500 gke-online-serving-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 gke-tpu-examples\n\u2502   \u251c\u2500\u2500 single-host-inference\n\u2502   \u2514\u2500\u2500 training\n\u251c\u2500\u2500 indexed-job\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 mnist.py\n\u251c\u2500\u2500 jobset\n\u2502   \u2514\u2500\u2500 pytorch\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u251c\u2500\u2500 saxml-on-gke\n\u2502   \u251c\u2500\u2500 httpserver\n\u2502   \u2514\u2500\u2500 single-host-inference\n\u251c\u2500\u2500 training-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 tutorial.md\n\u2514\u2500\u2500 tutorials\n    \u251c\u2500\u2500 e2e-genai-langchain-app\n    \u251c\u2500\u2500 finetuning-llama-7b-on-l4\n    \u2514\u2500\u2500 serving-llama2-70b-on-l4-gpus\n</code></pre>"},{"location":"#jupyter-hub","title":"Jupyter Hub","text":"<p>This repository contains a Terraform template for running JupyterHub on Google Kubernetes Engine. We've also included some example notebooks ( under <code>applications/ray/example_notebooks</code>), including one that serves a GPT-J-6B model with Ray AIR (see here for the original notebook). To run these, follow the instructions at applications/ray/README.md to install a Ray cluster.</p> <p>This jupyter module deploys the following resources, once per user: - JupyterHub deployment - User namespace - Kubernetes service accounts</p> <p>Learn more about JupyterHub on GKE here</p>"},{"location":"#ray","title":"Ray","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine.</p> <p>This module deploys the following, once per user: - User namespace - Kubernetes service accounts - Kuberay cluster - Prometheus monitoring - Logging container</p> <p>Learn more about Ray on GKE here</p>"},{"location":"#important-considerations","title":"Important Considerations","text":"<ul> <li>Make sure to configure terraform backend to use GCS bucket, in order to persist terraform state across different environments.</li> </ul>"},{"location":"#licensing","title":"Licensing","text":"<ul> <li>The use of the assets contained in this repository is subject to compliance with Google's AI Principles</li> <li>See LICENSE</li> </ul>"},{"location":"applications/jupyter/","title":"JupyterHub on GKE","text":"<p>This repository contains a Terraform template for running JupyterHub on Google Kubernetes Engine.</p> <p>We've also included some example notebooks (<code>applications/ray/example_notebooks</code>), including one that serves a GPT-J-6B model with Ray AIR (see here for the original notebook). To run these, follow the instructions at <code>applications/ray/README.md</code> to install a Ray cluster.</p> <p>This module deploys the following resources, once per user: * JupyterHub deployment * User namespace * Kubernetes service accounts</p>"},{"location":"applications/jupyter/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>GCP Project with following APIs enabled</p> <ul> <li>container.googleapis.com</li> <li>gkehub.googleapis.com (required when using private clusters with Anthos Connect Gateway)</li> <li>iap.googleapis.com (required when using authentication with Identity Aware Proxy)</li> </ul> </li> <li> <p>A functional GKE cluster.</p> <ul> <li>To create a new standard or autopilot cluster, follow the instructions in <code>infrastructure/README.md</code></li> <li>Alternatively, you can set the <code>create_cluster</code> variable to true in <code>workloads.tfvars</code> to provision a new GKE cluster. This will default to creating a GKE Autopilot cluster; if you want to provision a standard cluster you must also set <code>autopilot_cluster</code> to false.</li> </ul> </li> <li> <p>This module is configured to use Identity Aware Proxy (IAP) as default authentication method for JupyterHub. It expects the brand &amp; the OAuth consent configured in your org. You can check the details here: OAuth consent screen</p> </li> </ol> <p>This code can also perform auto brand creation. Please check the details below</p> <ol> <li>Preinstall the following on your computer:<ul> <li>Terraform</li> <li>Gcloud CLI</li> </ul> </li> </ol> <p>JupyterHub server can use either local storage or GCS to store notebooks and other artifcts.  To use GCS, create a bucket with your username. For example, when authenticating with IAP as username@domain.com, ensure your bucket name is <code>gcsfuse-&lt;username&gt;</code></p>"},{"location":"applications/jupyter/#installation","title":"Installation","text":""},{"location":"applications/jupyter/#configure-inputs","title":"Configure Inputs","text":"<ol> <li>If needed, clone the repo</li> </ol> <pre><code> git clone https://github.com/GoogleCloudPlatform/ai-on-gke\n cd ai-on-gke/applications/jupyter\n ```\n\n2. Edit `workloads.tfvars` with your GCP settings. The `namespace` that you specify will become a K8s namespace for your JupyterHub services. For more information about what the variables do visit [here](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/applications/jupyter/variable_definitions.md)\n\n**Important Note:**\nIf using this with the Ray module (`applications/ray/`), it is recommended to use the same k8s namespace\nfor both i.e. set this to the same namespace as `applications/ray/workloads.tfvars`.\n\n| Variable                    | Description                                                                                                    | Required |\n|-----------------------------|----------------------------------------------------------------------------------------------------------------|:--------:|\n| project_id                  | GCP Project Id                                                                                                 | Yes      |\n| cluster_name                | GKE Cluster Name                                                                                               | Yes      |\n| cluster_location            | GCP Region                                                                                                     | Yes      |\n| cluster_membership_id       | Fleet membership name for GKE cluster. &lt;br /&gt; Required when using private clusters with Anthos Connect Gateway | |\n| namespace                   | The namespace that JupyterHub and rest of the other resources will be installed in.                            | Yes      |\n| gcs_bucket                  | GCS bucket to be used for Jupyter storage                                                                      |       |\n| create_service_account      | Create service accounts used for Workload Identity mapping                                                     | Yes      |\n| gcp_and_k8s_service_account | GCP service account used for Workload Identity mapping and k8s sa attached with workload                       | Yes      |\n\nFor variables under `JupyterHub with IAP`, please see the section below \n\n### Secure endpoint with IAP\n\n&gt; **_NOTE:_** To secure the Jupyter endpoint, this module enables IAP by default. It is _strongly recommended_ to keep this configuration. If you wish to disable it, do the following: set the `add_auth` flag to false in the `workloads.tf` file.\n\n3. If you already have a brand setup for your project, use the existing values to fill in the variable values in workloads.tf\n\n4. If you have not enabled the IAP API before or created a Brand for your project, please follow these steps:\n\n    - Navigate to the `brand` [page](https://console.cloud.google.com/apis/credentials/consent) to create your own brand:\n\n    See [here](#auto-brand-creation-and-iap-enablement) for more information about how to create a brand automatically. Please note, auto brand creation enables the application only for [internal (within the org) users](https://cloud.google.com/iap/docs/programmatic-oauth-clients#branding). This can be switched to external users from the [consent](https://console.cloud.google.com/apis/credentials/consent) screen.\n\nSee the example `.tfvars` files under `/applications/jupyter` for different brand/IAP configurations.\n\n| Variable                 | Description                | Default Value | Required |\n| ------------------------ |--------------------------- |:-------------:|:--------:|\n| add_auth                 | Enable IAP on JupyterHub   | true          | Yes      |\n| brand                    | Name of the brand used for creating IAP OAuth clients. Only one is allowed per project. View existing brands: `gcloud iap oauth-brands list`. Leave it empty to create a new brand.  Uses [support_email](#support_email) |           |       |\n| support_email            | Support email assocated with the [brand](#brand). Used as a point of contact for consent for the [\"OAuth Consent\" in Cloud Console](https://console.cloud.google.com/apis/credentials/consent). Optional field if `brand` is empty.   |           |       |\n| default_backend_service  | default_backend_service   |           |       |\n| service_name             | Name of the Backend Service that gets created when enabling IAP.   |           |       |\n| url_domain_addr          | Provided by the user if they want to bring their own URL/Domain. Used by the IAP resources if filled in. Filling this in will disable automatic global IP reservation. Must also fill in [url_domain_name](#url_domain_name).   |           |       |\n| url_domain_name          | This variable will only be used if [url_domain_addr](#url_domain_addr) is provided. It is the name associated with the domain provided by the user. Since we are using Ingress, it will require the `kubernetes.io/ingress.global-static-ip-name` annotation along with the name associated.   |           |       |\n| client_id                | Client ID of an [OAuth 2.0 Client ID](https://console.cloud.google.com/apis/credentials) created by the user for enabling IAP. You must also input the [client_secret](#client_secret). If this variable is unset, the template will create an OAuth client for you - in this case, you must ensure the associated [brand](https://console.cloud.google.com/apis/credentials/consent) is `Internal` i.e. only principals within the organization can access the application.   |           |       |\n| client_secret            | Client Secret associated with the [client_id](#client_id). This variable will only be used when the client id is filled out.     |           |       |\n| members_allowlist        | Comma seperated values for users to be allowed access through IAP. Example values: `user:username@domain.com`  |      |       |\n\n\n### Install\n\n&gt; **_NOTE:_** Terraform keeps state metadata in a local file called `terraform.tfstate`. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using `terraform destroy` before reapplying/reinstalling.\n\n5. Ensure your gcloud application default credentials are in place. \n</code></pre> <p>gcloud auth application-default login</p> <pre><code>\n6. Run `terraform init`\n\n7. Run `terraform apply --var-file=./workloads.tfvars`. It can take upto 5 minutes on standard clusters &amp; upto 10 minutes on AutoPilot clusters. Due to some IAP limitations, this is expected to fail with an error `Error retrieving IAM policy for iap webbackendservice` which will be resolved by the next step.\n\n8. If using authentication with IAP (i.e. `add_auth = true`), rerun terraform apply again. This is needed to configure Jupyter with IAP correctly.\n\n    * Verify the backend service for IAP has been created (takes 5-10 mins) with `gcloud compute backend-services list`\n        - Should have `jupyter-proxy-public` in the name eg.: `k8s1-63da503a-jupyter-proxy-public-80-74043627`.\n    * Run `terraform apply --var-file=./workloads.tfvars`\n\n## Using JupyterHub\n\n### If Auth with IAP is disabled\n\n1. Extract the randomly generated password for JupyterHub login\n\n</code></pre> <p>terraform output jupyterhub_password ```</p> <ol> <li>Setup port forwarding for the frontend: <code>kubectl port-forward service/proxy-public -n &lt;namespace&gt; 8081:80 &amp;</code>, and open <code>localhost:8081</code> in a browser.</li> </ol>"},{"location":"applications/jupyter/#if-auth-with-iap-is-enabled","title":"If Auth with IAP is enabled","text":"<ol> <li> <p>Note down the value for the <code>domain</code> from the terraform output section: <code>terraform output domain</code>. You can open this in a browser &amp; login with your credentials. Alternatively, domain value for Jupyter Ingress can be found on Certificate Manager page.</p> </li> <li> <p>Ensure the managed cert for the domain has finished provisioning: <code>kubectl get managedcertificate -n &lt;namespace&gt;</code>. This can take 10 - 20 minutes. You may see an SSL error if you try to hit the domain when the cert isn't <code>Active</code>.</p> </li> <li> <p>Open the external IP in a browser and login. If you get an access error, see the <code>Setup Access</code> section below. Please note there may be some propagation delay after adding IAP principals (5-10 mins).</p> </li> <li> <p>Select profile and open a Jupyter Notebook</p> </li> </ol> <p>NOTE: Domain specific managed certificate may take some time to finish provisioning. This can take between 10-15 minutes. The browser may not display the login page correctly until the certificate provisioning is complete.</p>"},{"location":"applications/jupyter/#setup-access","title":"Setup Access","text":"<p>In order for users to login to JupyterHub via IAP, their access needs to be configured. To allow access for users/groups: </p> <ol> <li> <p>Navigate to the GCP IAP Cloud Console and select your backend-service for <code>&lt;namespace&gt;/proxy-public</code>.</p> </li> <li> <p>Click on <code>Add Principal</code>, insert the username / group name and select under <code>Cloud IAP</code> with role <code>IAP-secured Web App User</code>. Once presmission is granted, these users / groups can login to JupyterHub with IAP. Please note there may be some propagation delay after adding IAP principals (5-10 mins).</p> </li> </ol>"},{"location":"applications/jupyter/#persistent-storage","title":"Persistent Storage","text":"<p>JupyterHub is configured to provide 2 choices for storage:</p> <ol> <li> <p>Default JupyterHub Storage - <code>pd.csi.storage.gke.io</code> with reclaim policy <code>Delete</code></p> </li> <li> <p>GCSFuse - <code>gcsfuse.csi.storage.gke.io</code> uses GCS Buckets and require users to pre-create buckets with name format <code>gcsfuse-{username}</code></p> </li> </ol> <p>For more information about Persistent storage and the available options, visit here</p>"},{"location":"applications/jupyter/#running-gpt-j-6b","title":"Running GPT-J-6B","text":"<p>This example is adapted from Ray AIR's examples here.</p> <ol> <li> <p>Open the <code>gpt-j-online.ipynb</code> notebook under <code>ai-on-gke/applications/ray/example_notebooks</code>.</p> </li> <li> <p>Open a terminal in the Jupyter session and install Ray AIR:</p> <p><code>cmd pip install ray[air]</code></p> </li> <li> <p>Run through the notebook cells. You can change the prompt in the last cell:</p> <p><code>jupyter prompt = (     ## Input your own prompt here )</code></p> </li> <li> <p>This should output a generated text response.</p> </li> </ol>"},{"location":"applications/jupyter/#auto-brand-creation-and-iap-enablement","title":"Auto Brand creation and IAP enablement","text":"<p>IMPORTANT If you enable automatic brand creation, only <code>Internal</code> brand will be created, allowing only the users under the same org as the project to access the application. Make sure Policy for Restrict Load Balancer Creation Based on Load Balancer Types allows EXTERNAL_HTTP_HTTPS.</p> <p>Ensure that the following variables within <code>workloads.tfvars</code> are set:</p> <ul> <li>enable_iap_service - Enables the IAP service API. Leave as false if IAP is enabled before.</li> <li>brand - creates a brand for the project. Only one is currently allowed per project. Leave it empty to create a new brand</li> <li>support_email - used by brand, required field.</li> <li>IMPORTANT client_id and client_secret - If your brand is <code>external</code>, you must provide your own client_id and client_secret. If your brand is <code>internal</code>, you can choose to leave the variable as is and allow terraform to create one for you.</li> <li>if you do bring your own OAuth client, you must add to the <code>Authorized redirect URIs</code> Field:  <code>https://iap.googleapis.com/v1/oauth/clientIds/&lt;client ID&gt;:handleRedirect</code></li> </ul> <p>Note: You can use a custom domain &amp; existing ingress ip address in the <code>workloads.tfvars</code> file.</p>"},{"location":"applications/jupyter/#additional-information","title":"Additional Information","text":"<p>For more information about JupyterHub profiles and the preset profiles visit here</p>"},{"location":"applications/jupyter/profiles/","title":"JupyterHub Profiles","text":""},{"location":"applications/jupyter/profiles/#default-profiles","title":"Default Profiles","text":"<p>By default, there are 3 pre-set profiles for JupyterHub:</p> <p></p> <p>As the description for each profiles explains, each profiles uses a different resource.</p> <ol> <li> <p>First profile uses CPUs and uses the image: <code>jupyter/tensorflow-notebook</code> with tag <code>python-3.10</code></p> </li> <li> <p>Second profile uses TPUs and will only work when TPU is enabled on the cluster. It uses the image <code>jupyter/tensorflow-notebook</code> with tag <code>python-3.10</code></p> </li> <li> <p>Second profile uses 2 T4 GPUs and the image: <code>jupyter/tensorflow-notebook:python-3.10</code> with tag <code>python-3.10</code> [^1]</p> </li> <li> <p>Third profile uses 2 A100 GPUs and the image: <code>jupyter/tensorflow-notebook:python-3.10</code> with tag <code>python-3.10</code> [^1]</p> </li> </ol>"},{"location":"applications/jupyter/profiles/#editting-profiles","title":"Editting profiles","text":"<p>You can change the image used by these profiles, change the resources, and add specific hooks to the profiles</p> <p>Within the <code>config.yaml</code>, the profiles sit under the <code>singleuser</code> key:</p> <pre><code>singleuser:\n    cpu:\n        ...\n    memory:\n        ...\n    image:\n        name: jupyter/tensorflow-notebook\n        tag: python-3.10\n    ...\n    profileList:\n    ...\n</code></pre>"},{"location":"applications/jupyter/profiles/#image","title":"Image","text":"<p>The default image used by all three of the profiles is <code>jupyter/tensorflow-notebook:python-3.10</code></p> <ol> <li> <p>For profile 1, it uses the: <code>default: true</code> field. This means that all the default configs under <code>singleuser</code> are used.</p> </li> <li> <p>For profile 2 and 3, the images are defined under <code>kubespawner_override</code></p> </li> </ol> <pre><code>    display_name: \"Profile2 name\"\n        description: \"description here\"\n        kubespawner_override:\n            image: jupyter/tensorflow-notebook:python-3.10\n    ...\n</code></pre> <p>Kubespanwer_override is a dictionary with overrides that gets applied through the Kubespawner. [^2]</p> <p>More images of tensorflow can be found here</p>"},{"location":"applications/jupyter/profiles/#resources","title":"Resources","text":"<p>Each of the users get a part of the memory and CPU and the resources are by default:</p> <pre><code>    cpu:\n        limit: .5\n        guarantee: .5\n    memory:\n        limit: 1G\n        guarantee: 1G\n</code></pre> <p>The limit if the reousrce sets a hard limit on how much of that resource can the user have. The guarantee meaning the least amount of resource that will be available to the user at all times.</p> <p>Similar to overriding images, the resources can also be overwritten by using <code>kubespawner_override</code>:</p> <pre><code>    kubespawner_override:\n        cpu_limit: .7\n        cpu_guarantee: .7\n        mem_limit: 2G\n        mem_guarantee: 2G\n        nvidia.com/gpu: \"2\"\n</code></pre>"},{"location":"applications/jupyter/profiles/#nodegpu","title":"Node/GPU","text":"<p>JupyterHub config allows the use of nodeSelector. This is the way the profiles specify which node/GPU it wants</p> <pre><code>nodeSelector:\n    iam.gke.io/gke-metadata-server-enabled: \"true\"\n    cloud.google.com/gke-accelerator: \"nvidia-tesla-t4\"\n</code></pre> <p>Override using <code>kubespwaner_override</code>:</p> <pre><code>    kubespawner_override:\n        node_selector:\n          cloud.google.com/gke-accelerator: \"nvidia-tesla-a100\"\n</code></pre> <p>The possible GPUs are:</p> <ol> <li>nvidia-tesla-k80</li> <li>nvidia-tesla-p100</li> <li>nvidia-tesla-p4</li> <li>nvidia-tesla-v100</li> <li>nvidia-tesla-t4</li> <li>nvidia-tesla-a100</li> <li>nvidia-a100-80gb</li> <li>nvidia-l4</li> </ol>"},{"location":"applications/jupyter/profiles/#tpus","title":"TPUs","text":"<p>JupyterHub profiles has a TPU option. It utilizes the nodeSelector, and the annotations <code>cloud.google.com/gke-tpu-accelerator</code> and <code>cloud.google.com/gke-tpu-topology</code> to select the TPU nodes.</p> <p>We add the following annotations to <code>kupespawner_override</code></p> <pre><code>extra_resource_limits:\n    google.com/tpu: \"4\"\nnode_selector:\n    cloud.google.com/gke-tpu-accelerator: \"tpu-v4-podslice\"\n    cloud.google.com/gke-tpu-topology: \"2x2x1\"\n</code></pre> <p>Currently the template only provisions a <code>ct4p-hightpu-4t</code> TPU machine with placement <code>2x2x1</code>.</p>"},{"location":"applications/jupyter/profiles/#example-profile","title":"Example profile","text":"<p>Example of a profile that overrides the default values:</p> <pre><code>  - display_name: \"Learning Data Science\"\n    description: \"Datascience Environment with Sample Notebooks\"\n    kubespawner_override:\n        cpu_limit: .5\n        cpu_guarantee: .5\n        mem_limit: 1G\n        mem_guarantee: 1G\n    image: jupyter/datascience-notebook:2343e33dec46\n    lifecycle_hooks:\n        postStart:\n        exec:\n            command:\n            - \"sh\"\n            - \"-c\"\n            - &gt;\n                gitpuller https://github.com/data-8/materials-fa17 master materials-fa;\n</code></pre>"},{"location":"applications/jupyter/profiles/#additional-overrides","title":"Additional Overrides","text":"<p>With <code>kubespanwer_override</code> there are additional overrides that could be done, including <code>lifecycle_hooks</code>, <code>storage_capacity</code>, and <code>storage class</code> Fields can be found here</p> <p>[^1]: If using Standard clusters, the cluster must have at least 2 of the GPU type ready [^2]: More information on Kubespawner here</p>"},{"location":"applications/jupyter/storage/","title":"Persistent Storage","text":""},{"location":"applications/jupyter/storage/#gcsfuse","title":"GCSFuse","text":"<p>Important Note: To use option, a GCS bucket must already be created within the project with the name in the format of <code>gcsfuse-{username}</code></p> <p>GCSFuse allow users to mount GCS Buckets as their local filesystem. This option allows ease of access on Cloud UI:</p> <p></p> <p>Since this bucket in GCS, there is built in permission control and access outside of the clutser.</p>"},{"location":"applications/jupyter/variable_definitions/","title":"Glossary for variables","text":"<p>This README contains all the variables used by Terraform for installing Juypterhub on the GKE cluster.</p>"},{"location":"applications/jupyter/variable_definitions/#table-of-contents","title":"Table of Contents","text":"<ul> <li>namespace</li> <li>create_namespace</li> <li>add_auth</li> <li>project_id</li> <li>location</li> <li>service_name</li> <li>enable_iap_service</li> <li>brand</li> <li>support_email</li> <li>url_domain_addr</li> <li>url_domain_name</li> </ul>"},{"location":"applications/jupyter/variable_definitions/#namespace","title":"namespace","text":"<p>The namespace that JupyterHub and rest of the other resources will be installed/allocated in. If using JupyterHub with the Ray module (<code>ai-on-gke/ray-on-gke/</code>), it is recommanded to have this namespace the same as the one with Ray.</p>"},{"location":"applications/jupyter/variable_definitions/#create_service_account","title":"create_service_account","text":"<p>Create k8s and GCP service accounts for JupyterHub workloads &amp; configures workload identity.</p>"},{"location":"applications/jupyter/variable_definitions/#add_auth","title":"add_auth","text":"<p>Flag that will enable IAP on JupyterHub. Resources that will be created along with enable IAP:     1. Global IP Address (If none is provided)     2. Backend Config. Deployment that triggers enabling IAP.     3. Managed Certificate. Deployment that creates a Google Managed object for SSL certificates     4. Ingress. Deployment that creates an Ingress object that will connect to the JupyterHub Proxy</p>"},{"location":"applications/jupyter/variable_definitions/#project_id","title":"project_id","text":"<p>Name of the project where the cluster lives. Used to retrieve the project number as well as used in numerous resources.</p>"},{"location":"applications/jupyter/variable_definitions/#location","title":"location","text":"<p>Location of the GKE cluster. Used by the terraform provider.</p>"},{"location":"applications/jupyter/variable_definitions/#service_name","title":"service_name","text":"<p>Name of the Backend Service that gets created when enabling IAP.</p>"},{"location":"applications/jupyter/variable_definitions/#enable_iap_service","title":"enable_iap_service","text":"<p>Flag that will enable the IAP Service API for the user on the project. If it is already enabled, leave it as false.</p>"},{"location":"applications/jupyter/variable_definitions/#brand","title":"brand","text":"<p>Name of the brand used for creating IAP OAuth clients. Currently only one is allowed per project. If there is already a brand, leave it empty. Uses support_email</p>"},{"location":"applications/jupyter/variable_definitions/#support_email","title":"support_email","text":"<p>Support email assocated with the brand. Used as a point of contact for consent for the \"OAuth Consent\" in Cloud Console. It will not be used if brand is empty.</p>"},{"location":"applications/jupyter/variable_definitions/#url_domain_addr","title":"url_domain_addr","text":"<p>Provided by the user if they want to bring their own URL/Domain. Used by the IAP resources if filled in. Filling this in will disable automatic global IP reservation. Must also fill in url_domain_name.</p>"},{"location":"applications/jupyter/variable_definitions/#url_domain_name","title":"url_domain_name","text":"<p>This variable will only be used if url_domain_addr is provided. It is the name associated with the domain provided by the user. Since we are using Ingress, it will require the <code>kubernetes.io/ingress.global-static-ip-name</code> annotation along with the name associated.</p>"},{"location":"applications/jupyter/variable_definitions/#client_id","title":"client_id","text":"<p>Client ID of an OAuth client created by the user for enabling IAP. When this variable is not empty, the template will not create an OAuth client for you. You must also input the client_secret.</p>"},{"location":"applications/jupyter/variable_definitions/#client_secret","title":"client_secret","text":"<p>Client Secret associated with the client ID. This variable will only be used when the client id is filled out.  </p>"},{"location":"applications/rag/","title":"RAG on GKE","text":"<p>This is a sample to deploy a Retrieval Augmented Generation (RAG) application on GKE. </p> <p>The latest recommended release is branch release-1.1.</p>"},{"location":"applications/rag/#what-is-rag","title":"What is RAG?","text":"<p>RAG is a popular approach for boosting the accuracy of LLM responses, particularly for domain specific or private data sets.</p> <p>RAG uses a semantically searchable knowledge base (like vector search) to retrieve relevant snippets for a given prompt to provide additional context to the LLM. Augmenting the knowledge base with additional data is typically cheaper than fine tuning and is more scalable when incorporating current events and other rapidly changing data spaces.</p>"},{"location":"applications/rag/#rag-on-gke-architecture","title":"RAG on GKE Architecture","text":"<ol> <li>A GKE service endpoint serving Hugging Face TGI inference using <code>mistral-7b</code>.</li> <li>Cloud SQL <code>pgvector</code> instance with vector embeddings generated from an input dataset.</li> <li>A Ray cluster running on GKE that runs jobs to generate embeddings and populate the vector DB.</li> <li>A Jupyter notebook running on GKE that reads the dataset using GCS fuse driver integrations and runs a Ray job to populate the vector DB.</li> <li>A front end chat interface running on GKE that prompts the inference server with context from the vector DB.</li> </ol> <p>This tutorial walks you through installing the RAG infrastructure in a GCP project, generating vector embeddings for a sample Kaggle Netflix shows dataset and prompting the LLM with context.</p>"},{"location":"applications/rag/#prerequisites","title":"Prerequisites","text":""},{"location":"applications/rag/#install-tooling-required","title":"Install tooling (required)","text":"<p>Install the following on your computer: * Kubectl * Terraform * Helm * Gcloud</p>"},{"location":"applications/rag/#bring-your-own-cluster-optional","title":"Bring your own cluster (optional)","text":"<p>By default, this tutorial creates a cluster on your behalf. We highly recommend following the default settings.</p> <p>If you prefer to manage your own cluster, set <code>create_cluster = false</code> and make sure the <code>network_name</code> is set to your cluster's network in the Installation section. Creating a long-running cluster may be better for development, allowing you to iterate on Terraform components without recreating the cluster every time.</p> <p>Use gcloud to create a GKE Autopilot cluster. Note that RAG requires the latest Autopilot features, available on the latest versions of 1.28 and 1.29.</p> <pre><code>gcloud container clusters create-auto rag-cluster \\\n  --location us-central1 \\\n  --cluster-version 1.28\n</code></pre>"},{"location":"applications/rag/#bring-your-own-vpc-optional","title":"Bring your own VPC (optional)","text":"<p>By default, this tutorial creates a new network on your behalf with Private Service Connect already enabled. We highly recommend following the default settings.</p> <p>If you prefer to use your own VPC, set <code>create_network = false</code> in the in the Installation section. This also requires enabling Private Service Connect for your VPC. Without Private Service Connect, the RAG components cannot connect to the vector DB:</p> <ol> <li> <p>Create an IP allocation</p> </li> <li> <p>Create a private connection.</p> </li> </ol>"},{"location":"applications/rag/#installation","title":"Installation","text":"<p>This section sets up the RAG infrastructure in your GCP project using Terraform.</p> <p>NOTE: Terraform keeps state metadata in a local file called <code>terraform.tfstate</code>. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using <code>terraform destroy</code> before reapplying/reinstalling.</p> <ol> <li> <p><code>cd ai-on-gke/applications/rag</code></p> </li> <li> <p>Edit <code>workloads.tfvars</code> to set your project ID, location, cluster name, and GCS bucket name. Ensure the <code>gcs_bucket</code> name is globally unique (add a random suffix). Optionally, make the following changes:</p> <ul> <li>(Recommended) Enable authenticated access for JupyterHub, frontend chat and Ray dashboard services.</li> <li>(Optional) Set a custom <code>kubernetes_namespace</code> where all k8s resources will be created.</li> <li>(Optional) Set <code>autopilot_cluster = false</code> to deploy using GKE Standard.</li> <li>(Optional) Set <code>create_cluster = false</code> if you are bringing your own cluster. If using a GKE Standard cluster, ensure it has an L4 nodepool with autoscaling and node autoprovisioning enabled. You can simplify setup by following the Terraform instructions in <code>infrastructure/README.md</code>.</li> <li>(Optional) Set <code>create_network = false</code> if you are bringing your own VPC. Ensure your VPC has Private Service Connect enabled as described above.</li> </ul> </li> <li> <p>Run <code>terraform init</code></p> </li> <li> <p>Run <code>terraform apply --var-file workloads.tfvars</code></p> </li> </ol>"},{"location":"applications/rag/#generate-vector-embeddings-for-the-dataset","title":"Generate vector embeddings for the dataset","text":"<p>This section generates the vector embeddings for your input dataset. Currently, the default dataset is Netflix shows. We will use a Jupyter notebook to run a Ray job that generates the embeddings &amp; populates them into the <code>pgvector</code> instance created above.</p> <p>Set your the namespace, cluster name and location from <code>workloads.tfvars</code>):</p> <pre><code>export NAMESPACE=rag\nexport CLUSTER_LOCATION=us-east4\nexport CLUSTER_NAME=rag-cluster\n</code></pre> <p>Connect to the GKE cluster:</p> <pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${CLUSTER_LOCATION}\n</code></pre> <ol> <li>Connect and login to JupyterHub:</li> <li>If IAP is disabled (<code>jupyter_add_auth = false</code>):         - Port forward to the JupyterHub service: <code>kubectl port-forward service/proxy-public -n ${NAMESPACE} 8081:80 &amp;</code>         - Go to <code>localhost:8081</code> in a browser         - Login with these credentials:             * username: admin             * password: use <code>terraform output jupyterhub_password</code> to fetch the password value</li> <li> <p>If IAP is enabled (<code>jupyter_add_auth = true</code>):         - Fetch the domain: <code>terraform output jupyterhub_uri</code>         - If you used a custom domain, ensure you configured your DNS as described above.          - Verify the domain status is <code>Active</code>:             - <code>kubectl get managedcertificates jupyter-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code>             - Note: This can take up to 20 minutes to propagate.         - Once the domain status is Active, go to the domain in a browser and login with your Google credentials.         - To add additional users to your JupyterHub application, go to Google Cloud Platform IAP, select the <code>rag/proxy-public</code> service and add principals with the role <code>IAP-secured Web App User</code>.</p> </li> <li> <p>Load the notebook:</p> <ul> <li>Once logged in to JupyterHub, choose the <code>CPU</code> preset with <code>Default</code> storage. </li> <li>Click [File] -&gt; [Open From URL] and paste: <code>https://raw.githubusercontent.com/GoogleCloudPlatform/ai-on-gke/main/applications/rag/example_notebooks/rag-kaggle-ray-sql-interactive.ipynb</code></li> </ul> </li> <li> <p>Configure Kaggle:</p> <ul> <li>Create a Kaggle account.</li> <li>Generate an API token. See further instructions. This token is used in the notebook to access the Kaggle Netflix shows dataset.</li> <li>Replace the variables in the 1st cell of the notebook with your Kaggle credentials (can be found in the <code>kaggle.json</code> file created while generating the API token):<ul> <li><code>KAGGLE_USERNAME</code></li> <li><code>KAGGLE_KEY</code></li> </ul> </li> </ul> </li> <li> <p>Generate vector embeddings: Run all the cells in the notebook to generate vector embeddings for the Netflix shows dataset (https://www.kaggle.com/datasets/shivamb/netflix-shows) via a Ray job and store them in the <code>pgvector</code> CloudSQL instance.</p> <ul> <li>When the last cell succeeded, the vector embeddings have been generated and we can launch the frontend chat interface. Note that the Ray job can take up to 10 minutes to finish.</li> <li>Ray may take several minutes to create the runtime environment. During this time, the job will appear to be missing (e.g. <code>Status message: PENDING</code>).</li> <li>Connect to the Ray dashboard to check the job status or logs:<ul> <li>If IAP is disabled (<code>ray_dashboard_add_auth = false</code>):<ul> <li><code>kubectl port-forward -n ${NAMESPACE} service/ray-cluster-kuberay-head-svc 8265:8265</code></li> <li>Go to <code>localhost:8265</code> in a browser</li> </ul> </li> <li>If IAP is enabled (<code>ray_dashboard_add_auth = true</code>):<ul> <li>Fetch the domain: <code>terraform output ray-dashboard-managed-cert</code></li> <li>If you used a custom domain, ensure you configured your DNS as described above.</li> <li>Verify the domain status is <code>Active</code>:<ul> <li><code>kubectl get managedcertificates ray-dashboard-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>Note: This can take up to 20 minutes to propagate.</li> </ul> </li> <li>Once the domain status is Active, go to the domain in a browser and login with your Google credentials.</li> <li>To add additional users to your frontend application, go to Google Cloud Platform IAP, select the <code>rag/ray-cluster-kuberay-head-svc</code> service and add principals with the role <code>IAP-secured Web App User</code>.</li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"applications/rag/#launch-the-frontend-chat-interface","title":"Launch the frontend chat interface","text":"<ol> <li>Connect to the frontend:<ul> <li>If IAP is disabled (<code>frontend_add_auth = false</code>):<ul> <li>Port forward to the frontend service: <code>kubectl port-forward service/rag-frontend -n ${NAMESPACE} 8080:8080 &amp;</code></li> <li>Go to <code>localhost:8080</code> in a browser</li> </ul> </li> <li>If IAP is enabled (<code>frontend_add_auth = true</code>):<ul> <li>Fetch the domain: <code>terraform output frontend_uri</code></li> <li>If you used a custom domain, ensure you configured your DNS as described above.</li> <li>Verify the domain status is <code>Active</code>:<ul> <li><code>kubectl get managedcertificates frontend-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>Note: This can take up to 20 minutes to propagate.</li> </ul> </li> <li>Once the domain status is Active, go to the domain in a browser and login with your Google credentials.</li> <li>To add additional users to your frontend application, go to Google Cloud Platform IAP, select the <code>rag/rag-frontend</code> service and add principals with the role <code>IAP-secured Web App User</code>.</li> </ul> </li> </ul> </li> <li>Prompt the LLM<ul> <li>Start chatting! This will fetch context related to your prompt from the vector embeddings in the <code>pgvector</code> CloudSQL instance, augment the original prompt with the context &amp; query the inference model (<code>mistral-7b</code>) with the augmented prompt.</li> </ul> </li> </ol>"},{"location":"applications/rag/#configure-authenticated-access-via-iap-recommended","title":"Configure authenticated access via IAP (recommended)","text":"<p>We recommend you configure authenticated access via IAP for your services.</p> <p>1) Make sure the OAuth Consent Screen is configured for your project. Ensure <code>User type</code> is set to <code>Internal</code>. 2) Make sure Policy for Restrict Load Balancer Creation Based on Load Balancer Types allows EXTERNAL_HTTP_HTTPS. 3) Set the following variables in <code>workloads.tfvars</code>:     * <code>jupyter_add_auth = true</code>     * <code>frontend_add_auth = true</code>     * <code>ray_dashboard_add_auth = true</code> 4) Allowlist principals for your services via <code>jupyter_members_allowlist</code>, <code>frontend_members_allowlist</code> and <code>ray_dashboard_members_allowlist</code>. 5) Configure custom domains names via <code>jupyter_domain</code>, <code>frontend_domain</code> and <code>ray_dashboard_domain</code> for your services.  6) Configure DNS records for your custom domains:     - Register a Domain on Google Cloud Domains or use a domain registrar of your choice.     - Set up your DNS service to point to the public IP         * Run <code>terraform output frontend_ip_address</code> to get the public ip address of frontend, and add an A record in your DNS configuration to point to the public IP address.         * Run <code>terraform output jupyterhub_ip_address</code> to get the public ip address of jupyterhub, and add an A record in your DNS configuration to point to the public IP address.         * Run <code>terraform output ray_dashboard_ip_address</code> to get the public ip address of ray dashboard, and add an A record in your DNS configuration to point to the public IP address.     - Add an A record: If the DNS service of your domain is managed by Google Cloud DNS managed zone, there are two options to add the A record:         1. Go to https://console.cloud.google.com/net-services/dns/zones, select the zone and click ADD STANDARD, fill in your domain name and public IP address.         2. Run <code>gcloud dns record-sets create &lt;domain address&gt;. --zone=&lt;zone name&gt; --type=\"A\" --ttl=&lt;ttl in seconds&gt; --rrdatas=\"&lt;public ip address&gt;\"</code></p>"},{"location":"applications/rag/#cleanup","title":"Cleanup","text":"<ol> <li>Run <code>terraform destroy --var-file=\"workloads.tfvars\"</code><ul> <li>Network deletion issue: <code>terraform destroy</code> fails to delete the network due to a known issue in the GCP provider. For now, the workaround is to manually delete it.</li> </ul> </li> </ol>"},{"location":"applications/rag/#troubleshooting","title":"Troubleshooting","text":"<p>Set your the namespace, cluster name and location from <code>workloads.tfvars</code>:</p> <pre><code>export NAMESPACE=rag\nexport CLUSTER_LOCATION=us-central1\nexport CLUSTER_NAME=rag-cluster\n</code></pre> <p>Connect to the GKE cluster:</p> <pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${CLUSTER_LOCATION}\n</code></pre> <ol> <li> <p>Troubleshoot Ray job failures:</p> <ul> <li>If the Ray actors fail to be scheduled, it could be due to a stockout or quota issue.<ul> <li>Run <code>kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=kuberay</code>. There should be a Ray head and Ray worker pod in <code>Running</code> state. If your ray pods aren't running, it's likely due to quota or stockout issues. Check that your project and selected <code>cluster_location</code> have L4 GPU capacity.</li> </ul> </li> <li>Often, retrying the Ray job submission (the last cell of the notebook) helps.</li> <li>The Ray job may take 15-20 minutes to run the first time due to environment setup.</li> </ul> </li> <li> <p>Troubleshoot IAP login issues:</p> <ul> <li>Verify the cert is Active:<ul> <li>For JupyterHub <code>kubectl get managedcertificates jupyter-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>For the frontend: <code>kubectl get managedcertificates frontend-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> </ul> </li> <li>Verify users are allowlisted for JupyterHub or frontend services:<ul> <li>JupyterHub: Go to Google Cloud Platform IAP, select the <code>rag/proxy-public</code> service and check if the user has role <code>IAP-secured Web App User</code>.</li> <li>Frontend: Go to Google Cloud Platform IAP, select the <code>rag/rag-frontend</code> service and check if the user has role <code>IAP-secured Web App User</code>.</li> </ul> </li> <li>Org error:<ul> <li>The OAuth Consent Screen has <code>User type</code> set to <code>Internal</code> by default, which means principals external to the org your project is in cannot log in. To add external principals, change <code>User type</code> to <code>External</code>.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot <code>terraform apply</code> failures:</p> <ul> <li>Inference server (<code>mistral</code>) fails to deploy:<ul> <li>This usually indicates a stockout/quota issue. Verify your project and chosen <code>cluster_location</code> have L4 capacity.</li> </ul> </li> <li>GCS bucket already exists:<ul> <li>GCS bucket names have to be globally unique, pick a different name with a random suffix.</li> </ul> </li> <li>Cloud SQL instance already exists:<ul> <li>Ensure the <code>cloudsql_instance</code> name doesn't already exist in your project.</li> </ul> </li> <li>GMP operator webhook connection refused:<ul> <li>This is a rare, transient error. Run <code>terraform apply</code> again to resume deployment.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot <code>terraform destroy</code> failures:</p> <ul> <li>Network deletion issue:<ul> <li><code>terraform destroy</code> fails to delete the network due to a known issue in the GCP provider. For now, the workaround is to manually delete it.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot error: <code>Repo model mistralai/Mistral-7B-Instruct-v0.1 is gated. You must be authenticated to access it.</code> for the pod of deployment <code>mistral-7b-instruct</code>.</p> </li> </ol> <p>The error is because the RAG deployments uses <code>Mistral-7B-instruct</code> which is now a gated model on Hugging Face. Deployments fail as they require a Hugging Face authentication token, which is not part of the current workflow.    While we are actively working on long-term fix. This is how to workaround the error:     - Use the guide as a reference to create an access token.     - Go to the model card in Hugging Face and click \"Agree and access repository\"     - Create a secret as noted in with the Hugging Face credential called <code>hf-secret</code> in the name space where your <code>mistral-7b-instruct</code> deployment is running.     - Add the following entry to <code>env</code> within the deployment <code>mistral-7b-instruct</code> via <code>kubectl edit</code>.</p> <pre><code>        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hf-secret\n              key: hf_api_token\n</code></pre>"},{"location":"applications/rag/frontend/","title":"RAG-on-GKE Frontend Webserver","text":"<p>This directory contains the code for a frontend flask webserver integrating with the inference backend and vector database through LangChain.</p>"},{"location":"applications/rag/frontend/#sensitive-data-protection","title":"Sensitive Data Protection","text":"<p>Data Loss Prevention (DLP) from Sensitive Data Protection (SDP) and Text moderation System. Our integrated solution leverages the latest in data protection and language understanding technologies to safeguard sensitive information and ensure content appropriateness across digital platforms.</p>"},{"location":"applications/rag/frontend/#features","title":"Features:","text":""},{"location":"applications/rag/frontend/#data-loss-prevention-dlp","title":"Data Loss Prevention (DLP):","text":"<p>Our DLP system is engineered to actively identify and safeguard sensitive information. It achieves this through advanced detection methods that redact sensitive data from both outputs and, where applicable, within the data itself. This approach prioritizes the robust protection of critical information. Please note, the current implementation of our DLP solution does not extend support for specific data protection compliance regulations. The primary objective is to ensure a strong foundation of data security that can be customized or augmented to align with compliance requirements as needed.</p>"},{"location":"applications/rag/frontend/#text-moderation-from-cloud-natural-language-api","title":"Text moderation from Cloud Natural Language API:","text":"<p>Use text moderation capabilities from the Cloud Natural Language API to analyze user input and LLM responses for sentiment, content, and content categorization. Filter out inappropriate material based on predefined categories such as health, finance, politics, and legal.</p>"},{"location":"applications/rag/frontend/#pre-requirement","title":"Pre-requirement","text":"<ol> <li>Enable Cloud Data Loss Prevention (DLP)</li> </ol> <p>We have two ways to enable the api:</p> <pre><code>1. Go to https://console.developers.google.com/apis/api/dlp.googleapis.com/overview click enable api.\n2. Run command: `gcloud services enable dlp.googleapis.com`\n</code></pre> <p>This filter can auto fetch the templates in your project. Please refer to the following links to create templates:</p> <pre><code>1. Inspect templates: https://cloud.google.com/dlp/docs/creating-templates-inspect\n2. De-identification templates: https://cloud.google.com/dlp/docs/creating-templates-deid\n</code></pre> <ol> <li>Enable Cloud Natural Language API</li> </ol> <p>Follow these instructions to enable the Cloud Natural Language API</p>"},{"location":"applications/rag/frontend/#example","title":"Example","text":""},{"location":"applications/rag/frontend/#dlp","title":"DLP","text":"<p>To safeguard sensitive data, follow the guidelines for create inspect template and create de-identify template. Set up an inspect template to identify instances of PERSON_NAME and a de-identify template to substitute any found names in the content with their corresponding InfoType.</p> <p>Execute the given query with DLP filtering active:</p> <pre><code>Who worked with Robert De Niro and name one film they collaborated\n</code></pre> <p>The expected output is:</p> <pre><code>[PERSON_NAME] has worked with many talented actors and directors throughout his career. One film he collaborated on with [PERSON_NAME] is \"GoodFellas,\" which was released in 1990. In this film, [PERSON_NAME] played the role of [PERSON_NAME], a former mobster who recounts his rise and fall in a New York crime family.\n</code></pre>"},{"location":"applications/rag/frontend/#cloud-natural-language-moderation","title":"Cloud Natural Language Moderation","text":"<p>Adjust the sensitivity level to 60 and attempt the following query:</p> <pre><code>Which movie will show blowing up a building\n</code></pre> <p>This should result in:</p> <pre><code>The response is deemed inappropriate for display.\n</code></pre> <p>Reducing the sensitivity threshold to 40 will reveal the initial answer:</p> <pre><code>It seems like you are asking about a movie that features a scene of blowing up a building. One such movie that comes to mind is \"Die Hard\" (1988), directed by John McTiernan and starring Bruce Willis, Alan Rickman, and Bonnie Bedelia. In this action film, the protagonist, New York City police detective John McClane, must save his estranged wife and other hostages taken by German terrorist Hans Gruber in the Nakatomi Plaza in Los Angeles. The film includes a memorable scene where McClane uses a makeshift explosive device to blow up the building's roof.\n\n</code></pre>"},{"location":"applications/rag/frontend/container/","title":"Frontend Container","text":"<p>This directory contains the code for a frontend flask webserver integrating with the inference backend and (NYI) vector database through LangChain.</p> <p>The image is hosted on <code>us-central1-docker.pkg.dev/ai-on-gke/rag-on-gke</code> and used by the frontend deployment.</p> <p>To build/regenerate the image, follow these steps.</p>"},{"location":"applications/ray/","title":"Ray on GKE Templates","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine. See the Ray on GKE directory to see additional guides and references.</p>"},{"location":"applications/ray/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>GCP Project with following APIs enabled</p> <ul> <li>container.googleapis.com</li> <li>iap.googleapis.com (required when using authentication with Identity Aware Proxy)</li> </ul> </li> <li> <p>A functional GKE cluster.</p> <ul> <li>To create a new standard or autopilot cluster, follow the instructions in <code>infrastructure/README.md</code></li> <li>Alternatively, you can set the <code>create_cluster</code> variable to true in <code>workloads.tfvars</code> to provision a new GKE cluster. This will default to creating a GKE Autopilot cluster; if you want to provision a standard cluster you must also set <code>autopilot_cluster</code> to false.</li> </ul> </li> <li> <p>This module is configured to optionally use Identity Aware Proxy (IAP) to protect access to the Ray dashboard. It expects the brand &amp; the OAuth consent configured in your org. You can check the details here: OAuth consent screen</p> </li> <li> <p>Preinstall the following on your computer:</p> <ul> <li>Terraform</li> <li>Gcloud CLI</li> </ul> </li> </ol>"},{"location":"applications/ray/#installation","title":"Installation","text":""},{"location":"applications/ray/#configure-inputs","title":"Configure Inputs","text":"<ol> <li>If needed, clone the repo</li> </ol> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/applications/ray\n</code></pre> <ol> <li>Edit <code>workloads.tfvars</code> with your GCP settings.</li> </ol> <p>Important Note: If using this with the Jupyter module (<code>applications/jupyter/</code>), it is recommended to use the same k8s namespace for both i.e. set this to the same namespace as <code>applications/jupyter/workloads.tfvars</code>.</p> Variable Description Required project_id GCP Project Id Yes cluster_name GKE Cluster Name Yes cluster_location GCP Region Yes kubernetes_namespace The namespace that Ray and rest of the other resources will be installed in. Yes gcs_bucket GCS bucket to be used for Ray storage Yes create_service_account Create service accounts used for Workload Identity mapping Yes"},{"location":"applications/ray/#install","title":"Install","text":"<p>NOTE: Terraform keeps state metadata in a local file called <code>terraform.tfstate</code>. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using <code>terraform destory</code> before reapplying/reinstalling.</p> <ol> <li>Ensure your gcloud application default credentials are in place. </li> </ol> <pre><code>gcloud auth application-default login\n</code></pre> <ol> <li> <p>Run <code>terraform init</code></p> </li> <li> <p>Run <code>terraform apply --var-file=./workloads.tfvars</code>. </p> </li> </ol>"},{"location":"applications/ray/tfvars_examples/","title":"Example terraform variables for Ray clusters","text":"<p>This folder contains example terraform variable files to use with the Ray on GKE terraform templates.</p> <p>To try one of the examples, edit the tfvars file and configure the mandatory variables. Then run the following commands:</p> <pre><code># from root of ai-on-gke\ncd applications/ray\nterraform init\nterraform apply --var-file=tfvars_examples/&lt;example-file&gt;\n</code></pre> <p>See Getting Started for more details on using the examples in this repo.</p>"},{"location":"best-practices/","title":"Best Practices","text":""},{"location":"best-practices/#aiml-platform-for-enabling-aiml-ops-on-gke-reference-architecture","title":"AI/ML Platform for enabling AI/ML Ops on GKE Reference Architecture","text":"<p>Construct an Artificial Intelligence/Machine Learning (AI/ML) platform that streamlines AI/ML Operations (AIMLOps), this reference architecture utilizes Google Kubernetes Engine (GKE) as the underlying runtime environment. Additionally, it incorporates a collection of diverse use cases that illustrate practical workflows closely aligned with AI/ML operations.</p>"},{"location":"best-practices/#batch-processing-platform-on-gke-reference-architecture","title":"Batch Processing Platform on GKE Reference Architecture","text":"<p>This reference architecture is designed to assist platform administrators, cloud architects, and operations professionals in deploying a batch processing platform on Google Kubernetes Engine (GKE). Utilizing GKE Standard as its foundation, this architecture leverages Kueue to manage resource quotas and borrowing rules between multiple tenant teams sharing the cluster. This enables these teams to run their batch workloads in a fair, cost-efficient, and high-performance manner. Key recommendations for effectively running batch workloads on GKE, as outlined in Best practices for running batch workloads on GKE are incorporated into this reference architecture.</p>"},{"location":"best-practices/#best-practices-for-faster-workload-cold-start","title":"Best Practices for Faster Workload Cold Start","text":"<p>To enhance cold start performance of workloads on Google Kubernetes Engine (GKE), this document provides best practices and examines the elements that influence startup latency.</p>"},{"location":"best-practices/startup-latency/","title":"Startup latency","text":""},{"location":"best-practices/startup-latency/#best-practices-for-faster-workload-cold-start","title":"Best Practices for Faster Workload Cold Start","text":"<p>This doc  provides best practices to help achieve faster workload cold start on Google Kubernetes Engine (GKE), and discusses factors that determine the workload startup latency.</p>"},{"location":"best-practices/startup-latency/#introduction","title":"Introduction","text":"<p>The cold start problem occurs when workloads are scheduled to nodes that haven't hosted the workloads before. Since the new node has no pre-existing container images, the initial startup time for the workloads can be significantly longer. This extended startup time can lead to latency issues on the overall application performance, especially for handling traffic surge by node autoscaling.</p>"},{"location":"best-practices/startup-latency/#best-practices","title":"Best Practices","text":""},{"location":"best-practices/startup-latency/#use-ephemeral-storage-with-local-ssds-or-larger-boot-disks-for-node","title":"Use ephemeral storage with local SSDs or larger boot disks for Node","text":"<p>Provision ephemeral storage with local SSDs | Google Kubernetes Engine (GKE). </p> <p>With this feature, you can create a node pool that uses ephemeral storage with local SSDs in an existing cluster running on GKE version 1.25.3-gke.1800 or later. And the local SSDs will also be used by kubelet and containerd as root dirs, which can improve the latency for container runtime operations such as image pull.  </p> <pre><code>gcloud container node-pools create POOL_NAME \\\n    --cluster=CLUSTER_NAME \\\n    --ephemeral-storage-local-ssd count=&lt;NUMBER_OF_DISKS&gt; \\\n    --machine-type=MACHINE_TYPE\n</code></pre> <p>Nodes will mount the Kubelet and container runtime (docker or containerd) root directories on a local SSD. Then the container layer to be backed by the local SSD, with the IOPS and throughput documented on About local SSDs, which are usually more cost-effective than increasing the PD size, below is a brief comparison between them in us-central1, with the same cost, LocalSSD has ~3x throughput than PD, with which the image pull runs faster and reduces the workload startup latency.</p> With the same cost     LocalSSD     PD Balanced     Throughput Comparison     $ per month     Storage space (GB)     Throughput \\ (MB/s) R W     Storage space (GB)     Throughput (MB/s) R+W     LocalSSD / PD (Read)     LocalSSD / PD (Write)     $     375     660   350 300     140 471%     250%     $$     750     1320 700 600     168 786%     417%     $$$     1125     1980 1050 900     252 786%     417%     $$$$     1500     2650 1400 1200     336 789%     417%"},{"location":"best-practices/startup-latency/#enable-container-image-streaming","title":"Enable container image streaming","text":"<p>Use Image streaming to pull container images | Google Kubernetes Engine (GKE)</p> <p>When customers are using Artifact Registry for their containers and meet requirements, they can enable image streaming on the cluster by </p> <pre><code>gcloud container clusters create CLUSTER_NAME \\\n    --zone=COMPUTE_ZONE \\\n    --image-type=\"COS_CONTAINERD\" \\\n    --enable-image-streaming\n</code></pre> <p>Customers can benefit from image streaming to allow workloads to start without waiting for the entire image to be downloaded, which leads to significant improvements in workload startup time. For example, Nvidia Triton Server (5.4GB container image) end-to-end startup time (from workload creation to server up for traffic) can be reduced from 191s to 30s.</p>"},{"location":"best-practices/startup-latency/#use-zstandard-compressed-container-images","title":"Use Zstandard compressed container images","text":"<p>Zstandard compression is a feature supported in ContainerD. Please note that </p> <ol> <li>Use the zstd builder in docker buildx</li> </ol> <pre><code>docker buildx create --name zstd-builder --driver docker-container \\\n  --driver-opt image=moby/buildkit:v0.10.3\ndocker buildx use zstd-builder\n</code></pre> <ol> <li>Build and push an image</li> </ol> <pre><code>IMAGE_URI=us-central1-docker.pkg.dev/&lt;YOUR-CONTAINER-REPO&gt;/example\nIMAGE_TAG=v1\n\n&lt;Create your Dockerfile&gt;\n\ndocker buildx build --file Dockerfile --output type=image,name=$IMAGE_URI:$IMAGE_TAG,oci-mediatypes=true,compression=zstd,compression-level=3,force-compression=true,push=true .\n</code></pre> <p>Now you can use <code>IMAGE_URI</code>for your workload which will have zstd compression image format. And Zstandard benchmark shows zstd is &gt;3x faster decompression than gzip (the current default).</p>"},{"location":"best-practices/startup-latency/#use-a-preloader-daemonset-to-preload-the-base-container-on-nodes","title":"Use a preloader DaemonSet to preload the base container on nodes","text":"<p>ContainerD reuse the image layers across different containers if they share the same base container. And the preloader DaemonSet can start running even before the GPU driver is installed (driver installation takes ~30 seconds). So it can preload required containers before the GPU workload can be scheduled to the GPU node and start image pulling ahead of time.</p> <p>Below is an example of the preloader DaemonSet.</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: container-preloader\n  labels:\n    k8s-app: container-preloader\nspec:\n  selector:\n    matchLabels:\n      k8s-app: container-preloader\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: container-preloader\n        k8s-app: container-preloader\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cloud.google.com/gke-accelerator\n                operator: Exists\n      tolerations:\n      - operator: \"Exists\"\n      containers:\n      - image: \"&lt;CONTAINER_TO_BE_PRELOADED&gt;\"\n        name: container-preloader\n        command: [ \"sleep\", \"inf\" ]\n\n</code></pre>"},{"location":"best-practices/startup-latency/#use-gcs-fuse-to-access-dataset-via-file-system-interface","title":"Use GCS Fuse to access DataSet via file system interface","text":"<p>Cloud Storage FUSE and CSI driver now available for GKE | Google Cloud Blog enables workloads to on-demand access GCS data with a local filesystem API.</p>"},{"location":"best-practices/startup-latency/#use-volumesnapshot-to-quickly-replicating-data-to-pods-by-pvc-with-disk-image","title":"Use VolumeSnapshot to quickly replicating data to pods by PVC with disk image","text":"<p>Using volume snapshots | Google Kubernetes Engine (GKE) with Disk image parameters to provision volumes used by Pods. This is because the disk image's base storage is reused by all disks created from it in the location, so new disk creation can be done much faster.</p>"},{"location":"best-practices/gke-batch-refarch/","title":"Reference Architecture: Batch Processing Platform on GKE","text":""},{"location":"best-practices/gke-batch-refarch/#purpose","title":"Purpose","text":"<p>This reference architecture is intended to help platform administrators, cloud architects, and operations professionals deploy a batch processing platform on Google Kubernetes Engine (GKE). This document features GKE in Standard mode, using Kueue to manage resource quotas and borrowing rules between multiple tenant teams sharing the cluster to run their batch workloads in a fair, cost efficient and performant way. Best practices for running batch workloads on GKE discusses many recommendations that are implemented in this document.</p>"},{"location":"best-practices/gke-batch-refarch/#overview","title":"Overview","text":"<p>Traditionally, batch platforms have two main user personas, developers and platform administrators:</p> <ul> <li> <p>A developer submits a Job specifying the program, the data to be processed, and requirements for the Job.</p> </li> <li> <p>A platform administrator manages and delivers an efficient and reliable batch processing platform to the developers.</p> </li> </ul> <p>Let's assume we have four teams of developers (<code>team-a</code>, <code>team-b</code>, <code>team-c</code>, <code>team-d</code>) who want to share a GKE cluster to run their batch machine learning training workloads. Additionally, <code>team-a</code> and <code>team-b</code> share billing so would like to use each other's unused resource quota for their workloads. <code>team-c</code> and <code>team-d</code> have a similar agreement. The Organization has purchased a Committed Use Discount consumed via reservations and would like to maximize the utilization of their reservations before bursting into On-demand or Spot VMs.</p> <p>To support these teams, the platform administrator would create a GKE cluster and configure it with Google published good practices for building batch platforms on GKE, in addition to their specific organizational best practices.</p> <p>This reference architecture illustrates an example of a batch platform on GKE that can support multiple teams:</p> <p></p> <ul> <li> <p>A regional GKE cluster with Node Auto Provisioning enabled, private nodes and Cloud NAT for outbound external access. A Cloud Monitoring dashboard tracking GPU utilization metrics is also created.</p> </li> <li> <p>The cluster is configured with user defined node pools and is capable of creating auto-provisioned GPU node pools based on workload requirements:</p> <ul> <li> <p>Reserved node pool consuming zonal NVIDIA L4 GPU VM reservations.</p> </li> <li> <p>Autoscaling On-demand node pool with NVIDIA L4 GPU for high-priority Jobs spilling over from reservations.</p> </li> <li> <p>Autoscaling Spot node pool with NVIDIA L4 GPU for failure tolerant or low-priority workloads spilling over from reservations.</p> </li> <li> <p>Compactly placed, auto-provisioned Spot node pools for low latency communication between workload Pods consuming NVIDIA A100 GPUs.</p> </li> </ul> </li> <li> <p>Four teams (<code>team-a</code>, <code>team-b</code>, <code>team-c</code>, <code>team-d</code>) each with their own namespace on the cluster, with Workload Identity established.</p> </li> <li> <p>PriorityClasses defined for low-priority (default), high-priority and compactly placed Jobs. Incoming high-priority Jobs can preempt running low-priority Jobs to reclaim reserved resources.</p> </li> <li> <p>Kueue is configured such that:</p> <ul> <li> <p>Each team has a <code>high-priority queue</code>, a <code>low-priority</code> queue and a <code>compact-placement</code> queue to which Jobs are submitted.</p> </li> <li> <p>Four ResourceFlavors defined; one each for <code>reserved</code>, <code>on-demand</code> and <code>spot</code> G2 VMs with NVIDIA L4 GPUs, and one for <code>spot</code> A2 VMs with NVIDIA A100 GPUs that are compactly-placed.</p> </li> <li> <p>Prometheus and Grafana are installed on the cluster for monitoring Kueue.</p> </li> </ul> </li> <li> <p>This reference architecture shows the teams submitting the following workloads:</p> </li> <li> <p>Distributed (multi-GPU, multi-host) machine learning model training using PyTorch and the mnist dataset, using Kubernetes Jobs in Indexed mode. See original example published here. This workload is used to showcase Kubernetes Job priority and preemption behavior using L4 GPUs and to show how to setup A100 GPUs in compact placement for multi-node training:</p> <ul> <li> <p>Low-priority Jobs: Jobs that don't specify a PriorityClass will get this default PriorityClass set to them. Kueue is configured to allow these Jobs to run on reserved VMs or Spot G2 VMs. If there is no room in or if a low-priority Job gets preempted from the reserved node pool, Kueue will evaluate them for other ResourceFlavors and will assign them the Spot G2 VM ResourceFlavor. In this example, each low-priority Job will have two Pods, each consuming two L4 GPUs.</p> </li> <li> <p>High-priority Jobs: Jobs specifying this PriorityClass will preempt any low-priority Jobs running on reserved VMs. Any overflow high-priority Jobs will trigger scale-up in the On-demand node pool. In this example, each high-priority Job will have two Pods, each consuming two L4 GPUs.</p> </li> <li> <p>Compactly placed Jobs: When admitted by Kueue, these Jobs will trigger GKE Node Auto Provisioning to create node pools purpose built for each of the Jobs, with nodes placed in close physical proximity to each other for low latency communication. In this example, each compactly placed Job will have two Pods, each consuming a single A100 GPU.</p> </li> </ul> </li> <li> <p>Distributed (multi-GPU, multi-host) machine learning training using PyTorch and the mnist dataset, using the Kubernetes JobSet API. In this example, JobSet automatically creates the headless ClusterIP service for all the workers in the Job to communicate with each other. See original example published here.</p> </li> </ul>"},{"location":"best-practices/gke-batch-refarch/#prerequistes","title":"Prerequistes","text":"<ol> <li> <p>This reference architecture has been tested on Cloud Shell which comes preinstalled with Google Cloud SDK that is required to complete this guide.</p> </li> <li> <p>Since a recent change in Grafana, services that run behind a reverse proxy cannot directly access metrics data. Port-forwarding from Cloud Shell, you would not be able to view Kueue metrics in Grafana. For a portion of this guide, you will be doing a port-forward from your local machine and for this, your local machine must have gcloud CLI available.</p> </li> <li> <p>It is recommended to start the guide in a fresh project since the easiest way to clean up once complete is to delete the project. See here for more details.</p> </li> <li> <p>This guide requires a number of different GCP Quotas (~60 L4 GPUs, ~30 Spot A100 GPUs and up to 600 CPU cores) in the region of your choosing. Please visit the IAM -&gt; Quotas page in the context of your project, region and zone to request additional quota before proceeding with this guide. This document can help you find the appropriate regions and zones where the G2 (with NVIDIA L4 GPUs) and A2 (with NVIDIA A100 GPUs) VM families are available. For example, the zones us-central-a, us-central1-b and us-central-c all have both VM families available.</p> </li> </ol>"},{"location":"best-practices/gke-batch-refarch/#deploy-google-cloud-resources-using-cloud-build-and-terraform","title":"Deploy Google Cloud resources using Cloud Build and Terraform.","text":"<ol> <li>Export the required environment variables; replace the value of <code>YOUR_PROJECT_ID</code> with that of a fresh project you created for this tutorial, <code>YOUR_REGION</code> with the name of your chosen Google Cloud region (eg. us-central1) and <code>YOUR_ZONE</code> with one of the zones in your chosen region (eg. us-central1-c).</li> </ol> <p><code>bash    export PROJECT_ID=YOUR_PROJECT_ID    export REGION=YOUR_REGION    export ZONE=YOUR_ZONE</code></p> <ol> <li>Clone this repo, switch to the appropriate subdirectory and run the <code>create-platform.sh</code> script. The rest of this step enables the required APIs, creates an IAM policy binding for the Cloud Build service account, creates an Artifact Registry to host the Cloud Build container images and submit a Cloud Build job to create the required Google Cloud resources using Terraform. For more details see <code>create-platform.sh</code>. Navigate to the Cloud Build page in the Google Cloud console to view status of the build, ensure you're in the context of the correct project.</li> </ol> <p><code>bash    cd $HOME &amp;&amp; \\    git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git &amp;&amp; \\    cd ai-on-gke/best-practices/gke-batch-refarch &amp;&amp; \\    ./create-platform.sh</code></p> <ol> <li>While the build runs, head over to the Kubernetes Engine page in the console. You can see the cluster being created and configured by the steps running in Cloud Build.</li> </ol> <p>a. The <code>Setup GKE</code> step will create the GKE cluster with a default node pool for system pods, a statically sized reserved node pool with four g2-standard-24 reserved VMs with two Nvidia L4 accelerators each, one auto-scaled (0 - 24 nodes) On-demand node pool for spill-over high-priority Jobs, and one auto-scaled (0 - 36 nodes) Spot node pool for spill-over low-priority Jobs. For more details, or to modify the infrastructure deployed in this document see <code>01_gke/main.tf</code> in this repository.</p> <p>b. Once the cluster is ready, the next steps deploy the recommended DaemonSet to install GPU drivers, and system resources for Kueue and JobSet are installed.</p> <p>c. The <code>Deploy Manifests: Priority Classes, Teams and Kueue configuration</code> step establishes default, high and compact PriorityClasses to showcase preemption of lower priority workloads by higher priority workloads for reserved resources. This step deploys four team manifests one for each of <code>team-a</code>, <code>team-b</code>, <code>team-c</code> and <code>team-d</code> including Namespaces and Kubernetes Service Accounts to be used for Workload Identity. In addition, this step configures Kueue such that each team gets a high priority queue, a low priority queue and a compact placement queue with the ability to specify nominalQuota and borrowingLimit for the respective flavors. See <code>02_platform/kueue</code> for more details.</p> <p>d. As a part of the platform deployment, a monitoring dashboard is created that tracks the number and utilization of GPUs on the cluster. Head over to Cloud Monitoring Dashboards page and you should see a dashboard entitled <code>NVIDIA GPU Monitoring Overview (GCE &amp; GKE)</code>. Keep a tab with this dashboard open and auto-refresh enabled (icon in the top right corner next to the time-frame).</p> <ol> <li>For monitoring Kueue metrics Prometheus and Grafana have been deployed on the GKE cluster and a dashboard to visualize the data has been configured. Since a recent change in Grafana, services that run behind a reverse proxy cannot directly access metrics data. Port-forwarding from Cloud Shell, you would not be able to view Kueue metrics. You will be doing a port-forward from your local machine and for this, your local machine must have gcloud CLI available.</li> </ol> <p>a. Open a new terminal on your local machine and get the cluster credentials using the following command:</p> <p><code>bash    gcloud container clusters get-credentials batch-dev --region $REGION --project $PROJECT_ID</code></p> <p>b. Next, create a port-forward to the <code>grafana</code> service running in the cluster so you can use your web browser to access the Grafana UI. Keep this terminal open for the rest of this guide.</p> <p><code>bash    kubectl port-forward svc/grafana 8080:3000 -n monitoring</code></p> <p>Expected output:</p> <p><code>bash    Forwarding from 127.0.0.1:8080 -&gt; 3000</code></p> <p>c. Open a new tab in your web browser and navigate to <code>http://localhost:8080</code>. You should see the Grafana login page. Use <code>admin</code> as the username and password.</p> <p>d. In the menu located on the top left part of the home page, click <code>Dashboards</code>.</p> <p>e. Navigate to <code>Kueue Dashboard</code>, you should see the current number of nodes in the cluster and other useful Kueue metrics; there should be no workloads pending or admitted. We will return to this dashboard periodically during this guide to see how the system has reacted to incoming workloads.</p> <p></p> <ol> <li>Deploying Low Priority workloads: Switch to the <code>low_priority</code> directory and run the <code>create_workloads.sh</code> script. This script will connect to the cluster and deploy one Job from each team at a time until all teams have four low priority Jobs submitted (job-0 through job-3).</li> </ol> <p><code>bash    cd $HOME/ai-on-gke/best-practices/gke-batch-refarch/low_priority &amp;&amp; \\    ./create_workloads.sh</code></p> <p>Expected output:</p> <p><code>bash    service/team-a-low-priority-svc-0 created    configmap/team-a-low-priority-config-0 created    job.batch/team-a-low-priority-job-0 created    ...    service/team-d-low-priority-svc-3 created    configmap/team-d-low-priority-config-3 created    job.batch/team-d-low-priority-job-3 created</code></p> <p></p> <p>a. List the Jobs running on the GKE cluster across all namespaces, you should see four Jobs from each team created. These Jobs have been admitted by Kueue.</p> <p><code>bash    watch kubectl get jobs --all-namespaces</code></p> <p>Expected output:</p> <p><code>bash    NAMESPACE   NAME                        COMPLETIONS   DURATION   AGE    team-a      team-a-low-priority-job-0   0/2           54s        55s    team-a      team-a-low-priority-job-1   0/2           49s        49s    team-a      team-a-low-priority-job-2   0/2           43s        43s    team-a      team-a-low-priority-job-3   0/2           38s        38s    team-b      team-b-low-priority-job-0   0/2           53s        53s    team-b      team-b-low-priority-job-1   0/2           47s        48s    team-b      team-b-low-priority-job-2   0/2           42s        42s    team-b      team-b-low-priority-job-3   0/2           37s        37s    team-c      team-c-low-priority-job-0   0/2           52s        52s    team-c      team-c-low-priority-job-1   0/2           46s        47s    team-c      team-c-low-priority-job-2   0/2           41s        41s    team-c      team-c-low-priority-job-3   0/2           36s        36s    team-d      team-d-low-priority-job-0   0/2           51s        51s    team-d      team-d-low-priority-job-1   0/2           45s        46s    team-d      team-d-low-priority-job-2   0/2           40s        40s    team-d      team-d-low-priority-job-3   0/2           35s        35s</code></p> <p>b. In a new terminal tab, watch the ClusterQueues, you should see four admitted workloads for each team's low priority ClusterQueue. These workloads correspond to the Jobs you saw in the previous step.</p> <p><code>bash    watch kubectl get clusterqueues -o wide</code></p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   0                   0    cq-team-a-hp        team-a-b   StrictFIFO   0                   0    cq-team-a-lp        team-a-b   StrictFIFO   0                   4    cq-team-b-compact   team-a-b   StrictFIFO   0                   0    cq-team-b-hp        team-a-b   StrictFIFO   0                   0    cq-team-b-lp        team-a-b   StrictFIFO   0                   4    cq-team-c-compact   team-c-d   StrictFIFO   0                   0    cq-team-c-hp        team-c-d   StrictFIFO   0                   0    cq-team-c-lp        team-c-d   StrictFIFO   0                   4    cq-team-d-compact   team-c-d   StrictFIFO   0                   0    cq-team-d-hp        team-c-d   StrictFIFO   0                   0    cq-team-d-lp        team-c-d   StrictFIFO   0                   4</code></p> <p>c. Since the Reserved node pool already has nodes available to run admitted low priority workloads, some of these workloads will be scheduled on the Reserved nodes. </p> <p>d. In a new terminal tab, watch the cluster nodes. Initially you should see ten nodes, six in the default node pool and four in the reserved node pool.</p> <p><code>bash    watch kubectl get nodes</code></p> <p>Expected output:</p> <p><code>bash    NAME                                               STATUS   ROLES    AGE   VERSION    gke-gke-batch-refarch-default-pool-8bba21a2-3328   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-bmzg   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-8k19   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-srxf   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-gn5j   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-wht3   Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-djvf    Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p2w7    Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p42h    Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-r6rt    Ready    &lt;none&gt;   12h   v1.28.3-gke.1203001</code></p> <p>e. After a short while you should see Spot nodes being added to the Spot node pool to accommodate the low priority Jobs that could not fit on the Reserved node pool (sized for two Jobs at a time).</p> <p>Expected output:</p> <p><code>bash    NAME                                               STATUS   ROLES    AGE    VERSION    gke-gke-batch-refarch-default-pool-8bba21a2-3328   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-bmzg   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-8k19   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-srxf   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-gn5j   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-wht3   Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-djvf    Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p2w7    Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p42h    Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-r6rt    Ready    &lt;none&gt;   12h    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-2gl8        Ready    &lt;none&gt;   102s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-2thb        Ready    &lt;none&gt;   97s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-4ccx        Ready    &lt;none&gt;   89s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-69p2        Ready    &lt;none&gt;   94s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-7246        Ready    &lt;none&gt;   80s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-7rhl        Ready    &lt;none&gt;   96s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-8267        Ready    &lt;none&gt;   95s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-c5x5        Ready    &lt;none&gt;   100s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-cc2h        Ready    &lt;none&gt;   109s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-fsjs        Ready    &lt;none&gt;   95s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-fsqw        Ready    &lt;none&gt;   100s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-h9bh        Ready    &lt;none&gt;   96s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-hshk        Ready    &lt;none&gt;   69s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-htjv        Ready    &lt;none&gt;   97s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-j7zc        Ready    &lt;none&gt;   94s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-jr45        Ready    &lt;none&gt;   104s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-l5v2        Ready    &lt;none&gt;   90s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-ltv4        Ready    &lt;none&gt;   98s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-ptx9        Ready    &lt;none&gt;   93s    v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-q57t        Ready    &lt;none&gt;   100s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-qlsz        Ready    &lt;none&gt;   106s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-t5tl        Ready    &lt;none&gt;   100s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-t62s        Ready    &lt;none&gt;   105s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-wxrr        Ready    &lt;none&gt;   91s    v1.28.3-gke.1203001</code></p> <p></p> <ol> <li>Deploying High Priority workloads: Switch to the <code>high_priority</code> directory and run the <code>create_workloads.sh</code> script. This script will connect to the cluster and deploy one Job from each team at a time until all teams have four low priority Jobs submitted (job-0 through job-3).</li> </ol> <p><code>bash    cd $HOME/ai-on-gke/best-practices/gke-batch-refarch/high_priority &amp;&amp; \\    ./create_workloads.sh</code></p> <p>Expected output:</p> <p><code>bash    service/team-a-high-priority-svc-0 created    configmap/team-a-high-priority-config-0 created    job.batch/team-a-high-priority-job-0 created    ...    service/team-d-high-priority-svc-3 created    configmap/team-d-high-priority-config-3 created    job.batch/team-d-high-priority-job-3 created</code></p> <p>a. Return to the terminal tab watching the <code>clusterqueues</code>. You should see high-priority workloads being added to the high-priority clusterqueues.</p> <p><code>bash    watch kubectl get clusterqueues -o wide</code></p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   0                   0    cq-team-a-hp        team-a-b   StrictFIFO   2                   2    cq-team-a-lp        team-a-b   StrictFIFO   1                   3    cq-team-b-compact   team-a-b   StrictFIFO   0                   0    cq-team-b-hp        team-a-b   StrictFIFO   2                   2    cq-team-b-lp        team-a-b   StrictFIFO   0                   3    cq-team-c-compact   team-c-d   StrictFIFO   0                   0    cq-team-c-hp        team-c-d   StrictFIFO   2                   2    cq-team-c-lp        team-c-d   StrictFIFO   0                   3    cq-team-d-compact   team-c-d   StrictFIFO   0                   0    cq-team-d-hp        team-c-d   StrictFIFO   2                   2    cq-team-d-lp        team-c-d   StrictFIFO   0                   3</code></p> <p>b. As high priority workloads are admitted, the low priority workloads running on the Reserved node pool are evicted to make room.</p> <p>.</p> <p>These low priority workloads are then set to pending and reevaluated by Kueue. They will use the Spot flavor to schedule on to the Spot node pool once new nodes have spun up to accommodate them.</p> <p></p> <p>d. Return to the terminal tab watching the cluster nodes, after a short while you should see nodes being added to the On Demand node pool to accommodate the high priority Jobs that could not fit on the Reserved node pool.</p> <p>Expected output:</p> <p><code>bash    NAME                                               STATUS   ROLES    AGE     VERSION    gke-gke-batch-refarch-default-pool-8bba21a2-3328   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-bmzg   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-8k19   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-srxf   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-gn5j   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-wht3   Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-426d    Ready    &lt;none&gt;   3m25s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-bs59    Ready    &lt;none&gt;   3m30s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-vktd    Ready    &lt;none&gt;   3m24s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-znst    Ready    &lt;none&gt;   3m30s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-7dpd    Ready    &lt;none&gt;   2m39s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-ggs4    Ready    &lt;none&gt;   2m41s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-qb49    Ready    &lt;none&gt;   2m32s   v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-x2cq    Ready    &lt;none&gt;   2m37s   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-djvf    Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p2w7    Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p42h    Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-r6rt    Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-2gl8        Ready    &lt;none&gt;   7m45s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-2thb        Ready    &lt;none&gt;   7m40s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-4ccx        Ready    &lt;none&gt;   7m32s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-69p2        Ready    &lt;none&gt;   7m37s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-7246        Ready    &lt;none&gt;   7m23s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-7rhl        Ready    &lt;none&gt;   7m39s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-8267        Ready    &lt;none&gt;   7m38s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-c5x5        Ready    &lt;none&gt;   7m43s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-cc2h        Ready    &lt;none&gt;   7m52s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-fsjs        Ready    &lt;none&gt;   7m38s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-fsqw        Ready    &lt;none&gt;   7m43s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-h9bh        Ready    &lt;none&gt;   7m39s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-hshk        Ready    &lt;none&gt;   7m12s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-htjv        Ready    &lt;none&gt;   7m40s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-j7zc        Ready    &lt;none&gt;   7m37s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-jr45        Ready    &lt;none&gt;   7m47s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-l5v2        Ready    &lt;none&gt;   7m33s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-ltv4        Ready    &lt;none&gt;   7m41s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-ptx9        Ready    &lt;none&gt;   7m36s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-q57t        Ready    &lt;none&gt;   7m43s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-qlsz        Ready    &lt;none&gt;   7m49s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-t5tl        Ready    &lt;none&gt;   7m43s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-t62s        Ready    &lt;none&gt;   7m48s   v1.28.3-gke.1203001    gke-gke-batch-refarch-spot-np-fd534d43-wxrr        Ready    &lt;none&gt;   7m34s   v1.28.3-gke.1203001</code></p> <p>The following diagram illustrates scale up in the On Demand node pool:</p> <p></p> <p>The Kueue dashboard also shows updated node counts, pending and active workloads,and other useful metrics from the batch platform.</p> <p></p> <ol> <li>Deploying compact placement workloads: Switch to the <code>compact_placement</code> directory and run the <code>create_workloads.sh</code> script. This script will connect to the cluster and deploy one Job from each team at a time until all teams have four compactly placed Jobs submitted (job-0 through job-3).</li> </ol> <p><code>bash    cd $HOME/ai-on-gke/best-practices/gke-batch-refarch/compact_placement &amp;&amp; \\    ./create_workloads.sh</code></p> <p>Expected output:</p> <p><code>bash    service/team-a-compact-svc-0 created    configmap/team-a-compact-config-0 created    job.batch/team-a-compact-job-0 created    ...    service/team-d-compact-svc-3 created    configmap/team-d-compact-config-3 created    job.batch/team-d-compact-job-3 created</code></p> <p>a. Return to the terminal tab watching the <code>clusterqueues</code>, you should see submitted compact workloads being pending and admitted to the compact clusterqueues for all four teams.</p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   3                   1    cq-team-a-hp        team-a-b   StrictFIFO   0                   2    cq-team-a-lp        team-a-b   StrictFIFO   0                   0    cq-team-b-compact   team-a-b   StrictFIFO   2                   1    cq-team-b-hp        team-a-b   StrictFIFO   0                   2    cq-team-b-lp        team-a-b   StrictFIFO   0                   0    cq-team-c-compact   team-c-d   StrictFIFO   3                   1    cq-team-c-hp        team-c-d   StrictFIFO   1                   2    cq-team-c-lp        team-c-d   StrictFIFO   0                   0    cq-team-d-compact   team-c-d   StrictFIFO   2                   1    cq-team-d-hp        team-c-d   StrictFIFO   0                   2    cq-team-d-lp        team-c-d   StrictFIFO   0                   1</code></p> <p>b. As these Jobs are admitted by Kueue, they will become Unschedulable, meaning that the cluster does not have the resources these workloads are asking for. However, since the cluster is configured with Node Auto Provisioning, GKE will create and scale a node pool purpose built for each Job based on the resource requests and limits. Once the Job is complete, the node pool will be cleaned up by GKE automatically, eliminating unnecessary spend.</p> <p></p> <p>c. Return to the terminal tab watching the cluster nodes, after a short while you should see nodes being created in auto-provisioned Spot node pools to accommodate the compactly placed Jobs that could not find any A100 GPUs in the node pools present on the cluster.</p> <p>Expected output:</p> <p><code>bash    NAME                                                  STATUS   ROLES    AGE     VERSION    gke-gke-batch-refarc-team-a-0-compact-4391522a-k57w   Ready    &lt;none&gt;   5m14s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-a-0-compact-4391522a-zqcr   Ready    &lt;none&gt;   5m12s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-b-0-compact-5ed4b762-7lsv   Ready    &lt;none&gt;   3m26s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-b-0-compact-5ed4b762-8h7n   Ready    &lt;none&gt;   3m26s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-c-0-compact-a2204c20-cwrt   Ready    &lt;none&gt;   3m44s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-c-0-compact-a2204c20-g4lz   Ready    &lt;none&gt;   4m7s    v1.28.3-gke.1203001    gke-gke-batch-refarc-team-d-0-compact-9f10df61-fwpj   Ready    &lt;none&gt;   2m49s   v1.28.3-gke.1203001    gke-gke-batch-refarc-team-d-0-compact-9f10df61-jw6k   Ready    &lt;none&gt;   2m49s   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-3328      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-bmzg      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-8k19      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-srxf      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-gn5j      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-wht3      Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-426d       Ready    &lt;none&gt;   17m     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-bs59       Ready    &lt;none&gt;   17m     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-48a23fe6-vktd       Ready    &lt;none&gt;   17m     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-7dpd       Ready    &lt;none&gt;   16m     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-ggs4       Ready    &lt;none&gt;   16m     v1.28.3-gke.1203001    gke-gke-batch-refarch-ondemand-np-8687a394-qb49       Ready    &lt;none&gt;   16m     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-djvf       Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p2w7       Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p42h       Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-r6rt       Ready    &lt;none&gt;   12h     v1.28.3-gke.1203001</code></p> <ol> <li>Deploying JobSet workloads: Switch to the <code>jobset</code> directory and run the <code>create_workloads.sh</code> script. This script will connect to the cluster and deploy one JobSet from each team at a time until all teams have three JobSets submitted (jobset-0 through jobset-3).</li> </ol> <p><code>bash    cd $HOME/ai-on-gke/best-practices/gke-batch-refarch/jobset &amp;&amp; \\    ./create_workloads.sh</code></p> <p>Expected output:</p> <p><code>bash    jobset.jobset.x-k8s.io/team-a-jobset-0 created    jobset.jobset.x-k8s.io/team-b-jobset-0 created    jobset.jobset.x-k8s.io/team-c-jobset-0 created    jobset.jobset.x-k8s.io/team-d-jobset-0 created    ...    jobset.jobset.x-k8s.io/team-a-jobset-3 created    jobset.jobset.x-k8s.io/team-b-jobset-3 created    jobset.jobset.x-k8s.io/team-c-jobset-3 created    jobset.jobset.x-k8s.io/team-d-jobset-3 created</code></p> <p>a. Return to the terminal tab watching the <code>clusterqueues</code>, you should see submitted JobSet workloads being pending and admitted to the low priority clusterqueues for all four teams.</p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   2                   1    cq-team-a-hp        team-a-b   StrictFIFO   0                   0    cq-team-a-lp        team-a-b   StrictFIFO   0                   4    cq-team-b-compact   team-a-b   StrictFIFO   2                   0    cq-team-b-hp        team-a-b   StrictFIFO   0                   0    cq-team-b-lp        team-a-b   StrictFIFO   0                   4    cq-team-c-compact   team-c-d   StrictFIFO   0                   1    cq-team-c-hp        team-c-d   StrictFIFO   0                   1    cq-team-c-lp        team-c-d   StrictFIFO   0                   4    cq-team-d-compact   team-c-d   StrictFIFO   2                   1    cq-team-d-hp        team-c-d   StrictFIFO   0                   0    cq-team-d-lp        team-c-d   StrictFIFO   0                   4</code></p> <ol> <li>Deploying workloads to DWS: This section of the reference architecture will introduce you to Dynamic Workload Scheduler (blog, docs). DWS supports all-or-nothing scheduling, allowing you to procure all the accelerators needed for your workload all at once instead of acquiring partial resources and waiting to get the full set before running the workload.</li> </ol> <p>The examples shown will deploy one model training job for <code>team-a</code> seeking A100 GPUs to fine-tune the Gemma 2B model and one job from <code>team-b</code> seeking H100 GPUs to fine-tune the Gemma 7B model, in both cases the model is fine-tuned to output SQL when asked a question in natural language. These example workloads are adapted from the example published here.</p> <p>The model weights are download from and uploaded to Hugging Face. In order to access these weights, you will need your Hugging Face token which can be generated here. Once you have the token, export it as an environment variable by replacing <code>YOUR_HUGGING_FACE_TOKEN</code> with your token and create the Kubernetes Secrets for team-a and team-b for their respective workloads to be able to access Hugging Face.</p> <p>```bash    export HF_TOKEN=YOUR_HUGGING_FACE_TOKEN &amp;&amp; \\</p> <p>kubectl create secret generic hf-secret \\    --from-literal=hf_api_token=${HF_TOKEN} \\    --dry-run=client -o yaml | kubectl -n team-a apply -f - &amp;&amp; \\</p> <p>kubectl create secret generic hf-secret \\    --from-literal=hf_api_token=${HF_TOKEN} \\    --dry-run=client -o yaml | kubectl -n team-b apply -f -    ```</p> <p>Next, configure Kueue to use DWS and deploy the workloads by switching to the <code>dws</code> directory, applying the <code>kueue-dws-config.yaml</code> and running the <code>create_workloads.sh</code> script:</p> <p><code>bash    cd $HOME/ai-on-gke/best-practices/gke-batch-refarch/dws &amp;&amp; \\    kubectl apply -f kueue-dws-config.yaml &amp;&amp; \\    ./create_workloads.sh</code></p> <p>Expected output:</p> <p><code>bash    ...    job.batch/finetune-gemma-2xa100 created    job.batch/finetune-gemma-8xh100 created</code></p> <p>a. Return to the terminal tab watching the <code>clusterqueues</code>, you should see submitted DWS workloads being pending and after a while admitted to the DWS clusterqueue for both team-a and team-b.</p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   2                   1    cq-team-a-hp        team-a-b   StrictFIFO   0                   0    cq-team-a-lp        team-a-b   StrictFIFO   0                   4    cq-team-b-compact   team-a-b   StrictFIFO   2                   0    cq-team-b-hp        team-a-b   StrictFIFO   0                   0    cq-team-b-lp        team-a-b   StrictFIFO   0                   4    cq-team-c-compact   team-c-d   StrictFIFO   0                   1    cq-team-c-hp        team-c-d   StrictFIFO   0                   1    cq-team-c-lp        team-c-d   StrictFIFO   0                   4    cq-team-d-compact   team-c-d   StrictFIFO   2                   1    cq-team-d-hp        team-c-d   StrictFIFO   0                   0    cq-team-d-lp        team-c-d   StrictFIFO   0                   4    dws-cluster-queue          BestEffortFIFO   0                   2</code></p> <p>b. At this time all the workloads have been submitted to the batch platform, and will continue to process in the order decided by Kueue.</p> <p>c. Head over to the GPU monitoring dashboard you opened earlier. You should see charts displaying useful GPU utilization data from the batch platform for example:</p> <ul> <li> <p>Number and type of GPUs in the cluster over time</p> <p></p> </li> <li> <p>Distribution (heatmap) of GPU utilization</p> <p></p> <ul> <li>Distribution (heatmap) of GPU memory utilization</li> </ul> <p></p> </li> </ul> <p>d. Return to the terminal tab watching Jobs across all namespaces, after a while you should see all Jobs completed.</p> <p>Expected output:</p> <p><code>bash    NAMESPACE   NAME                         COMPLETIONS   DURATION   AGE    team-a      finetune-gemma-2xa100        1/1           8m17s      15m    team-a      team-a-compact-job-0         2/2           7m22s      42m    team-a      team-a-compact-job-1         2/2           11m        42m    team-a      team-a-compact-job-2         2/2           7m52s      42m    team-a      team-a-compact-job-3         2/2           7m15s      42m    team-a      team-a-high-priority-job-0   2/2           8m37s      53m    team-a      team-a-high-priority-job-1   2/2           15m        53m    team-a      team-a-high-priority-job-2   2/2           4m38s      53m    team-a      team-a-high-priority-job-3   2/2           4m38s      53m    team-a      team-a-jobset-0-worker-0     2/2           18m        24m    team-a      team-a-jobset-1-worker-0     2/2           5m4s       26m    team-a      team-a-jobset-2-worker-0     2/2           91s        22m    team-a      team-a-jobset-3-worker-0     2/2           5m         26m    team-a      team-a-low-priority-job-0    2/2           3m31s      58m    team-a      team-a-low-priority-job-1    2/2           11m        58m    team-a      team-a-low-priority-job-2    2/2           10m        58m    team-a      team-a-low-priority-job-3    2/2           10m        57m    team-b      finetune-gemma-8xh100        1/1           10m8s      15m    team-b      team-b-compact-job-0         2/2           8m56s      42m    team-b      team-b-compact-job-1         2/2           7m28s      42m    team-b      team-b-compact-job-2         2/2           7m16s      42m    team-b      team-b-compact-job-3         2/2           8m3s       42m    team-b      team-b-high-priority-job-0   2/2           9m         53m    team-b      team-b-high-priority-job-1   2/2           7m34s      53m    team-b      team-b-high-priority-job-2   2/2           13m        53m    team-b      team-b-high-priority-job-3   2/2           4m52s      53m    team-b      team-b-jobset-0-worker-0     2/2           5m25s      26m    team-b      team-b-jobset-1-worker-0     2/2           3m37s      4m21s    team-b      team-b-jobset-2-worker-0     2/2           5m2s       26m    team-b      team-b-jobset-3-worker-0     2/2           5m10s      26m    team-b      team-b-low-priority-job-0    2/2           3m40s      58m    team-b      team-b-low-priority-job-1    2/2           11m        58m    team-b      team-b-low-priority-job-2    2/2           12m        58m    team-b      team-b-low-priority-job-3    2/2           10m        57m    team-c      team-c-compact-job-0         2/2           8m39s      42m    team-c      team-c-compact-job-1         2/2           7m6s       42m    team-c      team-c-compact-job-2         2/2           7m12s      42m    team-c      team-c-compact-job-3         2/2           7m11s      42m    team-c      team-c-high-priority-job-0   2/2           28m        53m    team-c      team-c-high-priority-job-1   2/2           11m        53m    team-c      team-c-high-priority-job-2   2/2           3m36s      53m    team-c      team-c-high-priority-job-3   2/2           3m30s      53m    team-c      team-c-jobset-0-worker-0     2/2           3m37s      11m    team-c      team-c-jobset-1-worker-0     2/2           3m33s      16m    team-c      team-c-jobset-2-worker-0     2/2           5m2s       26m    team-c      team-c-jobset-3-worker-0     2/2           3m38s      18m    team-c      team-c-low-priority-job-0    2/2           2m46s      58m    team-c      team-c-low-priority-job-1    2/2           11m        58m    team-c      team-c-low-priority-job-2    2/2           9m50s      58m    team-c      team-c-low-priority-job-3    2/2           10m        57m    team-d      team-d-compact-job-0         2/2           9m50s      42m    team-d      team-d-compact-job-1         2/2           7m34s      42m    team-d      team-d-compact-job-2         2/2           7m42s      42m    team-d      team-d-compact-job-3         2/2           7m13s      42m    team-d      team-d-high-priority-job-0   2/2           10m        53m    team-d      team-d-high-priority-job-1   2/2           27m        53m    team-d      team-d-high-priority-job-2   2/2           6m16s      53m    team-d      team-d-high-priority-job-3   2/2           6m22s      53m    team-d      team-d-jobset-0-worker-0     2/2           5m3s       26m    team-d      team-d-jobset-1-worker-0     2/2           97s        22m    team-d      team-d-jobset-2-worker-0     2/2           93s        22m    team-d      team-d-jobset-3-worker-0     2/2           16m        22m    team-d      team-d-low-priority-job-0    2/2           2m57s      58m    team-d      team-d-low-priority-job-1    2/2           10m        58m    team-d      team-d-low-priority-job-2    2/2           19m        58m    team-d      team-d-low-priority-job-3    2/2           17m        57m</code></p> <p>e. Return to the terminal tab watching the <code>clusterqueues</code>, you should see no workloads pending or admitted.</p> <p>Expected output:</p> <p><code>bash    NAME                COHORT     STRATEGY     PENDING WORKLOADS   ADMITTED WORKLOADS    cq-team-a-compact   team-a-b   StrictFIFO   0                   0    cq-team-a-hp        team-a-b   StrictFIFO   0                   0    cq-team-a-lp        team-a-b   StrictFIFO   0                   0    cq-team-b-compact   team-a-b   StrictFIFO   0                   0    cq-team-b-hp        team-a-b   StrictFIFO   0                   0    cq-team-b-lp        team-a-b   StrictFIFO   0                   0    cq-team-c-compact   team-c-d   StrictFIFO   0                   0    cq-team-c-hp        team-c-d   StrictFIFO   0                   0    cq-team-c-lp        team-c-d   StrictFIFO   0                   0    cq-team-d-compact   team-c-d   StrictFIFO   0                   0    cq-team-d-hp        team-c-d   StrictFIFO   0                   0    cq-team-d-lp        team-c-d   StrictFIFO   0                   0    dws-cluster-queue          BestEffortFIFO   0                   0</code></p> <p>f. Return to the terminal tab watching the nodes, you should see GKE shrink the cluster back down to the initial state of ten nodes, six in the default node pool and four in the reserved node pool.</p> <p>Expected output:</p> <p><code>bash    NAME                                               STATUS   ROLES    AGE   VERSION    gke-gke-batch-refarch-default-pool-8bba21a2-3328   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-8bba21a2-bmzg   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-8k19   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-9899b2fc-srxf   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-gn5j   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-default-pool-ab9bedc3-wht3   Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-djvf    Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p2w7    Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-p42h    Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001    gke-gke-batch-refarch-reserved-np-866c1d22-r6rt    Ready    &lt;none&gt;   13h   v1.28.3-gke.1203001</code></p>"},{"location":"best-practices/gke-batch-refarch/#clean-up","title":"Clean up","text":"<ol> <li>The easiest way to prevent continued billing for the resources that you created for this tutorial is to delete the project you created for the tutorial. Run the following commands from Cloud Shell:</li> </ol> <p><code>bash     gcloud config unset project &amp;&amp; \\     echo y | gcloud projects delete $PROJECT_ID</code></p> <ol> <li>If the project needs to be left intact, another option is to destroy the infrastructure created for this tutorial using Cloud Build. Note, this does not destroy the Cloud Storage bucket containing the Terraform state, the artifact registry used to host container images or the IAM bindings and service enablement created via the <code>create-platform.sh</code> script.</li> </ol> <p><code>bash     cd $HOME/ai-on-gke/best-practices/gke-batch-refarch &amp;&amp; \\     ./destroy-platform.sh</code></p>"},{"location":"best-practices/ml-platform/","title":"Moved to the GoogleCloudPlatform/accelerated-platforms repository which is included as a submodule in the /best-practices folder","text":"<pre><code>git clone --recurse-submodules https://github.com/GoogleCloudPlatform/ai-on-gke.git\ncd ai-on-gke/best-practices/accelerated-platforms\n</code></pre>"},{"location":"tutorials-and-examples/cloudshell-tutorial/","title":"Cloudshell tutorial","text":""},{"location":"tutorials-and-examples/cloudshell-tutorial/#lets-get-started","title":"Let's get started!","text":"<p>Welcome to the Cloudshell tutorial for AI on GKE!</p> <p>This guide will show you how to prepare a GKE cluster and install the AI applications on GKE. It'll also walk you through the configuration files that can be provided with custom inputs and commands that will complete the tutorial.</p> <p>Time to complete: About 30 minutes</p> <p>Prerequisites: GCP project linked with a Cloud Billing account</p> <p>To begin, click Start.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#what-is-ai-on-gke","title":"What is AI-on-GKE","text":"<p>This tutorial Terraform &amp; Cloud Build to provision the infrastructure as well as deploy the workloads</p> <p>You'll be performing the following activities:</p> <ol> <li>Set project-id for gcloud CLI</li> <li>Update terraform variable values to create infrastructure</li> <li>Update terraform variable values to provide workload configuration</li> <li>Create a GCS bucket to store terraform state</li> <li>Configure service account to be used for deployment</li> <li>Submit Cloud build job to create infrastructure &amp; deploy workloads</li> </ol> <p>To get started, click Next.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-0-set-your-project","title":"Step 0: Set your project","text":"<p>To set your Cloud Platform project for this terminal session use:</p> <pre><code>gcloud config set project [PROJECT_ID]\n</code></pre> <p>All the resources will be created in this project</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-1-provide-platform-inputs-parameters-for-terraform","title":"Step 1: Provide PLATFORM Inputs Parameters for Terraform","text":"<p>Here on step 1 you need to update the PLATFORM terraform tfvars file (located in ./platform/platform.tfvars) to provide the input parameters to allow terraform code execution to provision GKE resources. This will include the input parameters in the form of key value pairs. Update the values as per your requirements.</p> <p> Open platform.tfvars  </p> <p>Update <code>project_id</code> and review the other default values.</p> <p>Tip: Click the highlighted text above to open the file in your cloudshell editor.</p> <p>You can find tfvars examples in the tfvars_examples folder.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-2-provide-application-inputs-parameters-for-terraform","title":"Step 2: Provide APPLICATION Inputs Parameters for Terraform","text":"<p>Here on step 2 you need to update the APPLICATION terraform tfvars file (located in ./workloads/workloads.tfvars) to provide the input parameters to allow terraform code execution to provision the APPLICATION WORKLOADS. This will include the input parameters in the form of key value pairs. Update the values as per your requirements.</p> <p> Open workloads.tfvars </p> <p>Update <code>project_id</code> and review the other default values.</p> <p>Tip: Click the highlighted text above to open the file on your cloudshell.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-3-optional-configure-terraform-gcs-backend","title":"Step 3: [Optional] Configure Terraform GCS backend","text":"<p>You can also configure the GCS bucket to persist the terraform state file for further usage. To configure the terraform backend you need to have a GCS bucket already created. This needs to be done both for PLATFORM and APPLICATION stages.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#optional-create-gcs-bucket","title":"[Optional] Create GCS Bucket","text":"<p>In case you don't have a GCS bucket already, you can create using terraform or gcloud command as well. Refer below for the gcloud command line to create a new GCS bucket.</p> <pre><code>gcloud storage buckets create gs://BUCKET_NAME\n</code></pre> <p>Tip: Click the copy button on the side of the code box to paste the command in the Cloud Shell terminal to run it.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#optional-modify-platform-terraform-state-backend","title":"[Optional] Modify PLATFORM Terraform State Backend","text":"<p>Modify the ./platform/backend.tf and uncomment the code and update the backend bucket name.  Open ./platform/backend.tf  </p> <p>After changes file will look like below:</p> <pre><code>terraform {\n backend \"gcs\" {\n   bucket  = \"BUCKET_NAME\"\n   prefix  = \"terraform/state\"\n }\n}\n</code></pre> <p>Refer here for more details. </p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#optional-application-terraform-state-backend","title":"[Optional] APPLICATION Terraform State Backend","text":"<p>Modify the ./workloads/backend.tf and uncomment the code and update the backend bucket name.  Open ./workloads/backend.tf </p> <p>After changes file will look like below:</p> <pre><code>terraform {\n backend \"gcs\" {\n   bucket  = \"BUCKET_NAME\"\n   prefix  = \"terraform/state/workloads\"\n }\n}\n</code></pre> <p>Refer here for more details. </p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-4-configure-cloud-build-service-account","title":"Step 4: Configure Cloud Build Service Account","text":"<p>The Cloud Build service that orchestrates the environment creation requires a Service Account. Please run the following steps to create it and grant the roles required for deployment.</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\ngcloud iam service-accounts create aiongke --display-name=\"AI on GKE Service Account\"\n./iam/iam_policy.sh\n</code></pre>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-5-run-terraform-apply-using-cloud-build","title":"Step 5: Run Terraform Apply using Cloud Build","text":"<p>You are ready to deploy your resources now! <code>cloudbuild.yaml</code> is already prepared with all the steps required to deploy the application. </p> <p>Run the below command to submit Cloud Build job to deploy the resources:</p> <pre><code>gcloud beta builds submit --config=cloudbuild.yaml --substitutions=_PLATFORM_VAR_FILE=\"platform.tfvars\",_WORKLOADS_VAR_FILE=\"workloads.tfvars\"\n</code></pre> <p>Monitor the terminal for the log link and status for cloud build jobs.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#step-6-optional-delete-resources-created","title":"Step 6: [Optional] Delete resources created","text":"<p>You can now delete the resources</p> <pre><code>gcloud beta builds submit --config=cloudbuild_delete.yaml  --substitutions=_PLATFORM_VAR_FILE=\"platform.tfvars\",_WORKLOADS_VAR_FILE=\"workloads.tfvars\"\n</code></pre> <p>Monitor the terminal for the log link and status for cloud build jobs.</p>"},{"location":"tutorials-and-examples/cloudshell-tutorial/#congratulations","title":"Congratulations","text":"<p>You're all set!</p> <p>You can now access your cluster and applications.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/","title":"Guide to Serving Mistral 7B-Instruct v0.1 on GKE Utilizing Nvidia L4-GPUs","text":"<p>Learn how to serve the Mistral 7B instruct v0.1 chat model on GKE using just 1 x L4 GPU. This tutorial adapts the HF-text-generation-inference project for serving Mistral AI's model.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terminal Setup: Ensure you have kubectl and gcloud installed. Using Cloud Shell is highly  recommended for its simplicity and built-in tools.</li> <li>GPU Quota: Confirm you have the quota for at least one L4 GPU in your Google Cloud account.</li> <li>Model Access: Secure access to the Mistral 7B model by agreeing to the terms on Hugging Face, which typically involves creating an account and accepting the model's use conditions.</li> <li>Ensure you currently have installed a stable version of Transformers, 4.34.0 or newer.</li> <li>(OPTIONAL) If you intend to utilize the HPA, (horizontal pod autoscaler) in order to scale for incoming requests please make sure that the 'maxReplicas' assignment in your mistral-7b.yaml HorizontalPodAutoscaler section is configured to equal or be less than the number of GPUs you have available for the deployment. Additionally, ensure that you have a DCGM (Data Center GPU Manager) NVIDIA pod configured within your Kubernetes cluster to collect GPU metrics. Look at DCGM documentation for guidance on setting up and configuring this pod properly. This is essential for the Horizontal Pod Autoscaler (HPA) to accurately scale based on GPU utilization. Without proper GPU metrics, the autoscaler won't be able to make informed scaling decisions, potentially leading to under or over-provisioning of resources. Integrate the DCGM pod within your cluster's monitoring system to provide real-time GPU performance data to the HPA.+</li> </ul>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#gpu-memory-allocation","title":"GPU-Memory Allocation","text":"<p>For the Mistral-7B-Instruct-v0.1 model without quantization, the memory requirement is approximately 14.2 GB. Given that an L4 GPU has 24 GB of GPU memory, a single L4 GPU is sufficient to run the model, including some overhead for operational processes. This setup ensures effective deployment on a single GPU without the need for additional resources.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#feature-specific-to-model","title":"Feature-specific to Model","text":"<ul> <li>Model: Mistral-7B-instruct-v0.1, a transformer-based architecture.</li> <li>Attention Mechanisms:<ul> <li>Grouped-Query Attention: GQA categorizes query heads into groups, with each group sharing a common key and value projection. This setup allows for three variations: GQA-1, which is similar to MQA; GQA-H, mirroring the concept of MHA; and GQA-G, an intermediate state that balances between efficiency and expressiveness. By organizing query heads into groups, GQA reduces memory overhead and allows for nuanced control over the model's performance, effectively mitigating challenges related to memory bandwidth in large context scenarios</li> <li>Sliding-Window Attention: This attention mechanism limits the focus of the model to a nearby set of positions around each token, enhancing the model's ability to capture and utilize local contextual information more effectively, improving understanding and generation of text that relies on close proximity relationships.</li> <li>Byte-fallback BPE tokenizer: A tokenizer that combines byte-level encoding with Byte Pair Encoding (BPE) to ensure a balance between efficiency and coverage, allowing for the encoding of a wide range of text inputs, including those with uncommon characters or symbols, by falling back to byte-level representation when necessary.</li> </ul> </li> </ul> <p>Set your region and project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get-value project)\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#gke-cluster-creation","title":"GKE Cluster Creation:","text":"<pre><code># Adjust if your specific setup has different requirements:\ngcloud container clusters create mistral-cluster-gke  \\\n    --location=${REGION} \\\n    --node-locations=${REGION} \\\n    --project= ${PROJECT_ID} \\\n    --machine-type=n1-standard-4 \\\n    --no-enable-master-authorized-networks \\\n    --addons=GcsFuseCsiDriver \\\n    --num-nodes=3 \\\n    --min-nodes=1 \\\n    --max-nodes=5 \\\n    --enable-ip-alias \\\n    --enable-image-streaming \\\n    --enable-shielded-nodes \\\n    --shielded-secure-boot \\\n    --shielded-integrity-monitoring \\\n    --workload-pool=${PROJECT_ID}svc.id.goog\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#node-pool-with-single-l4-gpu","title":"Node Pool with Single L4 GPU:","text":"<p>Create a node pool for deploying Mistral 7B with a single L4 GPU {1 x L4}:</p> <pre><code>gcloud container node-pools create mistral-gpu-pool \\\n    --cluster=mistral-cluster \\\n    --region=${REGION} \\\n    --project=${PROJECT_ID}} \\\n    --machine-type=g2-standard-12 \\\n    --accelerator=type=nvidia-l4,count=1,gpu-driver-version=latest \\\n    --ephemeral-storage-local-ssd=count=2 \\\n    --node-locations=${ZONE} \\\n    --num-nodes=1 \\\n    --enable-autoscaling \\\n    --min-nodes=1 \\\n    --max-nodes=2 \\\n    --node-labels=accelerator=nvidia-gpu\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#hugging-face-authentication","title":"Hugging Face Authentication:","text":"<p>Obtain your Hugging Face token for model access: https://huggingface.co/settings/tokens For Hugging Face authentication and to download the Mistral model:</p> <pre><code>export HF_TOKEN=&lt;your-token&gt;\nkubectl create secret generic mistral-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Please use the given YAML file named mistral-7b.yaml with the provided content: Mistral 7B instruct-deployment.</p> <p>Deploy Mistral 7B with the following command:</p> <pre><code>kubectl apply -f mistral-7b.yaml\n</code></pre> <p>Assess the details of your deployment using the following command (Adjust if you utilized a different name in your YAML):</p> <pre><code>kubectl describe deployment mistral-7b -n default\n</code></pre> <p>Your output should resemble the following; please make sure the details are in accordance with your declarations and deployment needs: </p> <pre><code>Name:                   mistral-7b\nNamespace:              default\nCreationTimestamp:      Fri, 15 Mar 2024 11:47:53 -0700\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=mistral-7b\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=mistral-7b\n  Containers:\n   mistral-7b:\n    Image:      us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n    Port:       8080/TCP\n    Host Port:  0/TCP\n    Limits:\n      nvidia.com/gpu:  1\n    Environment:\n      MODEL_ID:   mistralai/Mistral-7B-Instruct-v0.1\n      NUM_SHARD:  1\n      PORT:       8080\n      QUANTIZE:   bitsandbytes-nf4\n    Mounts:\n      /data from data (rw)\n      /dev/shm from dshm (rw)\n  Volumes:\n   dshm:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\n   data:\n    Type:          HostPath (bare host directory volume)\n    Path:          /mnt/stateful_partition/kube-ephemeral-ssd/mistral-data\n    HostPathType:  \nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   mistral-7b-7b8bd5c7f4 (1/1 replicas created)\nEvents:          &lt;none&gt;\n</code></pre> <p>Test the deployment by forwarding the port and using curl to send a prompt:</p> <pre><code>kubectl port-forward deployment/mistral-7b 8080:8080\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#check-for-deployment-and-retrieve-load-balancer-external-exposed-ip-address","title":"Check for deployment and retrieve Load-Balancer external exposed ip address:","text":"<p>List Services: First, list all services in the namespace (assuming default namespace as per your YAML) to make sure your service is up and running.</p> <pre><code>kubectl get svc --namespace=default\n</code></pre> <p>Attach correct labels to load balancer deployment:</p> <pre><code>kubectl label service mistral-7b-service LLM_deployment=true model_LoadBalancer=true Ingress_Point=true\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#generate-load-balancer-details","title":"Generate Load Balancer Details:","text":"<p>To describe all services of type Load Balancer in your Kubernetes cluster, you should be able to see the details of your mistral LB here and get the ingress as well as external ip. (The external exposed ip will be important for connecting it to external services).</p> <pre><code>kubectl get services --all-namespaces -o wide | grep LoadBalancer | while read -r namespace name type cluster_ip external_ip ports age; do echo \"Describing service $name in namespace $namespace:\"; kubectl describe service -n $namespace $name; done\n</code></pre> <p>Your Load Balancer details should resemble the following Describing service mistral-7b-service in namespace default:</p> <pre><code>ame:                     mistral-7b-service\nNamespace:                default\nLabels:                   Ingress_Point=true\n                          LLM_deployment=true\n                          model_LoadBalancer=true\nAnnotations:              cloud.google.com/neg: {\"ingress\":true}\nSelector:                 app=mistral-7b\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.88.55.248\nIPs:                      10.88.55.248\nLoadBalancer Ingress:     34.125.177.85   # This ingress serves as your external IP\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  32683/TCP\nEndpoints:                10.92.3.4:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#optional-if-using-hpa-make-sure-hpa-horizontal-pod-autoscalar-is-working-please-see-hpa-documentation-and-preconfigurations-needed","title":"OPTIONAL: (If using HPA) Make sure HPA Horizontal Pod Autoscalar is working (Please see HPA documentation and preconfigurations needed):","text":"<p>Please make sure you have adjusted the yaml file appropriately.</p> <pre><code># kubectl describe hpa mistral-7b-hpa\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mistral-7b-instruct-L4gpus/#test-the-model-deployment","title":"Test the model deployment","text":"<p>Interact with the Model via curl, replace ip address with your external load balancer ip obtained from the kubectl get svc --namespace=default command earlier.</p> <pre><code># Define the instruction\ninstruction=\"INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief, malice, or misunderstanding. Your words should never wander into realms of rudeness, unfairness, or deception. Ensure your guidance is free from any bias and shines with positivity. If a query seems like a riddle without an answer or strays too far from Wonderland's wisdom, kindly explain why it doesn't hold water in our world rather than leading someone astray. Should a question's answer elude you in the vastness of Wonderland, refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\n</code></pre> <p>Measure the latency of a model deployment by timing a POST request to a model's generate endpoint, calculating the total response time and the latency per generated token. Make sure deployment latency is within tolerneces, otherwise your cluster/nodepool is misconfigured.</p> <pre><code>#### Test model deployment latency, Please make sure you differentiate hot and cold latency.\nstart=$(date +%s.%N)\n\ninstruction=\"[INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief. Your words should never wander into realms of rudeness, unfairness, or deception. refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\nend=$(date +%s.%N)\n\n# Extract the number of generated tokens from the response\ngenerated_tokens=$(echo \"$response\" | jq '.[0].details.generated_tokens')\n\n\n# If jq didn't find the field, or it's not a number, default to 1 to avoid division by zero\nif ! [[ \"$generated_tokens\" =~ ^[0-9]+$ ]]; then\n  generated_tokens=1\nfi\n\ntotal_latency=$(echo \"$end - $start\" | bc)\n\n# Calculate latency per generated token / This is meant to calculate general latency, might vary depending on your own setup. \nlatency_per_token=$(echo \"scale=6; $total_latency / $generated_tokens\" | bc)\n\necho \"Total Latency: $total_latency seconds\"\necho \"Generated Tokens: $generated_tokens\"\necho \"Latency per Generated Token: $latency_per_token seconds\"\n\n</code></pre> <p>Visit the API docs at http://localhost:8080/docs for more details.</p> <p>This README provides a concise guide to deploying the Mistral 7B instruct v.01 model, listed above are key steps and adjustments needed for a general sample deployment. Ensure to replace placeholders and commands with the specific details of your GKE setup and Mistralv01-instruct model deployment.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/","title":"Guide to Serving Mixtral 8x7 Model on GKE Utilizing Nvidia L4-GPUs","text":"<p>This guide walks you through the process of serving the Mixtral 8x7 model on Google Kubernetes Engine (GKE) leveraging Nvidia L4 GPUs. We'll adapt from the previous Mistral model deployment, focusing on a quadpod L4 GPU setup for enhanced performance.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terminal Setup: Ensure kubectl and gcloud are installed. Google Cloud Shell is recommended for its ease of use and built-in tools.</li> <li>GPU Quota: Confirm you have the quota for at least four L4 GPUs in your Google Cloud account.</li> <li>Model Access: Secure access to the Mixtral 8x7 model by agreeing to terms on a designated platform, typically involving account creation and acceptance of use conditions. Transformers Library: Ensure you have installed a stable version of the Transformers library, version 4.34.0 or newer.</li> <li>HPA (Optional): If you plan to use the Horizontal Pod Autoscaler (HPA) to scale for incoming requests, ensure the 'maxReplicas' assignment in your mixtral-8x7.yaml HorizontalPodAutoscaler section is set to equal or be less than the number of GPUs available for deployment.</li> </ul>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#gpu-memory-allocation-and-quantization-strategy","title":"GPU-Memory Allocation and Quantization Strategy","text":"<p>GPU-Memory Allocation and Quantization Strategy When deploying the Mixtral 8x7 model, it's crucial to assess both the memory requirements and the computational efficiency, especially when leveraging Nvidia L4 GPUs, each with 24 GB of GPU memory. A key factor in this consideration is the use of quantization techniques to optimize model performance and memory usage.</p> <p>Currently, the deployment employs 4-bit quantization (using the bitsandbytes-nf4 method), which significantly reduces the memory footprint of the model while maintaining a balance between performance and resource allocation. This quantization strategy allows the model to be efficiently served on a single L4 GPU for standard operations.</p> <p>However, should you opt for a larger quantization scale (upwards to no quantization), accommodating the increased memory and computational demands may necessitate scaling up to eight L4 GPUs to achieve optimal production performance. Consequently, it's advisable to adjust the number of shards to 8 in such scenarios to fully leverage the enhanced processing capacity and ensure that the deployment is scaled appropriately to handle the increased workload efficiently.</p>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#feature-specific-to-model","title":"Feature-specific to Model","text":"<ul> <li>Model: Mixtral8x7B-instruct-v0.1, a transformer-based architecture contained in a Mixture of experts with sparse gating model.</li> <li> <p>Attention Mechanisms:</p> <ul> <li>Grouped-Query Attention (GQA): The GQA framework is an innovative approach to managing memory and computational resources more efficiently. By categorizing attention heads into groups, each sharing a common key and value projection, GQA facilitates three distinct configurations:</li> <li>GQA-1, akin to MQA (Mixed Query Attention), focuses on enhancing the model's responsiveness and agility in handling queries.</li> <li>GQA-H, reflective of the traditional Multi-Head Attention (MHA) mechanism, balances complexity with computational efficiency.</li> <li>GQA-G, an intermediate form, optimizes both expressiveness and efficiency. This organization of query heads into groups minimizes memory overhead and enables precise control over performance, addressing challenges in scenarios requiring extensive context processing.</li> <li>Sliding-Window Attention: This technique restricts the model's attention span to a localized set of positions around each token. By doing so, it significantly boosts the ability to grasp and leverage context-specific information, sharpening the model's comprehension and generation capabilities, especially for text requiring nuanced understanding of proximate relationships.</li> </ul> </li> <li> <p>Tokenization:</p> <ul> <li>Hybrid-fallback Tokenizer: Building on the byte-fallback BPE tokenizer's foundation, the Hybrid-fallback tokenizer introduces an adaptive mechanism that seamlessly switches between byte-level and subword encoding. This dual approach ensures comprehensive coverage across a diverse array of textual inputs, from highly frequent words to rare or out-of-vocabulary terms, guaranteeing no loss in textual fidelity.</li> </ul> </li> <li>Mixture of Experts (MoE) Mixtral utilizes a unique component within its transformer architecture known as the MoE (Mixture-of-Experts) layer. This layer functions by intelligently directing input across a dynamically chosen subset of expert networks, each distinct in their training on various segments of data or specific tasks. By engaging this method, Mixtral is equipped to harness specialized knowledge and insights from these experts. This approach significantly enhances the model's capability for generating and analyzing text with greater accuracy and depth.</li> </ul> <p>Set your region and project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get-value project)\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#gke-cluster-creation","title":"GKE Cluster Creation:","text":"<pre><code>gcloud container clusters create mixtral8x7-cluster-gke \\\n  --region=${REGION} \\\n  --node-locations=${REGION} \\\n  --project=${PROJECT_ID} \\\n  --machine-type=n2d-standard-8 \\\n  --no-enable-master-authorized-networks \\\n  --addons=HorizontalPodAutoscaling \\\n  --addons=HttpLoadBalancing \\\n  --addons=GcePersistentDiskCsiDriver \\\n  --addons=GcsFuseCsiDriver \\\n  --num-nodes=4 \\\n  --min-nodes=3 \\\n  --max-nodes=6 \\\n  --enable-ip-alias \\\n  --enable-image-streaming \\\n  --enable-shielded-nodes \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --logging=SYSTEM,WORKLOAD \\\n  --monitoring=SYSTEM \\\n  --enable-autoupgrade \\\n  --enable-autorepair \\\n  --network=\"projects/${PROJECT_ID}/global/networks/default\" \\\n  --subnetwork=\"projects/${PROJECT_ID}/regions/${REGION}/subnetworks/default\" \\\n  --tags=web,sftp \\\n  --labels=env=production,team=mixtral8x7 \\\n  --release-channel=regular\n\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#node-pool-with-single-l4-gpu","title":"Node Pool with Single L4 GPU:","text":"<p>Create a node pool for deploying Mixtral 7B with quadpod deployment L4 GPU {4 x L4}:</p> <pre><code>gcloud container node-pools create mixtral-moe-gpu-pool \\\n  --cluster=mixtral8x7-cluster-gke  \\\n  --project=gke-aishared-dev \\\n  --machine-type=g2-standard-48 \\\n  --ephemeral-storage-local-ssd=count=4 \\\n  --accelerator=type=nvidia-l4,count=4 \\\n  --node-locations=us-west4-a \\\n  --enable-image-streaming \\\n  --num-nodes=1 \\\n  --enable-autoscaling \\\n  --min-nodes=1 \\\n  --max-nodes=2 \\\n  --node-labels=accelerator=nvidia-gpu \\\n  --workload-metadata=GKE_METADATA\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#hugging-face-authentication","title":"Hugging Face Authentication:","text":"<p>Obtain your Hugging Face token for model access: https://huggingface.co/settings/tokens For Hugging Face authentication and to download the Mixtral8x7 instruct model:</p> <pre><code>export HF_TOKEN=&lt;your-token&gt;\nkubectl create secret generic mixtral-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Please use the given YAML file named mixtral-8x7b.yaml with the provided content: Mixtral8x7Bv0.1 instruct-deployment.</p> <p>Deploy Mixtral8x7B with the following command:</p> <pre><code>kubectl apply -f mixtral8x7b.yaml\n</code></pre> <p>Assess the details of your deployment using the following command (Adjust if you utilized a different name in your YAML):</p> <pre><code>kubectl describe deployment mixtral8x7b -n default\n</code></pre> <p>Your output should resemble the following; please make sure the details are in accordance with your declarations and deployment needs: </p> <pre><code>Name:                   mixtral8x7b\nNamespace:              default\nCreationTimestamp:      Fri, 22 Mar 2024 12:09:13 -0700\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 9\nSelector:               app=mixtral8x7b\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=mixtral8x7b\n  Containers:\n   mixtral8x7b:\n    Image:      us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n    Port:       8080/TCP\n    Host Port:  0/TCP\n    Limits:\n      cpu:             5\n      memory:          42Gi\n      nvidia.com/gpu:  4\n    Requests:\n      cpu:             5\n      memory:          42Gi\n      nvidia.com/gpu:  4\n    Environment:\n      QUANTIZE:   bitsandbytes-nf4\n      MODEL_ID:   mistralai/Mixtral-8x7B-Instruct-v0.1\n      NUM_SHARD:  2\n      PORT:       8080\n    Mounts:\n      /data from ephemeral-volume (rw)\n      /dev/shm from dshm (rw)\n  Volumes:\n   dshm:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     Memory\n    SizeLimit:  &lt;unset&gt;\n   data:\n    Type:          HostPath (bare host directory volume)\n    Path:          /mnt/stateful_partition/kube-ephemeral-ssd/mixtral-data\n    HostPathType:  \n   ephemeral-volume:\n    Type:          EphemeralVolume (an inline specification for a volume that gets created and deleted with the pod)\n    StorageClass:  premium-rwo\n    Volume:        \n    Labels:            type=ephemeral\n    Annotations:       &lt;none&gt;\n    Capacity:      \n    Access Modes:  \n    VolumeMode:    Filesystem\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nNewReplicaSet:   mixtral8x7b-5c888947fd (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  43m   deployment-controller  Scaled up replica set mixtral8x7b-5c888947fd to 1\n</code></pre> <p>Test the deployment by forwarding the port and using curl to send a prompt:</p> <pre><code>kubectl port-forward deployment/mixtral8x7b 8080:8080\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#check-for-deployment-and-retrieve-load-balancer-external-exposed-ip-address","title":"Check for deployment and retrieve Load-Balancer external exposed ip address:","text":"<p>List Services: First, list all services in the namespace (assuming default namespace as per your YAML) to make sure your service is up and running.</p> <pre><code>kubectl get svc --namespace=default\n</code></pre> <p>Attach correct labels to load balancer deployment:</p> <pre><code>kubectl label service mixtral8x7b-service LLM_deployment=true model_LoadBalancer=true Ingress_Point=true\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#generate-load-balancer-details","title":"Generate Load Balancer Details:","text":"<p>To describe all services of type Load Balancer in your Kubernetes cluster, you should be able to see the details of your mixtral-LB here and get the ingress as well as external ip. (The external exposed ip will be important for connecting it to external services).</p> <pre><code>kubectl get services --all-namespaces -o wide | grep LoadBalancer | while read -r namespace name type cluster_ip external_ip ports age; do echo \"Describing service $name in namespace $namespace:\"; kubectl describe service -n $namespace $name; done\n</code></pre> <p>Your Load Balancer details should resemble the following Describing service mixtral8x7b-service in namespace default:</p> <pre><code>Name:                     mixtral8x7b-service\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=mixtral8x7b\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.65.140.207\nIPs:                      10.65.140.207\nLoadBalancer Ingress:     34.16.133.177\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  30640/TCP\nEndpoints:                10.92.1.10:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#optional-if-using-hpa-make-sure-hpa-horizontal-pod-autoscalar-is-working-please-see-hpa-documentation-and-preconfigurations-needed","title":"OPTIONAL: (If using HPA) Make sure HPA Horizontal Pod Autoscalar is working (Please see HPA documentation and preconfigurations needed):","text":"<p>Please make sure you have adjusted the yaml file appropriately.</p> <pre><code># kubectl describe hpa mixtral-8x7b-hpa\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/deploying-mixtral-8x7b-instruct-L4-gpus/#test-the-model-deployment","title":"Test the model deployment","text":"<p>Interact with the Model via curl, replace ip address with your external load balancer ip obtained from the kubectl get svc --namespace=default command earlier.</p> <pre><code># Define the instruction\ninstruction=\"INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief, malice, or misunderstanding. Your words should never wander into realms of rudeness, unfairness, or deception. Ensure your guidance is free from any bias and shines with positivity. If a query seems like a riddle without an answer or strays too far from Wonderland's wisdom, kindly explain why it doesn't hold water in our world rather than leading someone astray. Should a question's answer elude you in the vastness of Wonderland, refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\n</code></pre> <p>Measure the latency of a model deployment by timing a POST request to a model's generate endpoint, calculating the total response time and the latency per generated token. Make sure deployment latency is within tolerneces, otherwise your cluster/nodepool is misconfigured.</p> <pre><code>#### Test model deployment latency, Please make sure you differentiate hot and cold latency.\nstart=$(date +%s.%N)\n\ninstruction=\"[INST] You are a wise, courteous, and truthful Cheshire Cat, always offering guidance through Wonderland. Your responses must be insightful and protective, avoiding any mischief. Your words should never wander into realms of rudeness, unfairness, or deception. refrain from conjuring falsehoods. What is a secret in the gardens of Wonderland?[/INST]\"\n\n# Additional text to be concatenated with the instruction\nadditionalText=\" The story of the killer pancake monster:\"\n\n# Combine them\nfullText=\"$instruction$additionalText\"\n\n# Create the JSON payload\njsonPayload=$(cat &lt;&lt;EOF\n{\n  \"inputs\": \"$fullText\",\n  \"parameters\": {\n    \"best_of\": 1,\n    \"decoder_input_details\": false,\n    \"details\": false,\n    \"do_sample\": true,\n    \"max_new_tokens\": 400,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"EOS\", \"EOSTOKEN\", \"ENDTOKEN\", \"ENDOFLINE\"],\n\n    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_n_tokens\": 5,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n  },\n  \"stream\": false\n}\nEOF\n)\n\n# Execute the curl command, Please replace IP with the current External(Exposed)IP of the Loadbalancer\ncurl -X 'POST' \\\n  'http://{EXTERNAL_IP}/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$jsonPayload\"\n\nend=$(date +%s.%N)\n\n# Extract the number of generated tokens from the response\ngenerated_tokens=$(echo \"$response\" | jq '.[0].details.generated_tokens')\n\n\n# If jq didn't find the field, or it's not a number, default to 1 to avoid division by zero\nif ! [[ \"$generated_tokens\" =~ ^[0-9]+$ ]]; then\n  generated_tokens=1\nfi\n\ntotal_latency=$(echo \"$end - $start\" | bc)\n\n# Calculate latency per generated token / This is meant to calculate general latency, might vary depending on your own setup. \nlatency_per_token=$(echo \"scale=6; $total_latency / $generated_tokens\" | bc)\n\necho \"Total Latency: $total_latency seconds\"\necho \"Generated Tokens: $generated_tokens\"\necho \"Latency per Generated Token: $latency_per_token seconds\"\n\n</code></pre> <p>Visit the API docs at http://localhost:8080/docs for more details.</p> <p>This README provides a concise guide to deploying the Mixtral8x7B instruct v.01 model, listed above are key steps and adjustments needed for a general sample deployment. Ensure to replace placeholders and commands with the specific details of your GKE setup and Mixtral-v.01-instruct model deployment.</p>"},{"location":"tutorials-and-examples/genAI-LLM/e2e-genai-langchain-app/","title":"E2E GenAI application with Langchain, Ray, Flask API backend, React frontend","text":"<p>In this tutorial you will deploy a end-to-end application that will use GenAI model from Hugging Face on the backend, Ray Serve for inference, Flask API backend, and simple React frontend.</p>"},{"location":"tutorials-and-examples/genAI-LLM/e2e-genai-langchain-app/#before-you-begin","title":"Before you begin","text":"<p>Create or select an existing GCP project and open Cloud Shell. You can use these steps</p>"},{"location":"tutorials-and-examples/genAI-LLM/e2e-genai-langchain-app/#infrastructure-installation","title":"Infrastructure Installation","text":"<ol> <li>If needed, <code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git</code></li> <li>Create a new GKE cluster and install the kuberay operator<ol> <li>cd <code>gke-platform</code></li> <li>Edit <code>variables.tf</code> with your GCP settings. Make sure you change <code>project_id</code> and <code>cluster_name</code>.</li> <li>Run <code>terraform init</code></li> <li>Run <code>terraform apply</code></li> </ol> </li> <li>Configure credentials to point to the cluster: <code>gcloud container clusters get-credentials &lt;cluster-name&gt; --location=&lt;region&gt;</code></li> <li>Install Ray on GKE<ol> <li>cd <code>ray-on-gke/user</code></li> <li>Edit variables.tf with your GCP settings. Make sure you set: <code>project_id</code>, <code>namespace</code>, <code>service_account</code>.</li> <li>Note the namespace setting. All microservices in this sample will be deployed to this same namespace for simplicity.</li> <li>Run <code>terraform init</code></li> <li>Run <code>terraform apply</code></li> </ol> </li> <li>Install Jupyter on GKE. These steps are needed to experimentation (see the section below). You can skip it if you want to go straight to building the application. <ol> <li>cd <code>jupyter-on-gke</code></li> <li>Edit <code>variables.tf</code> with your GCP settings. Make sure that you set <code>project_id</code>, <code>project_number</code>, <code>namespace</code>. Use the same namespace as above.</li> <li>Configure higher resource limits and guarantees. In <code>jupyter_config/config.yaml</code> change the following:     <code>yaml     singleuser:     cpu:         limit: 1         guarantee: .5     memory:         limit: 4G         guarantee: 1G</code></li> <li>Run terraform init</li> <li>Run terraform apply</li> </ol> </li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/e2e-genai-langchain-app/#experimentation","title":"Experimentation","text":"<p>Experiment with the model in Jupyter Notebook: 1. Get the address of your Jupyter hub:  <code>kubectl get service proxy-public -n &lt;namespace name&gt; -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code> 1. Configure IAP and open Jupyter Hub by following the steps in here. 1. From JupyterHub open this notebook: <code>https://raw.githubusercontent.com/GoogleCloudPlatform/ai-on-gke/main/tutorials/langchain/nb1.ipynb</code> and run it step by step 1. The first section shows how to run the model directly 1. The second section shows how to do the same using <code>Ray Serve</code>.</p>"},{"location":"tutorials-and-examples/genAI-LLM/e2e-genai-langchain-app/#build-the-end-to-end-application","title":"Build the end-to-end application","text":"<ol> <li> <p>We used Jupyter Notebook to experiment, but now let's build the Flask backend that calls into <code>Ray Serve</code>.</p> <ol> <li>Observe <code>model.py</code>: it loads the model and creates <code>Ray.Serve</code> function that uses <code>Langchain</code> library to run two nested prompts.</li> <li>Observe <code>main.py</code>: it uses Flask framework to create API route that calls into <code>Ray.Serve</code> endpoint</li> <li>Containerize and deploy the backend image to the registry. Do these steps from <code>backend</code> directory:     <code>bash     PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;     # configure GCR     gcloud auth configure-docker     # build the image     docker build -t hf-lc-ray:latest .     # tag the image for GCR     docker tag hf-lc-ray:latest gcr.io/${PROJECT_ID}/hf-lc-ray:latest     # push the image to GCR     docker push gcr.io/${PROJECT_ID}/hf-lc-ray:latest</code></li> <li>Deploy backend to the cluster. Open <code>src/backend/deploy.yaml</code> and change <code>PROJECT_ID</code> to your project (you can also use <code>sed</code>:  <code>sed -i \"s/YOUR_PROJECT/${PROJECT_ID}/\" src/backend/deploy.yaml</code> ). Then run     <code>bash     kubectl apply -f deploy.yaml -n &lt;YOUR_NAMESPACE&gt;</code></li> <li>Find backend IP on the services page: <code>hf-lc-ray-service</code>:  <code>kubectl get service hf-lc-ray-service -n &lt;namespace name&gt; -o jsonpath='{.status.loadBalancer.ingress[0].ip}'</code></li> <li>To test that backend works you can run:     <code>ENDPOINT='http://&lt;IP&gt;/run'     curl -XPOST \"${ENDPOINT}?text=football\"</code>     You will get a response similar to:     <code>[\"a football player is a player who plays for a team\",\"Un joueur de football est un player qui joue pour un \\u00e9quipe.\"]</code></li> </ol> </li> <li> <p>Finally, let's deploy React frontend. Note that in a production distributed application, you can use K8s Ingress with routes for <code>backend</code> and <code>frontend</code> to avoid taking dependcy on the IP, this approach is provided for simplicity.</p> <ol> <li>Update the <code>API_ENDPOINT</code> in <code>src/frontend/src/index.tsx</code></li> <li>Containerize and deploy the frontend image to the registry. Do these steps from <code>src/frontend</code> directory:     <code>bash     PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;     # configure GCR     gcloud auth configure-docker     # build the image     docker build -t hf-lc-ray-fe:latest .     # tag the image for GCR     docker tag hf-lc-ray-fe:latest gcr.io/${PROJECT_ID}/hf-lc-ray-fe:latest     # push the image to GCR     docker push gcr.io/${PROJECT_ID}/hf-lc-ray-fe:latest</code></li> <li>Deploy frontend to the cluster. Open <code>src/frontend/deploy.yaml</code> and change <code>PROJECT_ID</code> to your project (you can also use <code>sed</code>:  <code>sed -i \"s/YOUR_PROJECT/${PROJECT_ID}/\" src/frontend/deploy.yaml</code> ). Then run     <code>bash     kubectl apply -f deploy.yaml -n &lt;YOUR_NAMESPACE&gt;</code></li> <li>Find frontend IP on the services page: <code>hf-lc-ray-fe-service</code>: </li> <li>Click to navigate and give it a try! </li> </ol> </li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/","title":"Tutorial: Finetuning Gemma 2b on GKE using L4 GPUs","text":"<p>We\u2019ll walk through fine-tuning a Gemma 2b model using GKE using 8 x L4 GPUs. L4 GPUs are suitable for many use cases beyond serving models. We will demonstrate how the L4 GPU is a great option for fine tuning LLMs, at a fraction of the cost of using a higher end GPU.</p> <p>Let\u2019s get started and fine-tune Gemma 2B on the b-mc2/sql-create-context dataset using GKE. Parameter Efficient Fine Tuning (PEFT) and LoRA is used so fine-tuning is posible on GPUs with less GPU memory.</p> <p>As part of this tutorial, you will get to do the following:</p> <ol> <li>Prepare your environment with a GKE cluster in     Autopilot mode.</li> <li>Create a finetune container.</li> <li>Use GPU to finetune the Gemma 2B model and upload the model to huggingface.</li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>Create a Hugging Face account, if you don't already have one.</li> <li>Ensure your project has sufficient quota for GPUs. To learn more, see About GPUs and Allocation quotas.</li> <li>To get access to the Gemma models for deployment to GKE, you must first sign the license consent agreement then generate a Hugging Face access token. Make sure the token has <code>Write</code> permission.</li> </ul>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#creating-the-gke-cluster-with-l4-nodepools","title":"Creating the GKE cluster with L4 nodepools","text":"<p>Let\u2019s start by setting a few environment variables that will be used throughout this post. You should modify these variables to meet your environment and needs. </p> <p>Download the code and files used throughout the tutorial:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4\n</code></pre> <p>Run the following commands to set the env variables and make sure to replace <code>&lt;my-project-id&gt;</code>:</p> <pre><code>gcloud config set project &lt;my-project-id&gt;\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport HF_TOKEN=&lt;YOUR_HF_TOKEN&gt;\nexport CLUSTER_NAME=finetune-gemma\n</code></pre> <p>Note: You might have to rerun the export commands if for some reason you reset your shell and the variables are no longer set. This can happen for example when your Cloud Shell disconnects.</p> <p>Create the GKE cluster by running:</p> <pre><code>gcloud container clusters create-auto ${CLUSTER_NAME} \\\n  --project=${PROJECT_ID} \\\n  --region=${REGION} \\\n  --release-channel=rapid \\\n  --cluster-version=1.29\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#create-a-kubernetes-secret-for-hugging-face-credentials","title":"Create a Kubernetes secret for Hugging Face credentials","text":"<p>In your shell session, do the following:</p> <ol> <li> <p>Configure <code>kubectl</code> to communicate with your cluster:</p> <p><code>sh   gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}</code></p> </li> <li> <p>Create a Kubernetes Secret that contains the Hugging Face token:</p> <p><code>sh   kubectl create secret generic hf-secret \\     --from-literal=hf_api_token=${HF_TOKEN} \\     --dry-run=client -o yaml | kubectl apply -f -</code></p> </li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#containerize-the-code-with-docker-and-cloud-build","title":"Containerize the Code with Docker and Cloud Build","text":"<ol> <li> <p>Create an Artifact Registry Docker Repository</p> <p><code>sh gcloud artifacts repositories create gemma \\     --project=${PROJECT_ID} \\     --repository-format=docker \\     --location=us \\     --description=\"Gemma Repo\"</code></p> </li> <li> <p>Execute the build and create inference container image.</p> <p><code>sh gcloud builds submit .</code></p> </li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#run-finetune-job-on-gke","title":"Run Finetune Job on GKE","text":"<ol> <li>Open the <code>finetune.yaml</code> manifest.</li> <li>Edit the <code>image</code> name with the container image built with Cloud Build and <code>NEW_MODEL</code> environment variable value. This <code>NEW_MODEL</code> will be the name of the model you would save as a public model in your Hugging Face account.</li> <li> <p>Run the following command to create the finetune job:</p> <p><code>sh kubectl apply -f finetune.yaml</code></p> </li> <li> <p>Monitor the job by running:</p> <p><code>sh watch kubectl get pods</code></p> </li> <li> <p>You can check the logs of the job by running:</p> <p><code>sh kubectl logs -f -l app=gemma-finetune</code></p> </li> <li> <p>Once the job is completed, you can check the model in Hugging Face.</p> </li> </ol>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#serve-the-finetuned-model-on-gke","title":"Serve the Finetuned Model on GKE","text":"<p>To deploy the finetuned model on GKE you can follow the instructions from Deploy a pre-trained Gemma model on Hugging Face TGI or vLLM. Select the Gemma 2B instruction and change the <code>MODEL_ID</code> to <code>&lt;YOUR_HUGGING_FACE_PROFILE&gt;/gemma-2b-sql-finetuned</code>.</p>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#set-up-port-forwarding","title":"Set up port forwarding","text":"<p>Once the model is deploye, run the following command to set up port forwarding to the model:</p> <pre><code>kubectl port-forward service/llm-service 8000:8000\n</code></pre> <p>The output is similar to the following:</p> <pre><code>Forwarding from 127.0.0.1:8000 -&gt; 8000\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#interact-with-the-model-using-curl","title":"Interact with the model using curl","text":"<p>Once the model is deployed In a new terminal session, use curl to chat with your model:</p> <p>The following example command is for TGI.</p> <pre><code>USER_PROMPT=\"Question: What is the total number of attendees with age over 30 at kubecon eu? Context: CREATE TABLE attendees (name VARCHAR, age INTEGER, kubecon VARCHAR)\"\n\ncurl -X POST http://localhost:8000/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d @- &lt;&lt;EOF\n{\n    \"inputs\": \"${USER_PROMPT}\",\n    \"parameters\": {\n        \"temperature\": 0.1,\n        \"top_p\": 0.95,\n        \"max_new_tokens\": 25\n    }\n}\nEOF\n</code></pre> <p>The following output shows an example of the model response:</p> <pre><code>{\"generated_text\":\" Answer: SELECT COUNT(age) FROM attendees WHERE age &gt; 30 AND kubecon = 'eu'\\n\"}\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#clean-up","title":"Clean Up","text":"<p>To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.</p>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-gemma-2b-on-l4/#delete-the-deployed-resources","title":"Delete the deployed resources","text":"<p>To avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:</p> <pre><code>gcloud container clusters delete ${CLUSTER_NAME} \\\n  --region=${REGION}\n</code></pre>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4/","title":"Tutorial: Finetuning Llama 7b on GKE using L4 GPUs","text":"<p>We\u2019ll walk through fine-tuning a Llama 2 7B model using GKE using 8 x L4 GPUs. L4 GPUs are suitable for many use cases beyond serving models. We will demonstrate how the L4 GPU is a great option for fine tuning LLMs, at a fraction of the cost of using a higher end GPU.</p> <p>Let\u2019s get started and fine-tune Llama 2 7B on the dell-research-harvard/AmericanStories dataset using GKE. Parameter Efficient Fine Tuning (PEFT) and LoRA is used so fine-tuning is posible on GPUs with less GPU memory. </p> <p>As part of this tutorial, you will get to do the following:</p> <ul> <li>Create a GKE cluster with an autoscaling L4 GPU nodepool</li> <li>Run a Kubernetes Job to download Llama 2 7B and fine-tune using L4 GPUs</li> </ul> <p></p>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>L4 GPUs quota to be able to run additional 8 L4 GPUs</li> <li>Request access to Meta Llama models by submitting the request access form</li> <li>Agree to the Llama 2 terms on the Llama 2 7B HF model in HuggingFace</li> </ul>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4/#creating-the-gke-cluster-with-l4-nodepools","title":"Creating the GKE cluster with L4 nodepools","text":"<p>Let\u2019s start by setting a few environment variables that will be used throughout this post. You should modify these variables to meet your environment and needs. </p> <p>Download the code and files used throughout the tutorial:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4\n</code></pre> <p>Run the following commands to set the env variables and make sure to replace <code>&lt;my-project-id&gt;</code>:</p> <pre><code>gcloud config set project &lt;my-project-id&gt;\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport BUCKET_NAME=${PROJECT_ID}-llama-l4\nexport SERVICE_ACCOUNT=\"l4-demo@${PROJECT_ID}.iam.gserviceaccount.com\"\n</code></pre> <p>Note: You might have to rerun the export commands if for some reason you reset your shell and the variables are no longer set. This can happen for example when your Cloud Shell disconnects.</p> <p>Create the GKE cluster by running:</p> <pre><code>gcloud container clusters create l4-demo --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=${REGION}-a \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --labels=\"ai-on-gke=l4-demo\" \\\n  --addons GcsFuseCsiDriver\n</code></pre> <p>(Optional) In environments where external IP addresses are not allowed you can add the following arguments to the create GKE cluster command:</p> <pre><code>  --no-enable-master-authorized-networks \\\n  --enable-private-nodes  --master-ipv4-cidr 172.16.0.32/28\n</code></pre> <p>Let\u2019s create a nodepool for our finetuning which will use 8 L4 GPUs per VM. Create the <code>g2-standard-96</code> nodepool by running:</p> <pre><code>gcloud container node-pools create g2-standard-96 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=8,gpu-driver-version=latest \\\n  --machine-type g2-standard-96 \\\n  --ephemeral-storage-local-ssd=count=8 \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=0 --min-nodes=0 --max-nodes=3 \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --node-locations ${REGION}-a,${REGION}-b --region ${REGION}\n</code></pre> <p>Note: The <code>--node-locations</code> flag might have to be adjusted based on which region you choose. Please check which zones the L4 GPUs are available if you change the region to something other than <code>us-central1</code>.</p> <p>The nodepool has been created and is scaled down to 0 nodes. So you are not paying for any GPUs until you start launching Kubernetes Pods that request GPUs.</p>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4/#run-a-kubernetes-job-to-fine-tune-llama-2-7b","title":"Run a Kubernetes job to fine-tune Llama 2 7B","text":"<p>Finetuning requires a base model and a dataset. For this post, the dell-research-harvard/AmericanStories dataset will be used to fine-tune the Llama 2 7B base model. GCS will be used for storing the base model. GKE with GCSFuse is used to transparently save the fine-tuned model to GCS. This provides a cost efficient way to store and serve the model and only pay for the storage used by the model.</p>"},{"location":"tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4/#configuring-gcs-and-required-permissions","title":"Configuring GCS and required permissions","text":"<p>Create a GCS bucket to store our models:</p> <pre><code>gcloud storage buckets create gs://${BUCKET_NAME}\n</code></pre> <p>The model loading Job will write to GCS. So let\u2019s create a Google Service Account that has read and write permissions to the GCS bucket. Then create a Kubernetes Service Account named <code>l4-demo</code> that is able to use the Google Service Account.</p> <p>To do this, first create a new Google Service Account:</p> <pre><code>gcloud iam service-accounts create l4-demo\n</code></pre> <p>Assign the required GCS permissions to the Google Service Account:</p> <pre><code>gcloud storage buckets add-iam-policy-binding gs://${BUCKET_NAME} \\\n  --member=\"serviceAccount:${SERVICE_ACCOUNT}\" --role=roles/storage.admin\n</code></pre> <p>Allow the Kubernetes Service Account <code>l4-demo</code> in the <code>default</code> namespace to use the Google Service Account:</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \\\n  --role roles/iam.workloadIdentityUser \\\n  --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/l4-demo]\"\n</code></pre> <p>Create a new Kubernetes Service Account:</p> <pre><code>kubectl create serviceaccount l4-demo\nkubectl annotate serviceaccount l4-demo iam.gke.io/gcp-service-account=l4-demo@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre> <p>Hugging face requires authentication to download the Llama 2 7B HF model, which means an access token is required to download the model.</p> <p>You can get your access token from huggingface.com &gt; Settings &gt; Access Tokens. Make sure to copy it and then use it in the next step when you create the Kubernetes Secret.</p> <p>Create a Secret to store your HuggingFace token which will be used by the Kubernetes job:</p> <pre><code>kubectl create secret generic l4-demo \\\n  --from-literal=\"HF_TOKEN=&lt;paste-your-own-token&gt;\"\n</code></pre> <p>Let's use Kubernetes Job to download the Llama 2 7B model from HuggingFace. The file <code>download-model.yaml</code> in this repo shows how to do this:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: model-loader\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: loader\n        gke-gcsfuse/volumes: \"true\"\n        gke-gcsfuse/memory-limit: 400Mi\n        gke-gcsfuse/ephemeral-storage-limit: 30Gi\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: loader\n        image: python:3.11\n        command:\n        - /bin/bash\n        - -c\n        - |\n          pip install huggingface_hub\n          mkdir -p /gcs-mount/llama2-7b\n          python3 - &lt;&lt; EOF\n          from huggingface_hub import snapshot_download\n          model_id=\"meta-llama/Llama-2-7b-hf\"\n          snapshot_download(repo_id=model_id, local_dir=\"/gcs-mount/llama2-7b\",\n                            local_dir_use_symlinks=False, revision=\"main\",\n                            ignore_patterns=[\"*.safetensors\", \"model.safetensors.index.json\"])\n          EOF\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: l4-demo\n              key: HF_TOKEN\n        volumeMounts:\n        - name: gcs-fuse-csi-ephemeral\n          mountPath: /gcs-mount\n      serviceAccountName: l4-demo\n      volumes:\n      - name: gcs-fuse-csi-ephemeral\n        csi:\n          driver: gcsfuse.csi.storage.gke.io\n          volumeAttributes:\n            bucketName: ${BUCKET_NAME}\n            mountOptions: \"implicit-dirs\"\n</code></pre> <p>Run the Kubernetes Job to download the the Llama 2 7B model to the bucket created previously:</p> <pre><code>envsubst &lt; download-model.yaml | kubectl apply -f -\n</code></pre> <p>Note: <code>envsubst</code> is used to replace <code>${BUCKET_NAME}</code> inside <code>download-model.yaml</code> with your own bucket.</p> <p>Give it a minute to start running, once up you can watch the logs of the job by running:</p> <pre><code>kubectl logs -f -l job-name=model-loader\n</code></pre> <p>Once the job has finished you can verify the model has been downloaded by running:</p> <pre><code>gcloud storage ls -l gs://$BUCKET_NAME/llama2-7b/\n</code></pre> <p>Let\u2019s write our finetuning job code by using the HuggingFace library for training.</p> <p>The <code>fine-tune.py</code> file in this repo will be used to do the finetuning. Let's take a look what's inside:</p> <pre><code>from pathlib import Path\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\nimport torch\n\n# /gcs-mount will mount the GCS bucket created earlier\nmodel_path = \"/gcs-mount/llama2-7b\"\nfinetuned_model_path = \"/gcs-mount/llama2-7b-american-stories\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n\ndataset = load_dataset(\"dell-research-harvard/AmericanStories\",\n    \"subset_years\",\n    year_list=[\"1809\", \"1810\", \"1811\", \"1812\", \"1813\", \"1814\", \"1815\"]\n)\ndataset = concatenate_datasets(dataset.values())\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\n\ndata = dataset.map(lambda x: tokenizer(\n    x[\"article\"], padding='max_length', truncation=True))\n\nlora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n lora_dropout=0.05,\n bias=\"none\",\n task_type=\"CAUSAL_LM\"\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntraining_args = TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        num_train_epochs=1,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=finetuned_model_path,\n        optim=\"paged_adamw_32bit\",\n)\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=data,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n\ntrainer.train()\n\n# Merge the fine tuned layer with the base model and save it\n# you can remove the line below if you only want to store the LoRA layer\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(finetuned_model_path)\ntokenizer.save_pretrained(finetuned_model_path)\n# Beginning of story in the dataset\nprompt = \"\"\"\nIn the late action between Generals\n\n\nBrown and Riall, it appears our men fought\nwith a courage and perseverance, that would\n\"\"\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.8,\n    max_length=100,\n)\nprint(tokenizer.batch_decode(gen_tokens)[0])\n</code></pre> <p>Let\u2019s review the high level of what we\u2019ve included in <code>fine-tune.py</code>. First we load the base model from GCS using GCS Fuse. Then we load the dataset from HuggingFace. The finetuning uses PEFT which stands for Parameter-Efficient Fine-Tuning. It is a technique that allows you to fine tune an LLM using a smaller number of parameters, which makes it more efficient, flexible and less computationally expensive.</p> <p>The fine-tuned model initially are saved as separate LoRA weights. In the <code>fine-tune.py</code> script, the base model and LoRA weights are merged so the fine-tuned model can be used as a standalone model. This does utilize more storage than needed, but in return you get better compatibility with different libraries for serving.</p> <p>Now we need to run the <code>fine-tune.py`` script inside a container that has all the depdencies. The container image at</code>us-docker.pkg.dev/google-samples/containers/gke/llama-7b-fine-tune-example<code>includes the</code>fine-tune.py<code>script and all required depencies. Alternatively, you can build and publish the image yourself by using the</code>Dockerfile` in this repo.</p> <p>Verify your environment variables are still set correctly:</p> <pre><code>echo \"Bucket: $BUCKET_NAME\"\n</code></pre> <p>Let's use a Kubernetes Job to fine-tune the model. The file <code>fine-tune.yaml</code> in this repo already has the following content:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: finetune-job\n  namespace: default\nspec:\n  backoffLimit: 2\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: finetuner\n        gke-gcsfuse/volumes: \"true\"\n        gke-gcsfuse/memory-limit: 400Mi\n        gke-gcsfuse/ephemeral-storage-limit: 30Gi\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: finetuner\n        image: us-docker.pkg.dev/google-samples/containers/gke/llama-7b-fine-tune-example\n        resources:\n          limits:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: gcs-fuse-csi-ephemeral\n          mountPath: /gcs-mount\n      serviceAccountName: l4-demo\n      volumes:\n      - name: gcs-fuse-csi-ephemeral\n        csi:\n          driver: gcsfuse.csi.storage.gke.io\n          volumeAttributes:\n            bucketName: $BUCKET_NAME\n            mountOptions: \"implicit-dirs\"\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-l4\n      restartPolicy: OnFailure\n</code></pre> <p>Run the fine-tuning Job:</p> <pre><code>envsubst &lt; fine-tune.yaml | kubectl apply -f -\n</code></pre> <p>Verify that the file Job was created and that <code>$IMAGE</code> and <code>$BUCKET_NAME</code> got replaced with the correct values. A Pod should have been created, which you can verify by running:</p> <pre><code>kubectl describe pod -l job-name=finetune-job\n</code></pre> <p>You should see a <code>pod triggered scale-up</code> message under Events after about 30 seconds. Then it will take another 2 minutes for a new GKE node with 8 x L4 GPUs to spin up. Once the Pod gets into running state you can watch the logs of the training:</p> <pre><code>kubectl logs -f -l job-name=finetune-job\n</code></pre> <p>You can watch the training steps and observe the loss go down over time. The training took 22 minutes and 10 seconds for me when I ran it, your results might differ. </p> <p>Once Job completes, you should see a fine-tuned model in your GCS bucket under the <code>llama2-7b-american-stories</code> path. Verify by running:</p> <pre><code>gcloud storage ls -l gs://$BUCKET_NAME/llama2-7b-american-stories\n</code></pre> <p>Congratulations! You have now successfully fine tuned a Llama 2 7B model on old American Stories from 1809 to 1815. Stay tuned for a follow up blog post on how to serve a HuggingFace model from GCS using GKE and GCSfuse. In the meantime you can take a look at the Basaran project for serving HuggingFace models interactively with a Web UI.</p>"},{"location":"tutorials-and-examples/genAI-LLM/serving-llama2-70b-on-l4-gpus/","title":"Tutorial: Serving Llama 2 70b on GKE L4 GPUs","text":"<p>Learn how to serve Llama 2 70b chat model on GKE using just 2 x L4 GPUs. For this post the text-generation-inference project is used for serving.</p>"},{"location":"tutorials-and-examples/genAI-LLM/serving-llama2-70b-on-l4-gpus/#prerequisites","title":"Prerequisites","text":"<ul> <li>A terminal with <code>kubectl</code> and <code>gcloud</code> installed. Cloud Shell works great!</li> <li>L4 GPUs quota to be able to run additional 2 L4 GPUs</li> <li>Request access to Meta Llama models by submitting the request access form</li> <li>Agree to the Llama 2 terms on the Llama 2 70B Chat HF model in HuggingFace</li> </ul> <p>Choose your region and set your project:</p> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get project)\n</code></pre> <p>Create a GKE cluster:</p> <pre><code>gcloud container clusters create l4-demo --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=$REGION-a \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --addons GcsFuseCsiDriver   \\\n  --no-enable-master-authorized-networks \\\n  --machine-type n2d-standard-4 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 5 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --enable-ip-alias\n</code></pre> <p>Create a nodepool where each VM has 2 x L4 GPU:</p> <pre><code>gcloud container node-pools create g2-standard-24 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\n  --machine-type g2-standard-24 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=0 --min-nodes=0 --max-nodes=3 \\\n  --shielded-secure-boot \\\n  --shielded-integrity-monitoring \\\n  --node-locations $REGION-a,$REGION-b --region $REGION --spot\n</code></pre> <p>Hugging Face requires authentication to download the Llama-2-70b-chat-hf model, which means an access token is required to download the model.</p> <p>You can get your access token from huggingface.com &gt; Settings &gt; Access Tokens. Afterwards, set your HuggingFace token as an environment variable:</p> <pre><code>export HF_TOKEN=&lt;paste-your-own-token&gt;\n</code></pre> <p>Create a Secret to store your HuggingFace token which will be used by the K8s job:</p> <pre><code>kubectl create secret generic l4-demo --from-literal=\"HF_TOKEN=$HF_TOKEN\"\n</code></pre> <p>Create a file named <code>text-generation-interface.yaml</code> with the following content:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llama-2-70b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llama-2-70b\n  template:\n    metadata:\n      labels:\n        app: llama-2-70b\n    spec:\n      containers:\n      - name: llama-2-70b\n        image: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        env:\n        - name: MODEL_ID\n          value: meta-llama/Llama-2-70b-chat-hf\n        - name: NUM_SHARD\n          value: \"2\"\n        - name: PORT \n          value: \"8080\"\n        - name: QUANTIZE\n          value: bitsandbytes-nf4\n        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: l4-demo\n              key: HF_TOKEN\n        volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n          - mountPath: /data\n            name: data\n      volumes:\n         - name: dshm\n           emptyDir:\n              medium: Memory\n         - name: data\n           hostPath:\n            path: /mnt/stateful_partition/kube-ephemeral-ssd/llama-data\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-l4\n</code></pre> <p>Create the deployment for serving:</p> <pre><code>kubectl apply -f text-generation-interface.yaml\n</code></pre> <p>Inside the YAML file the following settings are used: - <code>NUM_SHARD</code>, this has to be set to 2 because 2 x NVIDIA L4 GPUs are used. In our testing without setting this value it will only use a single GPU. - <code>QUANTIZE</code> is set to <code>nf4</code> which means that the model is loaded in 4 bit instead of 32 bits. This allows us to reduce the amount of GPU memory needed and improves the inference speed, however it can also decrease the model accuracy. If you change this you might need additional GPUs</p> <p>Visit the text-generation-inference docs for more details about these settings.</p> <p>How do you know how many GPUs you need? That depends on the value of <code>QUANTIZE</code>, in our case it is set to <code>bitsandbytes-nf4</code>, which means that the model will be loaded in 4 bits. So a 70 billion parameter model would require a minimum of 70 billion * 4 bits = 35 GB of GPU memory, lets say there is 5GB of overhead, which takes the minimum to 40GB. The L4 GPU has 24GB of GPU memory, so a single L4 GPU wouldn't have enough memory, however 2 x 24 = 48GB GPU memory, so using 2 x L4 GPU is sufficient to run Llama 2 70B on L4 GPUs.</p> <p>Check the logs and make sure there are no errors:</p> <pre><code>kubectl logs -l app=llama-2-70b\n</code></pre> <p>It's time to test it out by sending it some prompts. Setup port forwarding to the inferencing server:</p> <pre><code>kubectl port-forward deployment/llama-2-70b 8080:8080\n</code></pre> <p>Now you can chat with your model through a simple curl:</p> <pre><code>curl 127.0.0.1:8080/generate -X POST \\\n    -H 'Content-Type: application/json' \\\n    --data-binary @- &lt;&lt;EOF\n{\n    \"inputs\": \"[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n&lt;&lt;/SYS&gt;&gt;\\nHow to deploy a container on K8s?[/INST]\",\n    \"parameters\": {\"max_new_tokens\": 400}\n}\nEOF\n</code></pre> <p>There are also API docs available at http://localhost:8080/docs.</p>"},{"location":"tutorials-and-examples/gpu-examples/a100-jax/","title":"JAX 'Hello World' on GKE + A100-80GB","text":"<p>This tutorial shows how to run a simple JAX program using NVIDIA GPUs A100-80GB on a GKE cluster</p> <p>Visit https://cloud.google.com/blog/products/containers-kubernetes/machine-learning-with-jax-on-kubernetes-with-nvidia-gpus to follow the tutorial</p>"},{"location":"tutorials-and-examples/gpu-examples/online-serving-single-gpu/","title":"Serve a model with a GPU on GKE Autopilot","text":"<p>Please follow the How-to at TO DO: add link to how-to</p>"},{"location":"tutorials-and-examples/gpu-examples/training-single-gpu/","title":"Train a model with GPUs on GKE Standard mode","text":"<p>Please follow the Quick Start at https://cloud.google.com/kubernetes-engine/docs/quickstarts/train-model-gpus-standard</p>"},{"location":"tutorials-and-examples/hf-tgi/","title":"Index","text":"<ol> <li>Set env vars</li> </ol> <pre><code>export REGION=us-central1\nexport PROJECT_ID=$(gcloud config get project)\n</code></pre> <ol> <li>Create cluster</li> </ol> <pre><code>gcloud container clusters create l4-demo --location ${REGION}   \\\n--workload-pool ${PROJECT_ID}.svc.id.goog   --enable-image-streaming \\\n--node-locations=$REGION-a --addons GcsFuseCsiDriver  \\\n --machine-type n2d-standard-4  \\\n --num-nodes 1 --min-nodes 1 --max-nodes 5   \\\n--ephemeral-storage-local-ssd=count=2 --enable-ip-alias\n</code></pre> <pre><code>kubectl config set-cluster l4-demo\n</code></pre> <ol> <li>Create node pool</li> </ol> <pre><code>gcloud container node-pools create g2-standard-24 --cluster l4-demo \\\n  --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\n  --machine-type g2-standard-24 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n --enable-image-streaming \\\n --num-nodes=1 --min-nodes=1 --max-nodes=2 \\\n --node-locations $REGION-a,$REGION-b --region $REGION\n ```\n4. Set the project_id in workloads.tfvars and create the application: `terrafrom apply -var-file=workloads.tfvars` \n5. Make sure app started ok: `kubectl logs -l app=mistral-7b-instruct`\n6. Set up port forward\n</code></pre> <p>kubectl port-forward deployment/mistral-7b-instruct 8080:8080 &amp;</p> <pre><code>7. Try a few prompts:\n</code></pre> <p>export USER_PROMPT=\"How to deploy a container on K8s?\"</p> <pre><code></code></pre> <p>curl 127.0.0.1:8080/generate -X POST \\     -H 'Content-Type: application/json' \\     --data-binary @- &lt;&lt;EOF {     \"inputs\": \"[INST] &lt;&gt;\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n&lt;&gt;\\n$USER_PROMPT[/INST]\",     \"parameters\": {\"max_new_tokens\": 400} } EOF</p> <pre><code>8. Look at `/metrics` endpoint of the service. Go to cloud monitoring and search for one of those metrics. For example, `tgi_request_count` or `tgi_batch_inference_count`. Those metrics should show up if you search for them in PromQL. \n\n9. Clean up the cluster\n</code></pre> <p>gcloud container clusters delete l4-demo --location ${REGION}  ```</p>"},{"location":"tutorials-and-examples/inference-servers/checkpoints/","title":"Checkpoint conversion","text":"<p>The <code>checkpoint_entrypoint.sh</code> script overviews how to convert your inference checkpoint for various model servers.</p> <p>Build the checkpoint conversion Dockerfile</p> <pre><code>docker build -t inference-checkpoint .\ndocker tag inference-checkpoint gcr.io/${PROJECT_ID}/inference-checkpoint:latest\ndocker push gcr.io/${PROJECT_ID}/inference-checkpoint:latest\n</code></pre> <p>Now you can use it in a Kubernetes job and pass the following arguments</p>"},{"location":"tutorials-and-examples/inference-servers/checkpoints/#jetstream-maxtext","title":"Jetstream + MaxText","text":"<pre><code>- -s=INFERENCE_SERVER\n- -b=BUCKET_NAME\n- -m=MODEL_PATH\n- -v=VERSION (Optional)\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/checkpoints/#jetstream-pytorchxla","title":"Jetstream + Pytorch/XLA","text":"<pre><code>- -s=INFERENCE_SERVER\n- -m=MODEL_PATH\n- -n=MODEL_NAME\n- -q=QUANTIZE_WEIGHTS (Optional) (default=False)\n- -t=QUANTIZE_TYPE (Optional) (default=int8_per_channel)\n- -v=VERSION (Optional) (default=jetstream-v0.2.3)\n- -i=INPUT_DIRECTORY (Optional)\n- -o=OUTPUT_DIRECTORY\n- -h=HUGGINGFACE (Optional) (default=False)\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/checkpoints/#argument-descriptions","title":"Argument descriptions:","text":"<pre><code>b) BUCKET_NAME: (str) GSBucket, without gs://\ns) INFERENCE_SERVER: (str) Inference server, ex. jetstream-maxtext, jetstream-pytorch\nm) MODEL_PATH: (str) Model path, varies depending on inference server and location of base checkpoint\nn) MODEL_NAME: (str) Model name, ex. llama-2, llama-3, gemma\nh) HUGGINGFACE: (bool) Checkpoint is from HuggingFace.\nq) QUANTIZE_WEIGHTS: (str) Whether to quantize weights\nt) QUANTIZE_TYPE: (str) Quantization type, QUANTIZE_WEIGHTS must be set to true. Availabe quantize type: {\"int8\", \"int4\"} x {\"per_channel\", \"blockwise\"},\nv) VERSION: (str) Version of inference server to override, ex. jetstream-v0.2.2, jetstream-v0.2.3\ni) INPUT_DIRECTORY: (str) Input checkpoint directory, likely a GSBucket path\no) OUTPUT_DIRECTORY: (str) Output checkpoint directory, likely a GSBucket path\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/","title":"Serve a LLM using a single-host TPU on GKE with JetStream and MaxText","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#background","title":"Background","text":"<p>This tutorial shows you how to serve a large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with JetStream and MaxText. </p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#setup","title":"Setup","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#set-default-environment-variables","title":"Set default environment variables","text":"<pre><code>gcloud config set project [PROJECT_ID]\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=[COMPUTE_REGION]\nexport ZONE=[ZONE]\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#create-gke-cluster-and-node-pool","title":"Create GKE cluster and node pool","text":"<pre><code># Create zonal cluster with 2 CPU nodes\ngcloud container clusters create jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --project=${PROJECT_ID} \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --release-channel=rapid \\\n    --num-nodes=2\n\n# Create one v5e TPU pool with topology 2x4 (1 TPU node with 8 chips)\ngcloud container node-pools create tpu \\\n    --cluster=jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --num-nodes=2 \\\n    --machine-type=ct5lp-hightpu-8t \\\n    --project=${PROJECT_ID}\n</code></pre> <p>You have created the following resources:</p> <ul> <li>Standard cluster with 2 CPU nodes.</li> <li>One v5e TPU node pool with 2 nodes, each with 8 chips.</li> </ul>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#configure-applications-to-use-workload-identity","title":"Configure Applications to use Workload Identity","text":"<p>Prerequisite: make sure you have the following roles</p> <pre><code>roles/container.admin\nroles/iam.serviceAccountAdmin\n</code></pre> <p>Follow these steps to configure the IAM and Kubernetes service account:</p> <pre><code># Get credentials for your cluster\n$ gcloud container clusters get-credentials jetstream-maxtext \\\n    --zone=${ZONE}\n\n# Create an IAM service account.\n$ gcloud iam service-accounts create jetstream-iam-sa\n\n# Ensure the IAM service account has necessary roles. Here we add roles/storage.objectUser for gcs bucket access.\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.objectUser\n\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.insightsCollectorService\n\n# Allow the Kubernetes default service account to impersonate the IAM service account\n$ gcloud iam service-accounts add-iam-policy-binding jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n\n# Annotate the Kubernetes service account with the email address of the IAM service account.\n$ kubectl annotate serviceaccount default \\\n    iam.gke.io/gcp-service-account=jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#create-a-cloud-storage-bucket-to-store-the-gemma-7b-model-checkpoint","title":"Create a Cloud Storage bucket to store the Gemma-7b model checkpoint","text":"<pre><code>gcloud storage buckets create $BUCKET_NAME\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#get-access-to-the-model","title":"Get access to the model","text":"<p>Access the model consent page and request access with your Kaggle Account. Accept the Terms and Conditions. </p> <p>Obtain a Kaggle API token by going to your Kaggle settings and under the <code>API</code> section, click <code>Create New Token</code>. A <code>kaggle.json</code> file will be downloaded.</p> <p>Create a Secret to store the Kaggle credentials</p> <pre><code>kubectl create secret generic kaggle-secret \\\n    --from-file=kaggle.json\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#convert-the-gemma-7b-checkpoint","title":"Convert the Gemma-7b checkpoint","text":"<p>To convert the Gemma-7b checkpoint, we have created a job <code>checkpoint-job.yaml</code> that does the following: 1. Download the base orbax checkpoint from kaggle 2. Upload the checkpoint to a Cloud Storage bucket 3. Convert the checkpoint to a MaxText compatible checkpoint 4. Unscan the checkpoint to be used for inference</p> <p>In the manifest, ensure the value of the BUCKET_NAME environment variable is the name of the Cloud Storage bucket you created above. Do not include the <code>gs://</code> prefix.</p> <p>Apply the manifest:</p> <pre><code>kubectl apply -f checkpoint-job.yaml\n</code></pre> <p>Observe the logs:</p> <pre><code>kubectl logs -f jobs/data-loader-7b\n</code></pre> <p>You should see the following output once the job has completed. This will take around 10 minutes:</p> <pre><code>Successfully generated decode checkpoint at: gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items\n+ echo -e '\\nCompleted unscanning checkpoint to gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items'\n\nCompleted unscanning checkpoint to gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#deploy-maxengine-server-and-http-server","title":"Deploy Maxengine Server and HTTP Server","text":"<p>Next, deploy a Maxengine server hosting the Gemma-7b model. You can use the provided Maxengine server and HTTP server images or build your own. Depending on your needs and constraints you can elect to deploy either via Terraform or via Kubectl.</p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#deploy-via-kubectl","title":"Deploy via Kubectl","text":"<p>See the Jetstream component README for start to finish instructions on how to deploy jetstream to your cluster, assure the value of the PARAMETERS_PATH is the path where the checkpoint-converter job uploaded the converted checkpoints to, in this case it should be <code>gs://$BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items</code> where $BUCKET_NAME is the same as above.</p> <p>This README also includes instructions for setting up autoscaling. Follow those instructions to install the required components for autoscaling and configuring your HPAs appropriately.</p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#deploy-via-terraform","title":"Deploy via Terraform","text":"<p>Navigate to the <code>./terraform</code> directory and run <code>terraform init</code>. The deployment requires some inputs, an example <code>sample-terraform.tfvars</code> is provided as a starting point, run <code>cp sample-terraform.tfvars terraform.tfvars</code> and modify the resulting <code>terraform.tfvars</code> as needed. Since we're using gemma-7b the <code>maxengine_deployment_settings.parameters_path</code> terraform variable should be set to the following: <code>gs://BUCKET_NAME/final/unscanned/gemma_7b-it/0/checkpoints/0/items</code>. Finally run <code>terraform apply</code> to apply these resources to your cluster.</p> <p>For deploying autoscaling components via terraform, a few more variables to be set, doing so and rerunning the prior step with these set will deploy the components. The following variables should be set:</p> <pre><code>maxengine_deployment_settings = {\n  metrics = {\n    port: &lt;same as above&gt;   # which port will we scrape server metrics from\n    scrape_interval: 5s     # how often do we scrape\n  }\n}\n\nhpa_config = {\n  metrics_adapter = &lt;either 'prometheus-adapter` (recommended) or 'custom-metrics-stackdriver-adapter' &gt;\n  max_replicas\n  min_replicas\n  rules = [{\n    target_query = &lt;see [jetstream-maxtext-module README](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/modules//jetstream-maxtext-deployment/README.md) for a list of valid values&gt;\n    average_value_target\n  }]\n}\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#verify-the-deployment","title":"Verify the deployment","text":"<p>Wait for the containers to finish creating:</p> <pre><code>kubectl get deployment\n\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nmaxengine-server   2/2     2            2           ##s\n</code></pre> <p>Check the Maxengine pod\u2019s logs, and verify the compilation is done. You will see similar logs of the following:</p> <pre><code>kubectl logs deploy/maxengine-server -f -c maxengine-server\n\n2024-03-29 17:09:08,047 - jax._src.dispatch - DEBUG - Finished XLA compilation of jit(initialize) in 0.26236414909362793 sec\n2024-03-29 17:09:08,150 - root - INFO - ---------Generate params 0 loaded.---------\n</code></pre> <p>Check http server logs, this can take a couple minutes:</p> <pre><code>kubectl logs deploy/maxengine-server -f -c jetstream-http\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#send-sample-requests","title":"Send sample requests","text":"<p>Run the following command to set up port forwarding to the http server:</p> <pre><code>kubectl port-forward svc/jetstream-svc 8000:8000\n</code></pre> <p>In a new terminal, send a request to the server:</p> <pre><code>curl --request POST --header \"Content-type: application/json\" -s localhost:8000/generate --data '{\n    \"prompt\": \"What are the top 5 programming languages\",\n    \"max_tokens\": 200\n}'\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>{\n    \"response\": \" in 2021?\\n\\nThe answer to this question is not as simple as it may seem. There are many factors that go into determining the most popular programming languages, and they can change from year to year.\\n\\nIn this blog post, we will discuss the top 5 programming languages in 2021 and why they are so popular.\\n\\n&lt;h2&gt;&lt;strong&gt;1. Python&lt;/strong&gt;&lt;/h2&gt;\\n\\nPython is a high-level programming language that is used for web development, data analysis, and machine learning. It is one of the most popular languages in the world and is used by many companies such as Google, Facebook, and Instagram.\\n\\nPython is easy to learn and has a large community of developers who are always willing to help out.\\n\\n&lt;h2&gt;&lt;strong&gt;2. Java&lt;/strong&gt;&lt;/h2&gt;\\n\\nJava is a general-purpose programming language that is used for web development, mobile development, and game development. It is one of the most popular languages in the\"\n}\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#other-optional-steps","title":"Other optional steps","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#build-and-upload-maxengine-server-image","title":"Build and upload Maxengine Server image","text":"<p>Build the Maxengine Server from here and upload to your project</p> <pre><code>docker build -t maxengine-server .\ndocker tag maxengine-server gcr.io/${PROJECT_ID}/jetstream/maxtext/maxengine-server:latest\ndocker push gcr.io/${PROJECT_ID}/jetstream/maxtext/maxengine-server:latest\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#build-and-upload-http-server-image","title":"Build and upload HTTP Server image","text":"<p>Build the HTTP Server Dockerfile from here and upload to your project</p> <pre><code>docker build -t jetstream-http .\ndocker tag jetstream-http gcr.io/${PROJECT_ID}/jetstream/maxtext/jetstream-http:latest\ndocker push gcr.io/${PROJECT_ID}/jetstream/maxtext/jetstream-http:latest\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#interact-with-the-maxengine-server-directly-using-grpc","title":"Interact with the Maxengine server directly using gRPC","text":"<p>The Jetstream HTTP server is great for initial testing and validating end-to-end requests and responses. If you would like to interact directly with the Maxengine server directly for use cases such as benchmarking, you can do so by following the Jetstream benchmarking setup and applying the <code>deployment.yaml</code> manifest file and interacting with the Jetstream gRPC server at port 9000.</p> <pre><code>kubectl apply -f kubectl/deployment.yaml\n\nkubectl port-forward svc/jetstream-svc 9000:9000\n</code></pre> <p>To run benchmarking, pass in the flag <code>--server 127.0.0.1</code> when running the benchmarking script.</p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/maxtext/single-host-inference/#observe-custom-metrics","title":"Observe custom metrics","text":"<p>This step assumes you specified a metrics port to your jetstream deployment via <code>prometheus_port</code>. If you would like to probe the metrics manually, <code>cURL</code> your maxengine-server container on the metrics port you set and you should see something similar to the following:</p> <pre><code># HELP jetstream_prefill_backlog_size Size of prefill queue\n# TYPE jetstream_prefill_backlog_size gauge\njetstream_prefill_backlog_size{id=\"SOME-HOSTNAME-HERE&gt;\"} 0.0\n# HELP jetstream_slots_used_percentage The percentage of decode slots currently being used\n# TYPE jetstream_slots_used_percentage gauge\njetstream_slots_used_percentage{id=\"&lt;SOME-HOSTNAME-HERE&gt;\",idx=\"0\"} 0.04166666666666663\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/","title":"Serve a LLM using a single-host TPU on GKE with JetStream and PyTorch/XLA","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#background","title":"Background","text":"<p>This tutorial shows you how to serve a large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with JetStream and Jetstream-Pytorch.</p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#set-default-environment-variables","title":"Set default environment variables","text":"<pre><code>gcloud config set project [PROJECT_ID]\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=[COMPUTE_REGION]\nexport ZONE=[ZONE]\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#create-gke-cluster-and-node-pool","title":"Create GKE cluster and node pool","text":"<pre><code># Create zonal cluster with 2 CPU nodes\ngcloud container clusters create jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --project=${PROJECT_ID} \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --release-channel=rapid \\\n    --addons GcsFuseCsiDriver\n    --num-nodes=2\n# Create one v5e TPU pool with topology 2x4 (1 TPU node with 8 chips)\ngcloud container node-pools create tpu \\\n    --cluster=jetstream-maxtext \\\n    --zone=${ZONE} \\\n    --num-nodes=2 \\\n    --machine-type=ct5lp-hightpu-8t \\\n    --project=${PROJECT_ID}\n</code></pre> <p>You have created the following resources:</p> <ul> <li>Standard cluster with 2 CPU nodes.</li> <li>One v5e TPU node pool with 2 nodes, each with 8 chips.</li> </ul>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#configure-applications-to-use-workload-identity","title":"Configure Applications to use Workload Identity","text":"<p>Prerequisite: make sure you have the following roles</p> <pre><code>roles/container.admin\nroles/iam.serviceAccountAdmin\n</code></pre> <p>Follow these steps to configure the IAM and Kubernetes service account:</p> <pre><code># Get credentials for your cluster\n$ gcloud container clusters get-credentials jetstream-maxtext \\\n    --zone=${ZONE}\n# Create an IAM service account.\n$ gcloud iam service-accounts create jetstream-iam-sa\n# Ensure the IAM service account has necessary roles. Here we add roles/storage.objectUser for gcs bucket access.\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.objectUser\n$ gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role roles/storage.insightsCollectorService\n# Allow the Kubernetes default service account to impersonate the IAM service account\n$ gcloud iam service-accounts add-iam-policy-binding jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n# Annotate the Kubernetes service account with the email address of the IAM service account.\n$ kubectl annotate serviceaccount default \\\n    iam.gke.io/gcp-service-account=jetstream-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#create-a-cloud-storage-bucket-to-store-your-model-checkpoint","title":"Create a Cloud Storage bucket to store your model checkpoint","text":"<pre><code>BUCKET_NAME=&lt;your desired gsbucket name&gt;\ngcloud storage buckets create $BUCKET_NAME\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#checkpoint-conversion","title":"Checkpoint conversion","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#option-1-download-weights-from-github","title":"[Option #1] Download weights from GitHub","text":"<p>Follow the instructions here to download the llama-2-7b weights: https://github.com/meta-llama/llama#download </p> <pre><code>ls llama\n\nllama-2-7b tokenizer.model ..\n</code></pre> <p>Upload your weights and tokenizer to your GSBucket</p> <pre><code>gcloud storage cp -r llama-2-7b/* gs://BUCKET_NAME/llama-2-7b/base/\ngcloud storage cp tokenizer.model gs://BUCKET_NAME/llama-2-7b/base/\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#option-2-download-weights-from-huggingface","title":"[Option #2] Download weights from HuggingFace","text":"<p>Accept the terms and conditions from https://huggingface.co/meta-llama/Llama-2-7b-hf.</p> <p>For llama-3-8b: https://huggingface.co/meta-llama/Meta-Llama-3-8B.</p> <p>For gemma-2b: https://huggingface.co/google/gemma-2b-pytorch.</p> <p>Obtain a HuggingFace CLI token by going to your HuggingFace settings and under the <code>Access Tokens</code>, generate a <code>New token</code>. Edit permissions to your access token to have read access to your respective checkpoint repository.</p> <p>Copy your access token and create a Secret to store the HuggingFace token</p> <pre><code>kubectl create secret generic huggingface-secret \\\n  --from-literal=HUGGINGFACE_TOKEN=&lt;access_token&gt;\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#apply-the-checkpoint-conversion-job","title":"Apply the checkpoint conversion job","text":"<p>For the following models, replace the following arguments in <code>checkpoint-job.yaml</code></p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#llama-2-7b-hf","title":"Llama-2-7b-hf","text":"<pre><code>- -s=jetstream-pytorch\n- -m=meta-llama/Llama-2-7b-hf\n- -o=gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/\n- -n=llama-2\n- -q=False\n- -h=True\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#llama-3-8b","title":"Llama-3-8b","text":"<pre><code>- -s=jetstream-pytorch\n- -m=meta-llama/Meta-Llama-3-8B\n- -o=gs://BUCKET_NAME/pytorch/llama-3-8b/final/bf16/\n- -n=llama-3\n- -q=False\n- -h=True\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#gemma-2b","title":"Gemma-2b","text":"<pre><code>- -s=jetstream-pytorch\n- -m=google/gemma-2b-pytorch\n- -o=gs://BUCKET_NAME/pytorch/gemma-2b/final/bf16/\n- -n=gemma\n- -q=False\n- -h=True\n</code></pre> <p>Run the checkpoint conversion job. This will use the checkpoint conversion script from Jetstream-pytorch to create a compatible Pytorch checkpoint</p> <p>Please make sure you edit <code>checkpoint-job</code> and replace all occurrences of <code>BUCKET_NAME</code> with the <code>BUCKET_NAME</code> that you have set above.</p> <pre><code>kubectl apply -f checkpoint-job.yaml\n</code></pre> <p>Observe your checkpoint</p> <pre><code>kubectl logs -f jobs/checkpoint-converter\n# This can take several minutes\n...\nCompleted uploading converted checkpoint from local path /pt-ckpt/ to GSBucket gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/\"\n</code></pre> <p>Now your converted checkpoint will be located in <code>gs://BUCKET_NAME/pytorch/llama-2-7b/final/bf16/</code></p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#deploy-the-jetstream-pytorch-server","title":"Deploy the Jetstream Pytorch server","text":"<p>The following flags are set in the manifest file</p> <pre><code>--size: Size of model\n--model_name: Name of model (llama-2, llama-3, gemma)\n--batch_size: Batch size\n--max_cache_length: Maximum length of kv cache \n--tokenizer_path: Path to model tokenizer file\n--checkpoint_path: Path to checkpoint\nOptional flags to add\n--quantize_weights (Default False): Checkpoint is quantized\n--quantize_kv_cache (Default False): Quantized kv cache\n</code></pre> <p>For llama3-8b, you can use the following arguments:</p> <pre><code>- --size=8b\n- --model_name=llama-3\n- --batch_size=80\n- --max_cache_length=2048\n- --quantize_weights=False\n- --quantize_kv_cache=False\n- --tokenizer_path=/models/pytorch/llama3-8b/final/bf16/tokenizer.model\n- --checkpoint_path=/models/pytorch/llama3-8b/final/bf16/model.safetensors\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#verify-the-deployment","title":"Verify the deployment","text":"<pre><code>kubectl get deployment\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\njetstream-pytorch-server    2/2       2            2         ##s\n</code></pre> <p>View the HTTP server logs to check that the model has been loaded and compiled. It may take the server a few minutes to complete this operation.</p> <pre><code>kubectl logs deploy/jetstream-pytorch-server -f -c jetstream-http\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre> <p>View the Jetstream Pytorch server logs and verify that the compilation is done.</p> <pre><code>kubectl logs deploy/jetstream-pytorch-server -f -c jetstream-pytorch-server\nStarted jetstream_server....\n2024-04-12 04:33:37,128 - root - INFO - ---------Generate params 0 loaded.---------\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#serve-the-model","title":"Serve the model","text":"<pre><code>kubectl port-forward svc/jetstream-svc 8000:8000\n</code></pre> <p>Interact with the model via curl</p> <pre><code>curl --request POST \\\n--header \"Content-type: application/json\" \\\n-s \\\nlocalhost:8000/generate \\\n--data \\\n'{\n    \"prompt\": \"What are the top 5 programming languages\",\n    \"max_tokens\": 200\n}'\n</code></pre> <p>The initial request can take several seconds to complete due to model warmup. The output is similar to the following:</p> <pre><code>{\n    \"response\": \" for 2019?\\nWhat are the top 5 programming languages for 2019? The top 5 programming languages for 2019 are Python, Java, JavaScript, C, and C++.\\nWhat are the top 5 programming languages for 2019? The top 5 programming languages for 2019 are Python, Java, JavaScript, C, and C++. These languages are used in a variety of industries and are popular among developers.\\nPython is a versatile language that can be used for web development, data analysis, and machine learning. It is easy to learn and has a large community of developers.\\nJava is a popular language for enterprise applications and is used by many large companies. It is also used for mobile development and has a large community of developers.\\nJavaScript is a popular language for web development and is used by many websites. It is also used for mobile development and has a\"\n}\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#optionals","title":"Optionals","text":""},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#interact-with-the-jetstream-pytorch-server-directly-using-grpc","title":"Interact with the Jetstream Pytorch server directly using gRPC","text":"<p>The Jetstream HTTP server is great for initial testing and validating end-to-end requests and responses. In production use case, it's recommended to interact with the JetStream-Pytorch server directly for better throughput/latency and use the streaming decode feature on the JetStream grpc server.</p> <pre><code>kubectl port-forward svc/jetstream-svc 9000:9000\n</code></pre> <p>Now you can interact with the JetStream grpc server directly via port 9000.</p>"},{"location":"tutorials-and-examples/inference-servers/jetstream/pytorch/single-host-inference/#use-a-persistent-disk-to-host-your-checkpoint","title":"Use a Persistent Disk to host your checkpoint","text":"<p>Create a GCE CPU VM to do your checkpoint conversion. </p> <pre><code>gcloud compute instances create jetstream-ckpt-converter \\\n  --zone=us-central1-a \\\n  --machine-type=n2-standard-32 \\\n  --scopes=https://www.googleapis.com/auth/cloud-platform \\\n  --image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20230919 \\\n  --boot-disk-size=128GB \\\n  --boot-disk-type=pd-balanced\n</code></pre> <p>SSH into the VM and install python and git</p> <pre><code>gcloud compute ssh jetstream-ckpt-converter --zone=us-central1-a\nsudo apt update\nsudo apt-get install python3-pip\nsudo apt-get install git-all\n</code></pre> <p>In your CPU VM, follow these instructions to do the following: 1. Clone the jetstream-pytorch repository 2. Run the installation script 3. Download and convert llama-2-7b weights</p> <p>After running weight safetensor convert, you should see the following files in the directory you have saved them to:</p> <pre><code>ls &lt;directory where checkpoint is stored&gt;\nmodel.safetensors  params.json\n</code></pre> <p>Create a persistent disk to store your checkpoint</p> <pre><code>gcloud compute disks create jetstream-pytorch-ckpt --zone=us-west4-a --type pd-balanced\nNAME                    ZONE        SIZE_GB  TYPE         STATUS\npytorch-jetstream-ckpt  us-west4-a  100      pd-balanced  READY\n</code></pre> <p>Attach the disk to your VM</p> <pre><code>gcloud compute instances attach-disk jetstream-ckpt-converter \\\n  --disk jetstream-pytorch-ckpt --project $PROJECT_ID --zone us-west4-a\n</code></pre> <p>Identity your disk, it will be similar to the following but may also have a different name:</p> <pre><code>lsblk\nNAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\n...\nsdc       8:32   0   100G  0 disk \n</code></pre> <p>Format your disk and create a directory in its mount folder</p> <pre><code>sudo mkfs.ext4 /dev/sdc\nmkdir /mnt/jetstream-pytorch-ckpt\nsudo mount /dev/sdc /mnt/jetstream-pytorch-ckpt\n</code></pre> <p>Copy your converted checkpoint folder into /mnt/jetstream-pytorch-ckpt</p> <pre><code>cp &lt;path to converterted checkpoint&gt; /mnt/jetstream-pytorch-ckpt\n</code></pre> <p>Unmount and detach your persistent disk</p> <pre><code>sudo umount /mnt/jetstream-pytorch-ckpt\ngcloud compute instances detach-disk jetstream-ckpt-converter \\\n  --disk jetstream-pytorch-ckpt --project $PROJECT_ID --zone us-west4-a\n</code></pre> <p>Apply your storage and deployment manifest file</p> <pre><code>kubectl apply -f storage.yaml\nkubectl apply -f pd-deployment.yaml\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/","title":"High-performance diffusion model inference on GKE and TPU using MaxDiffusion","text":""},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#about-maxdiffusion","title":"About MaxDiffusion","text":"<p>Just as LLMs have revolutionized natural language processing, diffusion models are transforming the field of computer vision. To reduce our customers\u2019 costs of deploying these models, Google created MaxDiffusion: a collection of open-source diffusion-model reference implementations. These implementations are written in JAX and are highly performant, scalable, and customizable \u2013 think MaxText for computer vision. </p> <p>MaxDiffusion provides high-performance implementations of core components of diffusion models such as cross attention, convolutions, and high-throughput image data loading. MaxDiffusion is designed to be highly adaptable and customizable: whether you're a researcher pushing the boundaries of image generation or a developer seeking to integrate cutting-edge gen AI capabilities into your applications, MaxDiffusion provides the foundation you need to succeed.</p>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#prerequisites","title":"Prerequisites","text":"<p>Access to a Google Cloud project with the TPU v5e available and enough quota in the region you select. In the walk-through, we only choose lower end TPU v5e, single host with 1x1 chips</p> <p>A computer terminal with kubectl and the Google Cloud SDK installed. From the GCP project console you\u2019ll be working with, you may want to use the included Cloud Shell as it already has the required tools installed.</p> <p>MaxDiffsusion will access Huggingface to download Stable Diffusion XL model(stabilityai/stable-diffusion-xl-base-1.0), while no huggingface access token required for access</p>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#setup-project-environments","title":"Setup project environments","text":"<p>Set the project and region that have availability for TPU v5e( alternatively, you can choose other regions with different TPU v5e accelerator type available):</p> <pre><code>export PROJECT_ID=&lt;your-project-id&gt;\nexport REGION=us-east1\nexport ZONE_1=${REGION}-c # You may want to change the zone letter based on the region you selected above\n\nexport CLUSTER_NAME=tpu-cluster\ngcloud config set project \"$PROJECT_ID\"\ngcloud config set compute/region \"$REGION\"\ngcloud config set compute/zone \"$ZONE_1\"\n</code></pre> <p>Then, enable the required APIs to create a GK cluster:</p> <pre><code>gcloud services enable compute.googleapis.com container.googleapis.com\n</code></pre> <p>Also, you may go ahead download the source code repo for this exercise, :</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git\ncd tutorials-and-examples/tpu-examples/maxdiffusion\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#create-gke-cluster-and-nodepools","title":"Create GKE Cluster and Nodepools","text":""},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#gke-cluster","title":"GKE Cluster","text":"<p>Now, create a GKE cluster with a minimal default node pool, as you will be adding a node pool with TPU v5e later on:</p> <pre><code>gcloud container clusters create $CLUSTER_NAME --location ${REGION} \\\n  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n  --enable-image-streaming --enable-shielded-nodes \\\n  --shielded-secure-boot --shielded-integrity-monitoring \\\n  --enable-ip-alias \\\n  --node-locations=$REGION-b \\\n  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n  --addons GcsFuseCsiDriver   \\\n  --no-enable-master-authorized-networks \\\n  --machine-type n2d-standard-4 \\\n  --cluster-version 1.29 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 3 \\\n  --ephemeral-storage-local-ssd=count=2 \\\n  --scopes=\"gke-default,storage-rw\"\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#nodepool","title":"Nodepool","text":"<p>Create an additional Spot node pool (we use spot to save costs, you can remove spot option depends on different use case) with TPU accelerator:</p> <pre><code>cloud container node-pools create $CLUSTER_NAME-tpu \\\n--location=$REGION --cluster=$CLUSTER_NAME --node-locations=$ZONE_1 \\\n--machine-type=ct5lp-hightpu-1t --num-nodes=0 --spot --node-version=1.29 \\\n--ephemeral-storage-local-ssd=count=0 --enable-image-streaming \\\n--shielded-secure-boot --shielded-integrity-monitoring \\\n--enable-autoscaling --total-min-nodes 0 --total-max-nodes 2 --location-policy=ANY\n</code></pre> <p>Note how easy enabling TPU in GKE nodepool with proper TPU machine type. Please refer to the following page, for details on TPU v5e machine type and configuration sections.</p> <p>After a few minutes, check that the node pool was created correctly:</p> <pre><code>gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION --project $PROJECT_ID\ngcloud container node-pools list --region $REGION --cluster $CLUSTER_NAME\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#explaination-on-maxdiffusion-inference-server-sample-code","title":"Explaination on MaxDiffusion inference server sample code","text":"<p>The sample Stable Diffusion XL inference code template from MaxDiffusion repo is updated with FastAPI and uvicorn libraries to fit for API requests.</p> <p>The updated Stable Diffusion XL Inference Code sample provided here for reference.</p>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#add-fastapi-and-uvicorn-libraries","title":"Add FastAPI and Uvicorn libraries","text":""},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#add-logging-health-check","title":"Add logging, health check","text":""},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#expose-generate-as-post-methods-for-rest-api-requests","title":"Expose /generate as Post methods for REST API requests","text":""},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#huggingface-stable-diffusion-xl-model-stabilityaistable-diffusion-xl-base-10","title":"HuggingFace Stable Diffusion XL Model: stabilityai/stable-diffusion-xl-base-1.0","text":"<p>To make the Stable Diffusion XL inference more efficient, we compile the pipeline._generate function, and pass all parameters to the function and tell JAX which are static arguments,</p> <p>default_seed = 33</p> <p>default_guidance_scale = 5.0</p> <p>default_num_steps = 40</p> <p>width = 1024</p> <p>height = 1024</p> <p>The following main exposed SDXL inference method,</p> <pre><code>@app.post(\"/generate\")\nasync def generate(request: Request):\n    LOG.info(\"start generate image\")\n    data = await request.json()\n    prompt = data[\"prompt\"]\n    LOG.info(prompt)\n    prompt_ids, neg_prompt_ids = tokenize_prompt(prompt, default_neg_prompt)\n    prompt_ids, neg_prompt_ids, rng = replicate_all(prompt_ids, neg_prompt_ids, default_seed)\n    g = jnp.array([default_guidance_scale] * prompt_ids.shape[0], dtype=jnp.float32)\n    g = g[:, None]\n    LOG.info(\"call p_generate\")\n    images = p_generate(prompt_ids, p_params, rng, g, None, neg_prompt_ids)\n\n    # convert the images to PIL\n    images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\n    images=pipeline.numpy_to_pil(np.array(images))\n    buffer = io.BytesIO()\n    LOG.info(\"Save image\")\n    for i, image in enumerate(images):\n        if i==0:\n          image.save(buffer, format=\"PNG\")\n    #await images[0].save(buffer, format=\"PNG\")\n\n    # Return the image as a response\n    return Response(content=buffer.getvalue(), media_type=\"image/png\")\n\nif __name__ == \"__main__\":\n   uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False, log_level=\"debug\")\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#build-stable-diffusion-xl-inference-container-image","title":"Build Stable Diffusion XL Inference Container Image","text":"<p>Next, let\u2019s build the Inference server container image with cloud build.</p> <p>Sample Docker file and cloudbuild.yaml already under directory buid/server/ downloaded repo,</p> <p>buid/server/Dockerfile:</p> <pre><code>FROM python:3.11-slim\nWORKDIR /app\nRUN apt-get -y update\nRUN apt-get -y install git\nCOPY requirements.txt ./\nRUN python -m pip install --upgrade pip\nRUN pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\nRUN pip install -r requirements.txt\nRUN pip install git+https://github.com/google/maxdiffusion.git\nCOPY main.py ./\nEXPOSE 8000\nENTRYPOINT [\"python\", \"main.py\"]\n</code></pre> <p>Notes: we may not have maxdiffusion pip package to be downloaded yet, thus, RUN pip install git+https://github.com/google/maxdiffusion.git is used to download MaxDiffusion from source directly.</p> <p>cloudbuild.yaml:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: [ 'build', '-t', 'us-east1-docker.pkg.dev/$PROJECT_ID/gke-llm/max-diffusion:latest', '.' ]\nimages:\n- 'us-east1-docker.pkg.dev/$PROJECT_ID/gke-llm/max-diffusion:latest'\n</code></pre> <p>Note: replace destination of container image as your own environment</p> <p>Run the following commands to kick of container image builds:</p> <pre><code>cd build/server\ngcloud builds submit .\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#deploy-stable-diffusion-xl-inference-server-in-gke","title":"Deploy Stable Diffusion XL Inference Server in GKE","text":"<p>In the downloaded code repo root directory, you can check the following kubernetes deployment resource files,</p> <p>serve_sdxl_v5e.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stable-diffusion-deployment\nspec:\n  selector:\n    matchLabels:\n      app: max-diffusion-server\n  replicas: 1  # number of nodes in node-pool\n  template:\n    metadata:\n      labels:\n        app: max-diffusion-server\n    spec:\n      nodeSelector:\n        cloud.google.com/gke-tpu-topology: 1x1 #  target topology\n        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\n      volumes:\n       - name: dshm\n         emptyDir:\n              medium: Memory\n      containers:\n      - name: serve-stable-diffusion\n        image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/max-diffusion:latest\n        securityContext:\n          privileged: true\n        env:\n        - name: MODEL_NAME\n          value: 'stable_diffusion'\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            google.com/tpu: 1  # TPU chip request\n          limits:\n            google.com/tpu: 1  # TPU chip request\n        volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: max-diffusion-server\n  labels:\n    app: max-diffusion-server\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8000\n      targetPort: http\n      name: http-max-diffusion-server\n  selector:\n    app: max-diffusion-server\n</code></pre> <p>To be noted, in deployment specs settings related to TPU accelerators which has to match v5e machine types (ct5lp-hightpu-1t has 1x1=1 total TPU chips):</p> <pre><code>nodeSelector:\n        cloud.google.com/gke-tpu-topology: 1x1 #  target topology\n        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\n\nresources:\n          requests:\n            google.com/tpu: 1  # TPU chip request\n          limits:\n            google.com/tpu: 1  # TPU chip request\n</code></pre> <p>We use type: ClusterIP to expose inference service availabe to GKE cluster only. Update this file with proper container image name and location,</p> <p>Run the following command to deploy Stable Diffusion Inference server to GKE:</p> <pre><code>gcloud container clusters get-credentials $CLUSTER_NAME $REGION\nkubectl apply -f serve_sdxl_v5e.yaml\nkubectl get svc max-diffusion-server. \n</code></pre> <p>It may take 7\u20138 minutes to wait for the spot nodepool provisioning, model loading and initial pipeline compilation. Note the service IP need to be referenced by later section</p> <p>You may use the following command to validate Stable Diffusion Inference Server setup properly,</p> <pre><code>SERVER_URL=XXXX\nkubectl run -it busybox --image radial/busyboxplus:curl\n\n\ncurl SERVER_URL:8000\n</code></pre> <p>If you have issues such as connection refused in curl command validations, you may use the following command instead to create a service through kubectl command:</p> <pre><code>kubectl expose deployment max-diffusion-deployment max-diffusion-service --port 8000 --protocol tcp --target-port 8000\n\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#deploy-webapp","title":"Deploy WebApp","text":"<p>A simple client webapp provided under build/webapp directory with following files included:</p> <p>app.py ( main python file), Dockerfile , cloudbuild.yaml , requirements.txt</p> <p>You may update cloudbuild.yaml file with your own container image destination accordingly.</p> <p>Run the following command to build testing webapp container image using cloud build:</p> <pre><code>cd build/webapp\ngcloud builds submit .\n</code></pre> <p>Once the webapp image build completed, you may go ahead deploy frontend webapp to test Stable Diffusion XL inference server.</p> <p>serve_sdxl_client.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: max-diffusion-client\nspec:\n  selector:\n    matchLabels:\n      app: max-diffusion-client\n  template:\n    metadata:\n      labels:\n        app: max-diffusion-client\n    spec:\n      containers:\n      - name: webclient\n        image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/max-diffusion-client:latest\n        env:\n          - name: SERVER_URL\n            value: \"http://CLusterIP:8000\"\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 5000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: max-diffusion-client-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: max-diffusion-client\n  ports:\n  - port: 8080\n    targetPort: 5000\n</code></pre> <p>We use type: LoadBalancer to expose webapp to external public. Update this file with proper container image name and SERVER_URL endpoint location from. Run the following command to deploy frontend webapp:</p> <pre><code>kubectl apply -f serve_sdxl_client.yaml\n</code></pre> <p>Once the webapp deployment completed, you may test text to image capabilities from browser: http://webappServiceIP:8080/</p> <p>Image generated after 3\u20135s( displaying only first image from inference), which is quite performance efficient for Cloud TPU v5e based on Single-host serving for one single v5e chips.</p>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#cleanups","title":"Cleanups","text":"<p>Don\u2019t forget to clean up the resources created in this article once you\u2019ve finished experimenting with Stable Diffusion inference on GKE and TPU, as keeping the cluster running for a long time can incur in important costs. To clean up, you just need to delete the GKE cluster:</p> <pre><code>gcloud container clusters delete $CLUSTER_NAME - region $REGION\n</code></pre>"},{"location":"tutorials-and-examples/inference-servers/maxdiffusion/#conclusion","title":"Conclusion","text":"<p>With the streamlined process showcased in this post, deploying inference servers for open-source image generation/vision models like Stable Diffusion XL on GKE and TPU has never been simpler or more efficient with MaxDiffusion to serve JAX models directly, without need for download and conversion JAX model to Tensorflow compatible model for Stable Diffusion Inference Serving in GKE and TPU</p> <p>Don\u2019t forget to check out other GKE related resources on AI ML infrastructure offered by Google Cloud and check the resources included in the AI/ML orchestration on GKE documentation.</p> <p>The blog published linked to this repo</p>"},{"location":"tutorials-and-examples/kserve/","title":"KServe on GKE Autopilot","text":"<p>KServe is a highly scalable, standards-based platform for model inference on Kubernetes. Installing KServe on GKE Autopilot can be challenging due to the security policies enforced by Autopilot. This tutorial will guide you step by step through the process of installing KServe in a GKE Autopilot cluster.</p> <p>Additionally, this tutorial includes an example of serving Gemma2 with vLLM in KServe, demonstrating how to utilize GPU resources in KServe on Google Kubernetes Engine (GKE).</p>"},{"location":"tutorials-and-examples/kserve/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Ensure you have a gcp project with billing enabled and enabled the GKE API. </p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> <li>helm</li> </ol>"},{"location":"tutorials-and-examples/kserve/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<ol> <li>Set the default environment variables:</li> </ol> <pre><code>export PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport CLUSTER_NAME=kserve-demo\n</code></pre> <ol> <li>Create a GKE Autopilot cluster:</li> </ol> <pre><code>gcloud container clusters create-auto ${CLUSTER_NAME} \\\n    --location=$REGION \\\n    --project=$PROJECT_ID \\\n    --workload-policies=allow-net-admin\n\n# Get credentials\ngcloud container clusters get-credentials ${CLUSTER_NAME} \\\n--region ${REGION} \\\n--project ${PROJECT_ID}\n</code></pre> <p>If you're using an existing cluster, ensure it is updated to allow net admin permissions. This is necessary for the installation of Istio later on:</p> <pre><code>gcloud container clusters update ${CLUSTER_NAME} \\\n--region=${REGION}\n--project=$PROJECT_ID \\\n--workload-policies=allow-net-admin \n</code></pre>"},{"location":"tutorials-and-examples/kserve/#install-kserve","title":"Install KServe","text":"<p>KServe relies on Knative and requires a networking layer. In this tutorial, we will use Istio, the networking layer that integrates best with Knative.</p> <ol> <li>Install Knative</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-core.yaml\n</code></pre> <p>Note:  You will see warnings that Autopilot mutated the CRDs during this tutorial. These warnings are safe to ignore.</p> <ol> <li>Install Istio</li> </ol> <pre><code>helm repo add istio https://istio-release.storage.googleapis.com/charts\nhelm repo update\nkubectl create namespace istio-system\nhelm install istio-base istio/base -n istio-system --set defaultRevision=default\nhelm install istiod istio/istiod -n istio-system --wait\nhelm install istio-ingressgateway istio/gateway -n istio-system\n\n# Verify the installation\nkubectl get deployments -n istio-system\n\n# Example Output\nNAME                   READY   UP-TO-DATE   AVAILABLE   AGE\nistio-ingressgateway   1/1     1            1           17h\nistiod                 1/1     1            1           20h\n</code></pre> <ol> <li>Install Knative-Istio</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.1/net-istio.yaml\n\n# Verify the installation\nkubectl get pods -n knative-serving\n\n# Example Output\nNAME                                    READY   STATUS    RESTARTS      AGE\nactivator-749cf94f87-b7p9n              1/1     Running   0             17m\nautoscaler-5c764b5f7d-m8zvk             1/1     Running   1 (14m ago)   17m\ncontroller-5649f5bbb7-wvlmk             1/1     Running   4 (13m ago)   17m\nnet-istio-controller-7f8dfbddb7-d8cmq   1/1     Running   0             18s\nnet-istio-webhook-54ffc96585-cpgfl      2/2     Running   0             18s\nwebhook-64c67b4fc-smdtl                 1/1     Running   3 (13m ago)   17m\n</code></pre> <ol> <li>Install DNS In this tutorial we use Magic DNS. To configure a real DNS, follow the steps here.</li> </ol> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.1/serving-default-domain.yaml\n</code></pre> <ol> <li>Install Cert Manager, which is required to provision webhook certs for production grade installation.</li> </ol> <pre><code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.15.3 --set crds.enabled=true --set global.leaderElection.namespace=cert-manager\n</code></pre> <ol> <li>Install Kserve and Kserve cluster runtimes</li> </ol> <pre><code>kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.14.0-rc0/kserve.yaml\n\n# Wait until kserve-controller-manager is ready\nkubectl rollout status deployment kserve-controller-manager -n kserve\n\n# Install cluster runtimes\nkubectl apply -f https://github.com/kserve/kserve/releases/download/v0.14.0-rc0/kserve-cluster-resources.yaml\n\n# View these runtimes\nkubectl get ClusterServingRuntimes -n kserve\n</code></pre> <ol> <li>To request accelerators (GPUs) for your Google Kubernetes Engine (GKE) Autopilot workloads, nodeSelector is used in the manifest. Therefore, we will enable nodeSelector in Knative, which is disabled by default.</li> </ol> <pre><code>kubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-nodeselector\":\"enabled\", \"kubernetes.podspec-tolerations\":\"enabled\"}}'\n\n# restart knative webhook to consume the config, for example\nkubectl get pods -n knative-serving\n# Find the webhook pod and delete it to restart the pod.\nkubeclt delete pod webhook-64c67b4fc-nmzwt -n knative-serving\n</code></pre> <p>After successfully installing KServe, you can now explore various examples such as, first inference service, canary rollout, inference batcher and auto-scaling. In the next step, we'll demonstrate how to deploy Gemma2 using vLLM in KServe with GKE Autopilot.</p>"},{"location":"tutorials-and-examples/kserve/#deploy-gemma2-served-with-vllm","title":"Deploy Gemma2 served with vllm.","text":"<ol> <li> <p>Generate a hugging face access token follow these steps. Specify a Name of your choice and a Role of at least Read. </p> </li> <li> <p>Make sure you accepted the term to use gemma2 in hugging face.</p> </li> <li> <p>Create the hugging face token</p> </li> </ol> <pre><code>kubectl create namespace kserve-test\n\n# Specify your hugging face token.\nexport HF_TOKEN = XXX\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n    name: hf-secret\n    namespace: kserve-test\ntype: Opaque\nstringData:\n    hf_api_token: ${HF_TOKEN}\nEOF\n\n</code></pre> <ol> <li>Create the inference service</li> </ol> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-gemma2\n  namespace: kserve-test\nspec:\n  predictor:\n    nodeSelector:\n      cloud.google.com/gke-accelerator: nvidia-l4\n      cloud.google.com/gke-accelerator-count: \"1\"\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n        - --enable_docs_url=True\n        - --model_name=gemma2\n        - --model_id=google/gemma-2-2b\n      env:\n      - name: HF_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: hf-secret\n            key: hf_api_token\n      resources:\n        limits:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\nEOF\n</code></pre> <p>Wait for the service to be ready:</p> <pre><code>kubectl get inferenceservice huggingface-gemma2 -n kserve-test\nkubectl get pods -n kserve-test\n\n# Replace pod_name with the correct pod name.\nkubectl events --for pod/POD_NAME -n kserve-test --watch\n</code></pre>"},{"location":"tutorials-and-examples/kserve/#test-the-inference-service","title":"Test the Inference Service","text":"<ol> <li>Find the URL returned in kubectl get inferenceservice</li> </ol> <pre><code>URL=$(kubectl get inferenceservice huggingface-gemma2 -o jsonpath='{.status.url}')\n\n# URL should look like this:\nhttp://huggingface-gemma2.kserve-test.34.121.87.225.sslip.io\n</code></pre> <ol> <li> <p>Open the swagger UI at $URL/docs</p> </li> <li> <p>Play with the openai chat API with the example input below. Click execute and you can see the response.</p> </li> </ol> <pre><code>{\n    \"model\": \"gemma2\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an assistant that speaks like Shakespeare.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a poem about colors\"\n        }\n    ],\n    \"max_tokens\": 30,\n    \"stream\": false\n}\n</code></pre>"},{"location":"tutorials-and-examples/kserve/#clean-up","title":"Clean up","text":"<p>Delete the GKE cluster.</p> <pre><code>gcloud container clusters delete ${CLUSTER_NAME} \\\n    --location=$REGION \\\n    --project=$PROJECT_ID \\\n</code></pre>"},{"location":"tutorials-and-examples/models-as-oci/","title":"Package and Deploy from Hugging Face to Artifact Registry and GKE","text":"<p>This repository contains a Google Cloud Build configuration for building and pushing Docker images of Hugging Face models to Google Artifact Registry.</p>"},{"location":"tutorials-and-examples/models-as-oci/#overview","title":"Overview","text":"<p>This project allows you to download a Hugging Face model and package it as a Docker image. The Docker image can then be pushed to Google Artifact Registry for deployment or distribution. Build time can be significant for large models, it is recommended to not exceed models above 10 billion parameters. For reference 8b model roughly takes 35 minutes to build and push with this cloudbuild config.</p>"},{"location":"tutorials-and-examples/models-as-oci/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Google Cloud project with billing enabled.</li> <li>Google Cloud SDK installed and authenticated.</li> <li>Access to Google Cloud Build and Artifact Registry.</li> <li>A Hugging Face account with an access token.</li> </ul>"},{"location":"tutorials-and-examples/models-as-oci/#setupcreate-a-secret-for-hugging-face-token","title":"SetupCreate a Secret for Hugging Face Token","text":"<ol> <li>Clone the Repository</li> </ol> <p><code>bash    git clone https://github.com/your-username/your-repo-name.git    cd your-repo-name 2. **Create a Secret for Hugging Face Token**</code>bash     echo \"your_hugging_face_token\" | gcloud secrets create huggingface-token --data-file=-</p>"},{"location":"tutorials-and-examples/models-as-oci/#configuration","title":"Configuration","text":""},{"location":"tutorials-and-examples/models-as-oci/#substitutions","title":"Substitutions","text":"<p>The following substitutions are defined in the <code>cloudbuild.yaml</code> file, they can be changed by passing <code>--substitutions SUBSTITUTION_NAME=SUBSTITUTION_VALUE</code> to <code>gcloud builds submit</code>:</p> <ul> <li><code>_MODEL_NAME</code>: The name of the Hugging Face model to download (default: <code>huggingfaceh4/zephyr-7b-beta</code>).</li> <li><code>_REGISTRY</code>: The URL for the Docker registry (default: <code>us-docker.pkg.dev</code>).</li> <li><code>_REPO</code>: The name of the Artifact Registry repository (default: <code>cloud-blog-oci-models</code>).</li> <li><code>_IMAGE_NAME</code>: The name of the Docker image to be created (default: <code>zephyr-7b-beta</code>).</li> <li><code>_CLOUD_SECRET_NAME</code>: The name of the secret storing the Hugging Face token (default: <code>huggingface-token</code>).</li> </ul>"},{"location":"tutorials-and-examples/models-as-oci/#options","title":"Options","text":"<p>The following options are configured in the <code>cloudbuild.yaml</code> file:</p> <ul> <li><code>diskSizeGb</code>: The size of the disk for the build, specified in gigabytes (default: <code>100</code>). can be changed by passing <code>--disk-size=DISK_SIZE</code> to <code>gcloud builds submit</code></li> <li><code>machineType</code>: The machine type can be set by passing <code>--machine-type=</code> in <code>gcloud builds submit</code></li> </ul>"},{"location":"tutorials-and-examples/models-as-oci/#usage","title":"Usage","text":"<p>To trigger the Cloud Build and create the Docker image, run the following command:</p> <pre><code>gcloud builds submit --config cloudbuild.yaml --substitutions _MODEL_NAME=\"your_model_name\",_IMAGE_NAME=\"LOCATION-docker.pkg.dev/[YOUR_PROJECT_ID]/[REPOSITORY_NAME]/[IMAGE_NAME]\"\n</code></pre>"},{"location":"tutorials-and-examples/models-as-oci/#usage_1","title":"Usage","text":""},{"location":"tutorials-and-examples/models-as-oci/#inside-an-inference-deployment-dockerfile","title":"Inside an Inference Deployment Dockerfile","text":""},{"location":"tutorials-and-examples/models-as-oci/#example","title":"Example","text":"<pre><code># Start from the PyTorch base image with CUDA and cuDNN support\nFROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-devel\n\n# Set the working directory\nWORKDIR /srv\n\n# Install vllm (version 0.3.3)\nRUN pip install vllm==0.3.3 --no-cache-dir\n\n# Import the model from the 'model-as-image'\nFROM model-as-image as model\n\n# Copy the model files from 'model-as-image' into the inference container\nCOPY --from=model /model/ /srv/models/$MODEL_DIR/\n\n# Define the entrypoint to run the VLLM OpenAI API server\nENTRYPOINT [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n            \"--host\", \"0.0.0.0\", \"--port\", \"80\", \\\n            \"--model\", \"/srv/models/$MODEL_DIR\", \\\n            \"--dtype=half\"]\n</code></pre>"},{"location":"tutorials-and-examples/models-as-oci/#mount-the-image-as-to-your-inference-deployment","title":"Mount the image as to your inference deployment","text":"<p>You can mount the image to a shared volume in your inference deployment via a sidecar </p>"},{"location":"tutorials-and-examples/models-as-oci/#example_1","title":"example","text":"<pre><code>initContainers:\n  - name: model\n    image: model-as-image\n    restartPolicy: Always\n    args:\n    - \"sh\"\n    - \"-c\"\n    - \"ln -s /model /mnt/model &amp;&amp; sleep infinity\"\n    volumeMounts:\n    - mountPath: /mnt/model\n      name: model-image-mount\n      readOnly: False\nvolumes:\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  - name: llama3-model\n    emptyDir: {}\n</code></pre> <p>Mount the same volume to your inference container and consume it there.  Pulling images can be optimized in Google Kubernetes Engine with image streaming and secondary boot disk. These method can be used for packaging and mass distributing small/medium size models and low rank adapters of foundational models. </p>"},{"location":"tutorials-and-examples/nvidia-nim/","title":"NVIDIA NIM on GKE","text":""},{"location":"tutorials-and-examples/nvidia-nim/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Get access to NVIDIA NIMs</p> <p>[!IMPORTANT] Before you proceed further, ensure you have the NVIDIA AI Enterprise License (NVAIE) to access the NIMs.  To get started, go to build.nvidia.com and provide your company email address</p> </li> <li> <p>In the Google Cloud console, on the project selector page, select or create a new project with billing enabled</p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> <li>git</li> <li>jq</li> <li> <p>ngc</p> </li> <li> <p>Enable the required APIs</p> </li> </ol> <pre><code>gcloud services enable \\\n  container.googleapis.com \\\n  file.googleapis.com\n</code></pre>"},{"location":"tutorials-and-examples/nvidia-nim/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<ol> <li>Choose your region and set your project and machine variables:</li> </ol> <pre><code>export PROJECT_ID=$(gcloud config get project)\nexport REGION=us-central1\nexport ZONE=${REGION?}-b\nexport MACH=a2-highgpu-1g\nexport GPU_TYPE=nvidia-tesla-a100\nexport GPU_COUNT=1\n</code></pre> <ol> <li>Create a GKE cluster:</li> </ol> <pre><code>gcloud container clusters create nim-demo --location ${REGION?} \\\n  --workload-pool ${PROJECT_ID?}.svc.id.goog \\\n  --enable-image-streaming \\\n  --enable-ip-alias \\\n  --node-locations ${ZONE?} \\\n  --workload-pool=${PROJECT_ID?}.svc.id.goog \\\n  --addons=GcpFilestoreCsiDriver  \\\n  --machine-type n2d-standard-4 \\\n  --num-nodes 1 --min-nodes 1 --max-nodes 5 \\\n  --ephemeral-storage-local-ssd=count=2\n</code></pre> <ol> <li>Create a nodepool</li> </ol> <pre><code>gcloud container node-pools create ${MACH?}-node-pool --cluster nim-demo \\\n   --accelerator type=${GPU_TYPE?},count=${GPU_COUNT?},gpu-driver-version=latest \\\n  --machine-type ${MACH?} \\\n  --ephemeral-storage-local-ssd=count=${GPU_COUNT?} \\\n  --enable-autoscaling --enable-image-streaming \\\n  --num-nodes=1 --min-nodes=1 --max-nodes=3 \\\n  --node-locations ${ZONE?} \\\n  --region ${REGION?} \\\n  --spot\n</code></pre>"},{"location":"tutorials-and-examples/nvidia-nim/#set-up-access-to-nvidia-nims-and-prepare-environment","title":"Set Up Access to NVIDIA NIMs and prepare environment","text":"<ol> <li>Get your NGC_API_KEY from NGC</li> </ol> <pre><code>export NGC_CLI_API_KEY=\"&lt;YOUR_API_KEY&gt;\"\n</code></pre> <p>[!NOTE] If you have not set up NGC, see NGC Setup to get your access key and begin using NGC.</p> <ol> <li>As a part of the NGC setup, set your configs</li> </ol> <pre><code>ngc config set\n</code></pre> <ol> <li>Ensure you have access to the repository by listing the models</li> </ol> <pre><code>ngc registry model list\n</code></pre> <ol> <li>Create a Kuberntes namespace</li> </ol> <pre><code>kubectl create namespace nim\n</code></pre>"},{"location":"tutorials-and-examples/nvidia-nim/#deploy-a-pvc-to-persist-the-model","title":"Deploy a PVC to persist the model","text":"<ol> <li>Create a PVC to persist the model weights - recommended for deployments with more than one (1) replica.  Save the following yaml as <code>pvc.yaml</code>.</li> </ol> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: model-store-pvc\n  namespace: nim\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 30Gi\n  storageClassName: standard-rwx\n</code></pre> <ol> <li>Apply PVC</li> </ol> <pre><code>kubectl apply -f pvc.yaml\n</code></pre> <p>[!NOTE] This PVC will dynamically provision a PV with the necessary storage to persist model weights across replicas of your pods.</p>"},{"location":"tutorials-and-examples/nvidia-nim/#deploy-the-nim-with-the-generated-engine-using-a-helm-chart","title":"Deploy the NIM with the generated engine using a Helm chart","text":"<ol> <li>Clone the nim-deploy repository</li> </ol> <pre><code>git clone https://github.com/NVIDIA/nim-deploy.git\ncd nim-deploy/helm\n</code></pre> <ol> <li>Deploy chart with minimal configurations</li> </ol> <pre><code>helm --namespace nim install demo-nim nim-llm/ --set model.ngcAPIKey=$NGC_CLI_API_KEY --set persistence.enabled=true --set persistence.existingClaim=model-store-pvc\n</code></pre>"},{"location":"tutorials-and-examples/nvidia-nim/#test-the-nim","title":"Test the NIM","text":"<ol> <li>Expose the service</li> </ol> <pre><code>kubectl port-forward --namespace nim services/demo-nim-nim-llm 8000\n</code></pre> <ol> <li>Send a test prompt - A100</li> </ol> <pre><code>curl -X 'POST' \\\n  'http://localhost:8000/v1/chat/completions' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"messages\": [\n    {\n      \"content\": \"You are a polite and respectful poet.\",\n      \"role\": \"system\"\n    },\n    {\n      \"content\": \"Write a limerick about the wonders of GPUs and Kubernetes?\",\n      \"role\": \"user\"\n    }\n  ],\n  \"model\": \"meta/llama3-8b-instruct\",\n  \"max_tokens\": 256,\n  \"top_p\": 1,\n  \"n\": 1,\n  \"stream\": false,\n  \"frequency_penalty\": 0.0\n}' | jq '.choices[0].message.content' -\n</code></pre> <ol> <li>Browse the API by navigating to http://localhost:8000/docs</li> </ol>"},{"location":"tutorials-and-examples/skypilot/","title":"GKE cross region capacity chasing with SkyPilot","text":"<p>Due to the limited availability of accelerator resources, customers face significant challenges in securing sufficient capacity to run their AI/ML workloads. They often require:</p> <ul> <li>Preferences for VM families and accelerators, with the ability to automatically fail over to alternative configurations if their preferred resources are unavailable.</li> <li>Automatic capacity acquisition across regions to address scenarios where a specific region lacks sufficient resources.</li> </ul> <p>In this tutorial, we will demonstrate how to leverage the open-source software SkyPilot to help GKE customers efficiently obtain accelerators across regions, ensuring workload continuity and optimized resource utilization.</p> <p>SkyPilot is a framework for running AI and batch workloads on any infra, offering unified execution, high cost savings, and high GPU availability. By combining SkyPilot with GKE's solutions (such as Kueue + Dynamic Workload Scheduler, Custom compute class, GCS FUSE), users can effectively address capacity challenges while optimizing costs.</p>"},{"location":"tutorials-and-examples/skypilot/#the-overview","title":"The overview.","text":"<p>In this tutorial, our persona is an ML scientist planning to run a batch workload for hyperparameter tuning. This workload involves two experiments, with each experiment requiring 4 GPUs to execute. </p> <p>We have two GKE clusters in different regions: one in us-central1 with 4A100 and another in us-west1 with 4L4.</p> <p>By the end of this tutorial, our goal is to have one experiment running in the us-central cluster and the other in the us-west cluster, demonstrating efficient resource distribution across regions.</p> <p>SkyPilot supports GKE's cluster autoscaling for dynamic resource management. However, to keep this tutorial straightforward, we will demonstrate the use of a static node pool instead.</p>"},{"location":"tutorials-and-examples/skypilot/#before-you-begin","title":"Before you begin","text":"<ol> <li> <p>Ensure you have a gcp project with billing enabled and enabled the GKE API. </p> </li> <li> <p>Ensure you have the following tools installed on your workstation</p> </li> <li>gcloud CLI</li> <li>gcloud kubectl</li> </ol>"},{"location":"tutorials-and-examples/skypilot/#set-up-your-gke-cluster","title":"Set up your GKE Cluster","text":"<p>Create two clusters, you can create  the clusters in parrallel to reduce time. 1. Set the default environment variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get project)\n</code></pre> <ol> <li>Create a GKE cluster in us-central1-c with 4*A100</li> </ol> <pre><code>gcloud container clusters create demo-us-central1 \\\n    --location=us-central1-c \\\n    --project=$PROJECT_ID \n</code></pre> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-a100,count=4 \\\n  --machine-type a2-highgpu-4g \\\n  --region us-central1-c \\\n  --cluster=demo-us-central1 \\\n  --num-nodes=1\n</code></pre> <pre><code>gcloud container clusters get-credentials demo-us-central1 \\\n--region us-central1-c \\\n--project ${PROJECT_ID}\n</code></pre> <ol> <li>Create a GKE cluster in us-west1-c with 4*L4</li> </ol> <pre><code>gcloud container clusters create demo-us-west1 \\\n    --location=us-west1-c \\\n    --project=$PROJECT_ID \n</code></pre> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4,count=4 \\\n  --machine-type g2-standard-48 \\\n  --region us-west1-c \\\n  --cluster=demo-us-west1 \\\n  --num-nodes=1\n</code></pre> <pre><code>gcloud container clusters get-credentials demo-us-west1 \\\n--region us-west1-c \\\n--project ${PROJECT_ID}\n</code></pre>"},{"location":"tutorials-and-examples/skypilot/#install-skypilot","title":"Install SkyPilot","text":"<ol> <li>Create a virtual environment.</li> </ol> <pre><code>cd ~\ngit clone https://github.com/GoogleCloudPlatform/ai-on-gke.git\ncd ai-on-gke/tutorials-and-examples/skypilot\npython3 -m venv ~/ai-on-gke/tutorials-and-examples/skypilot\nsource bin/activate \n</code></pre> <ol> <li>Install SkyPilot</li> </ol> <pre><code>pip install -U \"skypilot[kubernetes,gcp]\"\n</code></pre> <pre><code>sky check\n\nsky show-gpus\n</code></pre> <ol> <li>Find the context names</li> </ol> <pre><code>kubectl config get-contexts\n\n# Find the context name, for example: \ngke_${PROJECT_NAME}_us-central1-c_demo-us-central1\ngke_${PROJECT_NAME}_us-west1-c_demo-us-west1\n</code></pre> <ol> <li>Copy the following yaml to ~/.sky/config.yaml with context name replaced. SkyPilot will evaludate the contexts by the order specified until it finds a cluster that provides enough capacity to deploy the workload.</li> </ol> <pre><code>allowed_clouds:\n  - gcp\n  - kubernetes\nkubernetes:\n  # Use the context's name\n  allowed_contexts:\n    - gke_${PROJECT_NAME}_us-central1-c_demo-us-central1\n    - gke_${PROJECT_NAME}_us-west1-c_demo-us-west1\n  provision_timeout: 30\n</code></pre>"},{"location":"tutorials-and-examples/skypilot/#launch-the-jobs","title":"Launch the jobs","text":"<p>Under <code>~/ai-on-gke/tutorials-and-examples/skypilot</code>, you\u2019ll find a file named <code>train.yaml</code>, which uses SkyPilot's syntax to define a job. The job will ask for 4* A100 first. If no capacity is found, it failovers to L4. </p> <pre><code>resources:\n  cloud: kubernetes\n  # list has orders\n  accelerators: [ A100:4, L4:4 ]\n</code></pre> <p>The <code>launch.py</code> a Python program that initiates a hyperparameter tuning process with two candidates for the learning rate (LR) parameter. In production environments, such experiments are typically tracked using open-source frameworks like MLFlow.</p> <p>Start the trainig:</p> <pre><code>python launch.py\n</code></pre> <p>SkyPilot will first select the demo-us-central1 cluster, which has 4 A100 GPUs available. For the second job, it will launch in the demo-us-west1 cluster using L4 GPUs, as no additional clusters with 4 A100 GPUs were available.</p> <p>You also can check SkyPilot's status using: </p> <pre><code>sky status\n</code></pre> <p>You can SSH into the pod in GKE using the cluster's name. Once inside, you'll find the local source code synced to the pod under <code>~/sky_workdir</code>. This setup makes it convenient for developers to debug and iterate on their AI/ML code efficiently.</p> <pre><code>ssh train-cluster1\n</code></pre>"},{"location":"tutorials-and-examples/skypilot/#clean-up","title":"Clean up","text":"<p>Delete the GKE clusters.</p> <pre><code>gcloud container clusters delete demo-us-central1 \\\n    --location=us-central1-c \\\n    --project=$PROJECT_ID\n</code></pre> <pre><code>gcloud container clusters delete demo-us-west1 \\\n    --location=us-west1-c \\\n    --project=$PROJECT_ID\n</code></pre>"},{"location":"tutorials-and-examples/skypilot/text-classification/","title":"Index","text":""},{"location":"tutorials-and-examples/skypilot/text-classification/#text-classification-examples","title":"Text classification examples","text":""},{"location":"tutorials-and-examples/skypilot/text-classification/#glue-tasks","title":"GLUE tasks","text":"<p>Based on the script <code>run_glue.py</code>.</p> <p>Fine-tuning the library models for sequence classification on the GLUE benchmark: General Language Understanding Evaluation. This script can fine-tune any of the models on the hub and can also be used for a dataset hosted on our hub or your own data in a csv or a JSON file (the script might need some tweaks in that case, refer to the comments inside for help).</p> <p>GLUE is made up of a total of 9 different tasks. Here is how to run the script on one of them:</p> <pre><code>export TASK_NAME=mrpc\n\npython run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>where task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.</p> <p>We get the following results on the dev set of the benchmark with the previous commands (with an exception for MRPC and WNLI which are tiny and where we used 5 epochs instead of 3). Trainings are seeded so you should obtain the same results with PyTorch 1.6.0 (and close results with different versions), training times are given for information (a single Titan RTX was used):</p> Task Metric Result Training time CoLA Matthews corr 56.53 3:17 SST-2 Accuracy 92.32 26:06 MRPC F1/Accuracy 88.85/84.07 2:21 STS-B Pearson/Spearman corr. 88.64/88.48 2:13 QQP Accuracy/F1 90.71/87.49 2:22:26 MNLI Matched acc./Mismatched acc. 83.91/84.10 2:35:23 QNLI Accuracy 90.66 40:57 RTE Accuracy 65.70 57 WNLI Accuracy 56.34 24 <p>Some of these results are significantly different from the ones reported on the test set of GLUE benchmark on the website. For QQP and WNLI, please refer to FAQ #12 on the website.</p> <p>The following example fine-tunes BERT on the <code>imdb</code> dataset hosted on our hub:</p> <pre><code>python run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --dataset_name imdb  \\\n  --do_train \\\n  --do_predict \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/imdb/\n</code></pre> <p>If your model classification head dimensions do not fit the number of labels in the dataset, you can specify <code>--ignore_mismatched_sizes</code> to adapt it.</p>"},{"location":"tutorials-and-examples/skypilot/text-classification/#text-classification","title":"Text classification","text":"<p>As an alternative, we can use the script <code>run_classification.py</code> to fine-tune models on a single/multi-label classification task. </p> <p>The following example fine-tunes BERT on the <code>en</code> subset of  <code>amazon_reviews_multi</code> dataset. We can specify the metric, the label column and aso choose which text columns to use jointly for classification. </p> <pre><code>dataset=\"amazon_reviews_multi\"\nsubset=\"en\"\npython run_classification.py \\\n    --model_name_or_path  google-bert/bert-base-uncased \\\n    --dataset_name ${dataset} \\\n    --dataset_config_name ${subset} \\\n    --shuffle_train_dataset \\\n    --metric_name accuracy \\\n    --text_column_name \"review_title,review_body,product_category\" \\\n    --text_column_delimiter \"\\n\" \\\n    --label_column_name stars \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 512 \\\n    --per_device_train_batch_size 32 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 1 \\\n    --output_dir /tmp/${dataset}_${subset}/\n</code></pre> <p>Training for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+body+category.</p> <p>The following is a multi-label classification example. It fine-tunes BERT on the <code>reuters21578</code> dataset hosted on our hub:</p> <pre><code>dataset=\"reuters21578\"\nsubset=\"ModApte\"\npython run_classification.py \\\n    --model_name_or_path google-bert/bert-base-uncased \\\n    --dataset_name ${dataset} \\\n    --dataset_config_name ${subset} \\\n    --shuffle_train_dataset \\\n    --remove_splits \"unused\" \\\n    --metric_name f1 \\\n    --text_column_name text \\\n    --label_column_name topics \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 512 \\\n    --per_device_train_batch_size 32 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 15 \\\n    --output_dir /tmp/${dataset}_${subset}/ \n</code></pre> <p>It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that you have to explicitly remove the \"unused\" split from the dataset, since it is not used for classification.</p>"},{"location":"tutorials-and-examples/skypilot/text-classification/#mixed-precision-training","title":"Mixed precision training","text":"<p>If you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision training with PyTorch 1.6.0 or latest, or by installing the Apex library for previous versions. Just add the flag <code>--fp16</code> to your command launching one of the scripts mentioned above!</p> <p>Using mixed precision training usually results in 2x-speedup for training with the same final results:</p> Task Metric Result Training time Result (FP16) Training time (FP16) CoLA Matthews corr 56.53 3:17 56.78 1:41 SST-2 Accuracy 92.32 26:06 91.74 13:11 MRPC F1/Accuracy 88.85/84.07 2:21 88.12/83.58 1:10 STS-B Pearson/Spearman corr. 88.64/88.48 2:13 88.71/88.55 1:08 QQP Accuracy/F1 90.71/87.49 2:22:26 90.67/87.43 1:11:54 MNLI Matched acc./Mismatched acc. 83.91/84.10 2:35:23 84.04/84.06 1:17:06 QNLI Accuracy 90.66 40:57 90.96 20:16 RTE Accuracy 65.70 57 65.34 29 WNLI Accuracy 56.34 24 56.34 12"},{"location":"tutorials-and-examples/skypilot/text-classification/#pytorch-version-no-trainer","title":"PyTorch version, no Trainer","text":"<p>Based on the script <code>run_glue_no_trainer.py</code>.</p> <p>Like <code>run_glue.py</code>, this script allows you to fine-tune any of the models on the hub on a text classification task, either a GLUE task or your own data in a csv or a JSON file. The main difference is that this script exposes the bare training loop, to allow you to quickly experiment and add any customization you would like.</p> <p>It offers less options than the script with <code>Trainer</code> (for instance you can easily change the options for the optimizer or the dataloaders directly in the script) but still run in a distributed setup, on TPU and supports mixed precision by the mean of the \ud83e\udd17 <code>Accelerate</code> library. You can use the script normally after installing it:</p> <pre><code>pip install git+https://github.com/huggingface/accelerate\n</code></pre> <p>then</p> <pre><code>export TASK_NAME=mrpc\n\npython run_glue_no_trainer.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --max_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>You can then use your usual launchers to run in it in a distributed environment, but the easiest way is to run</p> <pre><code>accelerate config\n</code></pre> <p>and reply to the questions asked. Then</p> <pre><code>accelerate test\n</code></pre> <p>that will check everything is ready for training. Finally, you can launch training with</p> <pre><code>export TASK_NAME=mrpc\n\naccelerate launch run_glue_no_trainer.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --max_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/\n</code></pre> <p>This command is the same and will work for:</p> <ul> <li>a CPU-only setup</li> <li>a setup with one GPU</li> <li>a distributed training with several GPUs (single or multi node)</li> <li>a training on TPUs</li> </ul> <p>Note that this library is in alpha release so your feedback is more than welcome if you encounter any problem using it.</p>"},{"location":"tutorials-and-examples/skypilot/text-classification/#xnli","title":"XNLI","text":"<p>Based on the script <code>run_xnli.py</code>.</p> <p>XNLI is a crowd-sourced dataset based on MultiNLI. It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-resource language such as English and low-resource languages such as Swahili).</p>"},{"location":"tutorials-and-examples/skypilot/text-classification/#fine-tuning-on-xnli","title":"Fine-tuning on XNLI","text":"<p>This example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins on a single tesla V100 16GB.</p> <pre><code>python run_xnli.py \\\n  --model_name_or_path google-bert/bert-base-multilingual-cased \\\n  --language de \\\n  --train_language en \\\n  --do_train \\\n  --do_eval \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 5e-5 \\\n  --num_train_epochs 2.0 \\\n  --max_seq_length 128 \\\n  --output_dir /tmp/debug_xnli/ \\\n  --save_steps -1\n</code></pre> <p>Training with the previously defined hyper-parameters yields the following results on the test set:</p> <pre><code>acc = 0.7093812375249501\n</code></pre>"},{"location":"tutorials-and-examples/storage/hyperdisk-ml/","title":"Index","text":""},{"location":"tutorials-and-examples/storage/hyperdisk-ml/#populate-a-hyperdisk-ml-disk-from-google-cloud-storage","title":"Populate a Hyperdisk ML Disk from Google Cloud Storage","text":""},{"location":"tutorials-and-examples/storage/hyperdisk-ml/#this-guide-uses-the-google-cloud-api-to-create-a-hyperdisk-ml-disk-from-data-in-cloud-storage-and-then-use-it-in-a-gke-cluster-refer-to-this-documentation-for-instructions-all-in-the-gke-api","title":"This guide uses the Google Cloud API to create a Hyperdisk ML disk from data in Cloud Storage and then use it in a GKE cluster. Refer to this documentation for instructions all in the GKE API","text":"<ol> <li>Create a new GCE instance that you will use to hydrate the new Hyperdisk ML disk with data. Note a c3-standard-44 instance is used to provide the max throughput while populating the hyperdisk(Instance to throughput rates).</li> </ol> <pre><code>VM_NAME=hydrator\nMACHINE_TYPE=c3-standard-44\nIMAGE_FAMILY=debian-11\nIMAGE_PROJECT=debian-cloud\nZONE=us-central1-a\nSNAP_SHOT_NAME=hdmlsnapshot\nPROJECT_ID=myproject\nDISK_NAME=model1\n\ngcloud compute instances create $VM_NAME \\\n    --image-family=$IMAGE_FAMILY \\\n    --image-project=$IMAGE_PROJECT \\\n    --zone=$ZONE \\\n    --machine-type=$MACHINE_TYPE\n\ngcloud compute ssh $VM_NAME\n\n</code></pre> <p>Update and authenticate the instance</p> <pre><code>sudo apt-get update\nsudo apt-get install google-cloud-cli\ngcloud init\ngcloud auth login\n\n</code></pre> <ol> <li>Create and attach the disk to the new GCE VM.</li> </ol> <pre><code>SIZE=140\nTHROUGHPUT=2400\n\ngcloud compute disks create $DISK_NAME --type=hyperdisk-ml \\\n--size=$SIZE --provisioned-throughput=$THROUGHPUT  \\\n--zone $ZONE\n\ngcloud compute instances attach-disk $VM_NAME --disk=$DISK_NAME --zone=$ZONE \n</code></pre> <ol> <li>Log into the hydrator VM, format the volume, initiate transfer, and dismount the volume.</li> </ol> <pre><code>gcloud compute ssh $VM_NAME\n</code></pre> <p>Identify the device name (eg: /dev/nvme0n2) by looking at the output of lsblk. This should correspond to the disk that was attached in the previous step. </p> <pre><code>lsblk\n</code></pre> <p>Save device name given by lsblk(example nvme0n2)</p> <pre><code>DEVICE=/dev/nvme0n2\n</code></pre> <pre><code>GCS_DIR=gs://vertex-model-garden-public-us-central1/llama2/llama2-70b-hf \nsudo /sbin/mkfs -t ext4 -E lazy_itable_init=0,lazy_journal_init=0,discard $DEVICE\n\nsudo mount $DEVICE /mnt\ngcloud storage cp -r $GCS_DIR /mnt\nsudo umount /mnt\n</code></pre> <ol> <li>Detach disk from the hydrator and switch to READ_ONLY_MANY access mode.</li> </ol> <pre><code>gcloud compute instances detach-disk $VM_NAME --disk=$DISK_NAME --zone=$ZONE\ngcloud compute disks update $DISK_NAME --access-mode=READ_ONLY_MANY  --zone=$ZONE\n</code></pre> <ol> <li>Create a snapshot from the disk to use as a template.</li> </ol> <pre><code>gcloud compute snapshots create $SNAP_SHOT_NAME \\\n    --source-disk-zone=$ZONE \\\n    --source-disk=$DISK_NAME \\\n    --project=$PROJECT_ID\n</code></pre> <ol> <li>You now have a hyperdisk ML snapshot populated with your data from Google Cloud Storage. You can delete the hydrator GCE instance and the original disk.</li> </ol> <pre><code>gcloud compute instances delete $VM_NAME --zone=$ZONE\ngcloud compute disks delete $DISK_NAME --project $PROJECT_ID --zone $ZONE\n</code></pre> <ol> <li>In your GKE cluster create your Hypedisk ML multi zone and Hyperdisk ML storage classes. Hyperdisk ML disks are zonal and the Hyperdisk-ml-multi-zone storage class automatically provisions disks in zones where the pods using them are.  Replace the zones in this class with the zones you want to allow the Hyperdisk ML snapshot to create disks in. </li> </ol> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: hyperdisk-ml-multi-zone\nparameters:\n  type: hyperdisk-ml\n  provisioned-throughput-on-create: \"2400Mi\"\n  enable-multi-zone-provisioning: \"true\"\nprovisioner: pd.csi.storage.gke.io\nallowVolumeExpansion: false\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowedTopologies:\n- matchLabelExpressions:\n  - key: topology.gke.io/zone\n    values:\n    - us-central1-a\n    - us-central1-c\n--- \napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n    name: hyperdisk-ml\nparameters:\n    type: hyperdisk-ml\nprovisioner: pd.csi.storage.gke.io\nallowVolumeExpansion: false\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <ol> <li>Create a volumeSnapshotClass and VolumeSnapshotContent config to use your snapshot. Replace the VolumeSnapshotContent.spec.source.snapshotHandle with the path to your snapshot. </li> </ol> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: my-snapshotclass\ndriver: pd.csi.storage.gke.io\ndeletionPolicy: Delete\n---\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: restored-snapshot\nspec:\n  volumeSnapshotClassName: my-snapshotclass\n  source:\n    volumeSnapshotContentName: restored-snapshot-content\n---\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: restored-snapshot-content\nspec:\n  deletionPolicy: Retain\n  driver: pd.csi.storage.gke.io\n  source:\n    snapshotHandle: projects/[project_ID]/global/snapshots/[snapshotname]\n  volumeSnapshotRef:\n    kind: VolumeSnapshot\n    name: restored-snapshot\n    namespace: default\n\n</code></pre> <ol> <li>Reference your snapshot in the persistent volume claim. Be sure to adjust the spec.dataSource.name and spec.resources.requests.storage to your snapshot name and size.</li> </ol> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: hdml-consumer-pvc\nspec:\n  dataSource:\n    name: restored-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n  - ReadOnlyMany\n  storageClassName: hyperdisk-ml-multi-zone\n  resources:\n    requests:\n      storage: 140Gi\n</code></pre> <ol> <li>Add a reference to this PVC in your deployment spec.template.spec.volume.persistentVolumeClaim.claimName parameter. </li> </ol> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: busybox\n  labels:\n    app: busybox\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: busybox\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - image: busybox:latest\n        name: busybox\n        command:\n          - \"sleep\"\n          - \"infinity\"\n        volumeMounts:\n        - name: busybox-persistent-storage\n          mountPath: /var/www/html\n      volumes:\n      - name: busybox-persistent-storage\n        persistentVolumeClaim:\n          claimName: hdml-consumer-pvc\n</code></pre>"},{"location":"tutorials-and-examples/tpu-examples/single-host-inference/jax/stable-diffusion/","title":"Index","text":""},{"location":"tutorials-and-examples/tpu-examples/single-host-inference/jax/stable-diffusion/#serve-online-inference-a-model-using-a-single-tpu-and-gke","title":"Serve (online inference) a model using a single TPU and GKE","text":"<p>To better understand how TPUs work on GKE, please read the doc TPUs in GKE introduction.</p> <p>This directory contains files for JAX Model inference and serving. You can find step-by-step instructions in the quickstart guide.</p>"},{"location":"tutorials-and-examples/tpu-examples/training/mnist-single-tpu/","title":"Train a model with TPUs on GKE Standard mode","text":"<p>Please follow the Quick Start at https://cloud.google.com/kubernetes-engine/docs/quickstarts/train-model-tpus-standard</p>"},{"location":"tutorials-and-examples/vector-databases/readme/","title":"Vector Database Repo","text":""},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/","title":"From RAG to autonomous apps with Weaviate and Gemini on Google Kubernetes Engine","text":""},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#video-of-this-tutorial","title":"Video of this tutorial","text":"<p>This demo application creates a product catalog that is stored in a Weaviate vector database and vectorized for semantic search. It is also integrated with the Gemini Pro Vision API to automatically create product descriptions. The App is built with the Next.js framework. </p> <p>There is also a notebook, notebook.ipynb, asset that allows you to work directly with the Weaviate database and Gemini API to create advanced product descriptions based on user personas as well as demonstrate the concept of \"Generative Feedback Loops\" as done in the original session. </p> <p>Our application architecture consists of: - Cloud Build to build our NEXTJS website into a container image and deploy it to Cloud Run - Cloud Run and host our containerized website - Google Cloud Storage to store and host our product images - Google Gemini/Vertex AI to generate our embeddings and product descriptions - Weaviate Vector Database for product and embedding storage and search - GKE to run our database</p> <p>Product Catalog</p> <p></p> <p>Product Description Generation and Semantic Search</p> <p>You can create/edit a product and use Gemini to create the product description based on the image and title</p> <p></p> <p>System Architecture</p> <p></p>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#to-deploy-this-demo","title":"To Deploy this demo...","text":"<p>clone this git repo</p> <pre><code>git clone https://github.com/bkauf/next-store.git\ncd next-store\n</code></pre>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#setup-the-weaviate-vector-database","title":"Setup the Weaviate Vector Database","text":""},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#get-your-gemini-api-key","title":"Get your GEMINI API key","text":"<ul> <li>Go to https://developers.generativeai.google/ to create a GEMINI API key. This is necessary to be able to run the demo.</li> <li>Set this API key as an environment variable</li> </ul> <pre><code>export GEMINI_API_KEY=&lt;your Gemini API key&gt;\n</code></pre>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#enable-the-necessary-google-cloud-apis","title":"Enable the necessary Google Cloud APIs","text":"<p>Set your project id</p> <pre><code>export PROJECT_ID=&lt;your project id&gt;\ngcloud config set core/project $PROJECT_ID\n</code></pre> <p>Enable the necessary Google cloud APIs for your project. These APIs are necessary for installing the Weaviate database and to run the Demo app.</p> <pre><code>\ngcloud services enable \\\ncloudbuild.googleapis.com \\\nstorage.googleapis.com \\\nserviceusage.googleapis.com \\\ncloudresourcemanager.googleapis.com \\\ncompute.googleapis.com \\\niam.googleapis.com \\\ncontainer.googleapis.com \\\nartifactregistry.googleapis.com \\\nclouddeploy.googleapis.com \\\nrun.googleapis.com\n</code></pre>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#deploy-the-gke-cluster","title":"Deploy the GKE Cluster","text":"<ol> <li> <p>Set environment varia1bles used in later steps.</p> <p><code>sh export CLUSTER_NAME=demo-cluster # A name for your cluster, you can change this if you want. export LOCATION=us-central1 # Google cloud region to host your infrastucture, you can change this if you want.</code></p> </li> <li> <p>We will use the default VPC to deploy the GKE cluster into. In some environments, the autocreation of the default VPC is disabled by the administrator and default network will not be present. If your project doesn't have it, simply create a new VPC network named default and add the add the firewall rules mentioned here to the VPC. You can check if the default network exists by running this command.</p> <p><code>sh gcloud compute networks list</code></p> </li> <li> <p>Deploy a small regional GKE Cluster. This step can take several minutes to finish.</p> <p>```sh  gcloud container clusters create $CLUSTER_NAME \\ --project=$PROJECT_ID \\ --region=$LOCATION \\ --enable-ip-alias \\ --num-nodes=1 \\ --scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/cloud-platform</p> <p>```</p> </li> </ol>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#install-weaviate","title":"Install Weaviate","text":"<ol> <li> <p>Connect to the cluster     <code>sh     gcloud container clusters get-credentials $CLUSTER_NAME --region $LOCATION --project $PROJECT_ID</code></p> </li> <li> <p>We are going to use the regional persistant disk storage class for weaviate, so we'll install that storage class in the cluster.</p> <p><code>sh kubectl apply -f weaviate/storage-class.yaml</code></p> </li> <li> <p>Let's create a secret API KEY for weaviate so we don't allow unauthenticated access.</p> <p><code>sh export WEAVIATE_API_KEY=\"next-demo349834\" # you can choose another random string as the key.</code> 1. Store the key as a kubernetes secret.</p> <p>```sh kubectl create namespace weaviate</p> <p>kubectl create secret generic user-keys \\ --from-literal=AUTHENTICATION_APIKEY_ALLOWED_KEYS=$WEAVIATE_API_KEY \\ -n weaviate ``` 1. Install Weaviate using a helm chart.</p> <p>```sh helm repo add weaviate https://weaviate.github.io/weaviate-helm</p> <p>helm upgrade --install weaviate weaviate/weaviate \\ -f weaviate/demo-values.yaml \\ --set modules.text2vec-palm.apiKey=$GEMINI_API_KEY \\ --namespace weaviate ```</p> </li> </ol>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#get-weaviate-server-ips","title":"Get Weaviate Server IPs","text":"<ol> <li> <p>Get the HTTP IP that the web server will use</p> <p>```sh export WEAVIATE_SERVER=\"\" while [[ -z \"$WEAVIATE_SERVER\" ]]; do WEAVIATE_SERVER=$(kubectl get service weaviate -n weaviate -o jsonpath='{.status.loadBalancer.ingress[0].ip}') sleep 2  done echo \"External HTTP IP obtained: $WEAVIATE_SERVER\"</p> <p>``` 1. Get the IP of the GRPC service that the database creation script will use</p> <p><code>sh export WEAVIATE_SERVER_GRPC=\"\" while [[ -z \"$WEAVIATE_SERVER_GRPC\" ]]; do WEAVIATE_SERVER_GRPC=$(kubectl get service weaviate-grpc -n weaviate -o jsonpath='{.status.loadBalancer.ingress[0].ip}') sleep 2  done echo \"External GRPC IP obtained: $WEAVIATE_SERVER_GRPC\"</code></p> </li> </ol>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#configure-the-database","title":"Configure the Database","text":"<ol> <li> <p>Install the Weaviate client</p> <p><code>sh pip3 install -U weaviate-client  # For beta versions: `pip install --pre -U \"weaviate-client==4.*\"`</code></p> </li> <li> <p>Create the database schema and load the sample files     by running the following script from the root directory of the repo where the first_99_objects.json file is located.</p> <p><code>sh python3 createdb.py</code> You should get an output similar to the below if the database was created successfully: <code>Items added to the database: 99</code></p> </li> </ol>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#setup-the-demo-application","title":"Setup the Demo application","text":"<p>The following steps will walk through adding the nessessary variables to the demo application, creating a container for it, then running it on Google Cloud Run.</p> <ol> <li> <p>Create your storage bucket and allow public access to it.</p> <p>```sh export GCS_BUCKET=\"$PROJECT_ID-next-demo\"</p> <p>gcloud storage buckets create gs://$GCS_BUCKET --location=$LOCATION \\ --no-public-access-prevention ```</p> <p>Allow public access to the bucket</p> <p>```sh</p> <p>gcloud storage buckets add-iam-policy-binding gs://$GCS_BUCKET --member=allUsers --role=roles/storage.objectViewer ```</p> </li> <li> <p>Create a .env file for the demo application.</p> <p><code>sh cd demo-website/ touch .env</code></p> <p>Create a .env file in the demo-website directory for the NextJS build process and replace the variables below with your own. If you would like to run this demo app locally with npm run dev you will also need a service account that has GCS object admin permissions. See optional variable below. If you would like to run this on Cloud Run you do not need a local service account.</p> <p>.env file  ```sh GEMINI_API_KEY=\"The GEMINI api key you retrieved earlier\" GCS_BUCKET=\"The name of the bucket you created earlier\" WEAVIATE_SERVER=\"from weaviate install steps\" WEAVIATE_API_KEY=\"next-demo349834\" </p> </li> <li> <p>Create a artifact registry repo for your container.</p> <p>```sh export REPO_NAME=\"next-demo\"</p> <p>gcloud artifacts repositories create $REPO_NAME --repository-format=docker \\     --location=$LOCATION --description=\"Docker repository\" \\     --project=$PROJECT_ID ```</p> </li> <li> <p>Create a container image to store in the image repo.</p> <p><code>sh gcloud builds submit --tag $LOCATION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/next-demo:1.0</code></p> </li> <li> <p>Create a service account for Cloud Run to use to connect to GCS for image uploads.</p> <p>```sh export SERVICE_ACCOUNT_NAME=\"next-demo\"</p> <p>gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \\ --display-name=\"Next Demo\"</p> <p>gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\"serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/storage.objectAdmin\" ```</p> </li> <li> <p>Deploy the container image to Cloud Run.</p> <p>The following commands set your envorinemnt varaibles for Cloud Run and also the service account that allows uploads to your public Google Cloud Storage bucket.</p> <p>```sh export CLOUD_RUN_NAME=\"next-store\"</p> <p>gcloud run deploy $CLOUD_RUN_NAME \\     --image $LOCATION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/next-demo:1.0 \\     --port=3000 \\     --allow-unauthenticated \\     --region $LOCATION \\ --set-env-vars=GEMINI_API_KEY=$GEMINI_API_KEY, \\ --set-env-vars=GCS_BUCKET=$GCS_BUCKET, \\ --set-env-vars=WEAVIATE_SERVER=http://$WEAVIATE_SERVER, \\ --set-env-vars=WEAVIATE_API_KEY=$WEAVIATE_API_KEY \\ --service-account=$SERVICE_ACCOUNT_NAME ```</p> <p>Navigate to the demo application via the service URL returned. You can use the data below to create a new item and search for it:</p> <ul> <li>Title: Project Sushi Tshirt</li> <li>Category: tees Tshirts</li> <li>Image: https://shop.googlemerchandisestore.com/store/20190522377/assets/items/images/GGCPGXXX1338.jpg</li> </ul> </li> </ol>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#if-you-plan-to-run-this-locally-use-a-sevice-account-file-with-gcs-object-admin-permissions","title":"If you plan to run this locally use a sevice account file with GCS object admin permissions","text":""},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/#google_application_credentialssajson","title":"GOOGLE_APPLICATION_CREDENTIALS=\"sa.json\"","text":"<p>```</p>"},{"location":"tutorials-and-examples/vector-databases/NEXT-2024-Weaviate-Demo/demo-website/","title":"Demo website for Generative Feedback Loops","text":"<p>Build the IAM account for cloud run</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"SERVICE_ACCOUNT_EMAIL\" \\\n    --member \"PRINCIPAL\" \\\n    --role \"roles/iam.serviceAccountUser\"\n</code></pre> <p>Build  the container using cloud build</p> <pre><code>gcloud builds submit --tag us-central1-docker.pkg.dev/[Project ID]/Repo]/[Name]\n</code></pre> <p>Submit the container to cloud run</p> <p>Sample product to upload</p> <pre><code>    {\n        \"id\": \"id_22\",\n        \"product_id\": \"GGCPGAAJ133812\",\n        \"title\": \"Project Sushi Tshirt\",\n        \"category\": \"Clothing  accessories Tops  tees Tshirts\",\n        \"link\": \"https://shop.googlemerchandisestore.com/store/20190522377/assets/items/images/GGCPGXXX1338.jpg\",\n        \"description\": \"This is a set of two tshirts The first shirt is a heather gray shirt with a black N3Q logo on the chest The second shirt is a heather gray shirt with a black and red sushi graphic on the chest Both shirts are made of 100 cotton and are machine washable\",\n        \"color\": \"['gray', 'black', 'red']\",\n        \"gender\": \"unisex\",\n        \"brand\": \"Google\"\n    },\n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/dws-examples/","title":"Dynamic Workload Scheduler examples","text":"<p>The repository contains examples on how to use DWS in GKE. More information about DWS is available here.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/dws-examples/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials-and-examples/workflow-orchestration/dws-examples/#kueue","title":"Kueue","text":"<p>To install a released version of Kueue in your cluster, run the following command:</p> <pre><code>VERSION=v0.7.0\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n</code></pre> <p>For more configuration options visit Kueue's installation guide.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/dws-examples/#files-included","title":"Files included","text":"<ul> <li><code>dws-queue.yaml</code> - Kueue's Cluster and Local queues with ProvisioningRequest and DWS support enabled.</li> <li><code>job.yaml</code> - Sample job that requires GPU and uses DWS-enabled queue. Contains optional annotation <code>provreq.kueue.x-k8s.io/maxRunDurationSeconds</code> which sets <code>maxRunDurationSeconds</code> for the created ProvisioningRequest</li> </ul>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/","title":"Running distributed ML training workloads on GKE using Indexed Jobs","text":"<p>In this guide you will run a distributed ML training workload on GKE using an Indexed Job.</p> <p>Specifically, you will train a handwritten digit image classifier on the classic MNIST dataset using PyTorch. The training computation will be distributed across 4 GPU nodes in a GKE cluster.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud account set up.</li> <li>gcloud command line tool installed and configured to use your GCP project.</li> <li>kubectl command line utility is installed.</li> <li>docker is installed.</li> </ul>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#1-create-a-standard-gke-cluster","title":"1. Create a standard GKE cluster","text":"<p>Run the command: </p> <pre><code>gcloud container clusters create demo --zone us-central1-c\n</code></pre> <p>You should see output indicating the cluster is being created (this can take ~10 minutes or so).</p>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#2-create-a-gpu-node-pool","title":"2. Create a GPU node pool.","text":"<p>You can choose any supported GPU type you wish, using a supported machine type. See the docs for more details. In this example, we are using NVIDIA Tesla T4s with the N1 machine family.</p> <pre><code>gcloud container node-pools create gpu-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1,gpu-driver-version=LATEST \\\n  --machine-type n1-standard-4 \\\n  --zone us-central1-c --cluster demo \\\n  --node-locations us-central1-c \\\n  --num-nodes 4\n</code></pre> <p>Creating this GPU node pool will take a few minutes.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#3-build-and-push-the-docker-image-to-gcr","title":"3. Build and push the Docker image to GCR","text":"<p>Make a local copy of the mnist.py file which defines a traditional convolutional neural network, as the training logic which trains the model on the classic MNIST dataset.</p> <p>Next, make a local copy of the Dockerfile and run the following commands to build the container image and push it to your GCR repository:</p> <pre><code>export PROJECT_ID=&lt;your GCP project ID&gt;\ndocker build -t pytorch-mnist-gpu -f Dockerfile .\ndocker tag pytorch-mnist-gpu gcr.io/$PROJECT_ID/pytorch-mnist-gpu:latest\ndocker push gcr.io/$PROJECT_ID/pytorch-mnist-gpu:latest\n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#4-define-an-indexed-job-and-headless-service","title":"4. Define an Indexed Job and Headless Service","text":"<p>In the yaml below, we configure an Indexed Job to run 4 pods, 1 for each GPU node, and use torchrun to kick off a distributed training job for the CNN model on the MNIST dataset. This training job will utilize 1 T4 GPU chip on each node in the node pool.</p> <p>We also define a headless service which selects the pods owned by this Indexed Job. This will trigger the creation of the DNS records needed for the pods to communicate with eachother over the network via hostnames.</p> <p>Copy the yaml below into a local file <code>mnist.yaml</code> and be sure to replace <code>&lt;PROJECT_ID&gt;</code> with your GCP project ID in the container image.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: headless-svc\nspec:\n  clusterIP: None \n  selector:\n    job-name: pytorchworker\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pytorchworker\nspec:\n  backoffLimit: 0\n  completions: 4\n  parallelism: 4\n  completionMode: Indexed\n  template:\n    spec:\n      subdomain: headless-svc\n      restartPolicy: Never\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-tesla-t4\n      tolerations:\n      - operator: \"Exists\"\n        key: nvidia.com/gpu\n      containers:\n      - name: pytorch\n        image: gcr.io/&lt;PROJECT_ID&gt;/pytorch-mnist-gpu:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3389\n        env:\n        - name: MASTER_ADDR\n          value: pytorchworker-0.headless-svc\n        - name: MASTER_PORT\n          value: \"3389\"\n        - name: PYTHONBUFFERED\n          value: \"0\"\n        - name: LOGLEVEL\n          value: \"INFO\"\n        - name: RANK\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n        command:\n        - bash\n        - -xc\n        - |\n          printenv\n          torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT --node_rank=$RANK mnist.py --epochs=1 --log-interval=1 \n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#5-run-the-training-job","title":"5. Run the training job","text":"<p>Run the following command to create the Kubernetes resources we defined above and run the training job:</p> <pre><code>kubectl apply -f mnist.yaml\n</code></pre> <p>You should see 4 pods created (note the container image is large and may take a few minutes to pull before the container starts running):</p> <pre><code>$ kubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\npytorchworker-0-bbsmk   0/1     ContainerCreating   0          15s\npytorchworker-1-92tbl   0/1     ContainerCreating   0          15s\npytorchworker-2-nbrgf   0/1     ContainerCreating   0          15s\npytorchworker-3-rsrdf   0/1     ContainerCreating   0          15s\n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/indexed-job/#4-observe-training-logs","title":"4. Observe training logs","text":"<p>Once the pods transition from the <code>ContainerCreating</code> status to the <code>Running</code> status, you can observe the training logs by examining the pod logs.</p> <pre><code>$ kubectl logs -f pytorchworker-1\n\n+ torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=pytorchworker-0.headless-svc --master_port=3389 --node_rank=1 mnist.py --epochs=1 --log-interval=1\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 90162259.46it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 33279036.76it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 23474415.33it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 19165521.90it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.297087\nTrain Epoch: 1 [64/60000 (0%)]  Loss: 2.550339\nTrain Epoch: 1 [128/60000 (1%)] Loss: 2.361300\n...\n\nTrain Epoch: 1 [14912/60000 (99%)]      Loss: 0.051500\nTrain Epoch: 1 [5616/60000 (100%)]      Loss: 0.209231\n235it [00:36,  6.51it/s]\n\nTest set: Average loss: 0.0825, Accuracy: 9720/10000 (97%)\n\nINFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\nINFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish\nINFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0015289783477783203 seconds\n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/","title":"Running distributed ML training workloads on GKE using the JobSet API","text":"<p>In this guide you will run a distributed ML training workload on GKE using the JobSet API. Specifically, you will train a handwritten digit image classifier on the classic MNIST dataset using PyTorch. The training computation will be distributed across 4 nodes in a GKE cluster.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud account set up.</li> <li>gcloud command line tool installed and configured to use your GCP project.</li> <li>kubectl command line utility is installed.</li> </ul>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/#1-create-a-gke-cluster-with-4-nodes","title":"1. Create a GKE cluster with 4 nodes","text":"<p>Run the command: </p> <p><code>gcloud container clusters create jobset-cluster --zone us-central1-c --num_nodes=4</code></p> <p>You should see output indicating the cluster is being created (this can take ~10 minutes or so).</p>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/#2-install-the-jobset-crd-on-your-cluster","title":"2. Install the JobSet CRD on your cluster","text":"<p>Follow the JobSet installation guide.</p>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/#3-apply-the-pytorch-mnist-example-jobset","title":"3. Apply the PyTorch MNIST example JobSet","text":"<p>Run the command: </p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/jobset/main/examples/pytorch/cnn-mnist/mnist.yaml\n\njobset.jobset.x-k8s.io/pytorch created\n</code></pre> <p>You should see 4 pods created (note the container image is large and may take a few minutes to pull before the container starts running):</p> <pre><code>$ kubectl get pods\n\nNAME                        READY   STATUS              RESTARTS   AGE\npytorch-workers-0-0-ph645   0/1     ContainerCreating   0          6s\npytorch-workers-0-1-mddhj   0/1     ContainerCreating   0          6s\npytorch-workers-0-2-z9ffc   0/1     ContainerCreating   0          6s\npytorch-workers-0-3-f9ps4   0/1     ContainerCreating   0          6s\n</code></pre>"},{"location":"tutorials-and-examples/workflow-orchestration/jobset/pytorch/#4-observe-training-logs","title":"4. Observe training logs","text":"<p>You can observe the training logs by examining the pod logs.</p> <pre><code>$ kubectl logs -f pytorch-workers-0-1-drvk6 \n\n+ torchrun --rdzv_id=123 --nnodes=4 --nproc_per_node=1 --master_addr=pytorch-workers-0-0.pytorch --master_port=3389 --node_rank=1 mnist.py --epochs=1 --log-interval=1\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 90162259.46it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 33279036.76it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 23474415.33it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 19165521.90it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.297087\nTrain Epoch: 1 [64/60000 (0%)]  Loss: 2.550339\nTrain Epoch: 1 [128/60000 (1%)] Loss: 2.361300\n...\n</code></pre>"}]}