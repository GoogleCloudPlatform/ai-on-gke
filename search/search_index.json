{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI on GKE Assets","text":"<p>This repository contains assets related to AI/ML workloads on Google Kubernetes Engine (GKE).</p>"},{"location":"#overview","title":"Overview","text":"<p>Run optimized AI/ML workloads with Google Kubernetes Engine (GKE) platform orchestration capabilities. A robust AI/ML platform considers the following layers:</p> <ul> <li>Infrastructure orchestration that support GPUs and TPUs for training and serving workloads at scale</li> <li>Flexible integration with distributed computing and data processing frameworks</li> <li>Support for multiple teams on the same infrastructure to maximize utilization of resources</li> </ul>"},{"location":"#infrastructure","title":"Infrastructure","text":"<p>The AI-on-GKE application modules assumes you already have a functional GKE cluster. If not, follow the instructions under infrastructure/README.md to install a Standard or Autopilot GKE cluster.</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 infrastructure\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 platform.tfvars\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 versions.tf\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u2514\u2500\u2500 tutorial.md\n</code></pre> <p>To deploy new GKE cluster update the <code>platform.tfvars</code> file with the appropriate values and then execute below terraform commands:</p> <pre><code>terraform init\nterraform apply -var-file platform.tfvars\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>The repo structure looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 applications\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u2514\u2500\u2500 ray\n\u251c\u2500\u2500 contributing.md\n\u251c\u2500\u2500 dcgm-on-gke\n\u2502   \u251c\u2500\u2500 grafana\n\u2502   \u2514\u2500\u2500 quickstart\n\u251c\u2500\u2500 gke-a100-jax\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 build_push_container.sh\n\u2502   \u251c\u2500\u2500 kubernetes\n\u2502   \u2514\u2500\u2500 train.py\n\u251c\u2500\u2500 gke-batch-refarch\n\u2502   \u251c\u2500\u2500 01_gke\n\u2502   \u251c\u2500\u2500 02_platform\n\u2502   \u251c\u2500\u2500 03_low_priority\n\u2502   \u251c\u2500\u2500 04_high_priority\n\u2502   \u251c\u2500\u2500 05_compact_placement\n\u2502   \u251c\u2500\u2500 06_jobset\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cloudbuild-create.yaml\n\u2502   \u251c\u2500\u2500 cloudbuild-destroy.yaml\n\u2502   \u251c\u2500\u2500 create-platform.sh\n\u2502   \u251c\u2500\u2500 destroy-platform.sh\n\u2502   \u2514\u2500\u2500 images\n\u251c\u2500\u2500 gke-disk-image-builder\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 cli\n\u2502   \u251c\u2500\u2500 go.mod\n\u2502   \u251c\u2500\u2500 go.sum\n\u2502   \u251c\u2500\u2500 imager.go\n\u2502   \u2514\u2500\u2500 script\n\u251c\u2500\u2500 gke-dws-examples\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 dws-queues.yaml\n\u2502   \u251c\u2500\u2500 job.yaml\n\u2502   \u2514\u2500\u2500 kueue-manifests.yaml\n\u251c\u2500\u2500 gke-online-serving-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 gke-tpu-examples\n\u2502   \u251c\u2500\u2500 single-host-inference\n\u2502   \u2514\u2500\u2500 training\n\u251c\u2500\u2500 indexed-job\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 mnist.py\n\u251c\u2500\u2500 jobset\n\u2502   \u2514\u2500\u2500 pytorch\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 gke-autopilot-private-cluster\n\u2502   \u251c\u2500\u2500 gke-autopilot-public-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-private-cluster\n\u2502   \u251c\u2500\u2500 gke-standard-public-cluster\n\u2502   \u251c\u2500\u2500 jupyter\n\u2502   \u251c\u2500\u2500 jupyter_iap\n\u2502   \u251c\u2500\u2500 jupyter_service_accounts\n\u2502   \u251c\u2500\u2500 kuberay-cluster\n\u2502   \u251c\u2500\u2500 kuberay-logging\n\u2502   \u251c\u2500\u2500 kuberay-monitoring\n\u2502   \u251c\u2500\u2500 kuberay-operator\n\u2502   \u2514\u2500\u2500 kuberay-serviceaccounts\n\u251c\u2500\u2500 saxml-on-gke\n\u2502   \u251c\u2500\u2500 httpserver\n\u2502   \u2514\u2500\u2500 single-host-inference\n\u251c\u2500\u2500 training-single-gpu\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 tutorial.md\n\u2514\u2500\u2500 tutorials\n    \u251c\u2500\u2500 e2e-genai-langchain-app\n    \u251c\u2500\u2500 finetuning-llama-7b-on-l4\n    \u2514\u2500\u2500 serving-llama2-70b-on-l4-gpus\n</code></pre>"},{"location":"#jupyter-hub","title":"Jupyter Hub","text":"<p>This repository contains a Terraform template for running JupyterHub on Google Kubernetes Engine. We've also included some example notebooks ( under <code>applications/ray/example_notebooks</code>), including one that serves a GPT-J-6B model with Ray AIR (see here for the original notebook). To run these, follow the instructions at applications/ray/README.md to install a Ray cluster.</p> <p>This jupyter module deploys the following resources, once per user: - JupyterHub deployment - User namespace - Kubernetes service accounts</p> <p>Learn more about JupyterHub on GKE here</p>"},{"location":"#ray","title":"Ray","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine.</p> <p>This module deploys the following, once per user: - User namespace - Kubernetes service accounts - Kuberay cluster - Prometheus monitoring - Logging container</p> <p>Learn more about Ray on GKE here</p>"},{"location":"#important-considerations","title":"Important Considerations","text":"<ul> <li>Make sure to configure terraform backend to use GCS bucket, in order to persist terraform state across different environments.</li> </ul>"},{"location":"#licensing","title":"Licensing","text":"<ul> <li>The use of the assets contained in this repository is subject to compliance with Google's AI Principles</li> <li>See LICENSE</li> </ul>"},{"location":"applications/jupyter/","title":"Ray on GKE Templates","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine. See the Ray on GKE directory to see additional guides and references.</p>"},{"location":"applications/jupyter/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>GCP Project with following APIs enabled</p> <ul> <li>container.googleapis.com</li> <li>iap.googleapis.com (required when using authentication with Identity Aware Proxy)</li> </ul> </li> <li> <p>A functional GKE cluster.</p> <ul> <li>To create a new standard or autopilot cluster, follow the instructions in <code>infrastructure/README.md</code></li> <li>Alternatively, you can set the <code>create_cluster</code> variable to true in <code>workloads.tfvars</code> to provision a new GKE cluster. This will default to creating a GKE Autopilot cluster; if you want to provision a standard cluster you must also set <code>autopilot_cluster</code> to false.</li> </ul> </li> <li> <p>This module is configured to optionally use Identity Aware Proxy (IAP) to protect access to the Ray dashboard. It expects the brand &amp; the OAuth consent configured in your org. You can check the details here: OAuth consent screen</p> </li> <li> <p>Preinstall the following on your computer:</p> <ul> <li>Terraform</li> <li>Gcloud CLI</li> </ul> </li> </ol>"},{"location":"applications/jupyter/#installation","title":"Installation","text":""},{"location":"applications/jupyter/#configure-inputs","title":"Configure Inputs","text":"<ol> <li>If needed, clone the repo</li> </ol> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/applications/ray\n</code></pre> <ol> <li>Edit <code>workloads.tfvars</code> with your GCP settings.</li> </ol> <p>Important Note: If using this with the Jupyter module (<code>applications/jupyter/</code>), it is recommended to use the same k8s namespace for both i.e. set this to the same namespace as <code>applications/jupyter/workloads.tfvars</code>.</p> Variable Description Required project_id GCP Project Id Yes cluster_name GKE Cluster Name Yes cluster_location GCP Region Yes kubernetes_namespace The namespace that Ray and rest of the other resources will be installed in. Yes gcs_bucket GCS bucket to be used for Ray storage Yes create_service_account Create service accounts used for Workload Identity mapping Yes"},{"location":"applications/jupyter/#install","title":"Install","text":"<p>NOTE: Terraform keeps state metadata in a local file called <code>terraform.tfstate</code>. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using <code>terraform destory</code> before reapplying/reinstalling.</p> <ol> <li>Ensure your gcloud application default credentials are in place. </li> </ol> <pre><code>gcloud auth application-default login\n</code></pre> <ol> <li> <p>Run <code>terraform init</code></p> </li> <li> <p>Run <code>terraform apply --var-file=./workloads.tfvars</code>. </p> </li> </ol>"},{"location":"applications/rag/","title":"RAG on GKE","text":"<p>This is a sample to deploy a Retrieval Augmented Generation (RAG) application on GKE. </p> <p>The latest recommended release is branch release-1.1.</p>"},{"location":"applications/rag/#what-is-rag","title":"What is RAG?","text":"<p>RAG is a popular approach for boosting the accuracy of LLM responses, particularly for domain specific or private data sets.</p> <p>RAG uses a semantically searchable knowledge base (like vector search) to retrieve relevant snippets for a given prompt to provide additional context to the LLM. Augmenting the knowledge base with additional data is typically cheaper than fine tuning and is more scalable when incorporating current events and other rapidly changing data spaces.</p>"},{"location":"applications/rag/#rag-on-gke-architecture","title":"RAG on GKE Architecture","text":"<ol> <li>A GKE service endpoint serving Hugging Face TGI inference using <code>mistral-7b</code>.</li> <li>Cloud SQL <code>pgvector</code> instance with vector embeddings generated from an input dataset.</li> <li>A Ray cluster running on GKE that runs jobs to generate embeddings and populate the vector DB.</li> <li>A Jupyter notebook running on GKE that reads the dataset using GCS fuse driver integrations and runs a Ray job to populate the vector DB.</li> <li>A front end chat interface running on GKE that prompts the inference server with context from the vector DB.</li> </ol> <p>This tutorial walks you through installing the RAG infrastructure in a GCP project, generating vector embeddings for a sample Kaggle Netflix shows dataset and prompting the LLM with context.</p>"},{"location":"applications/rag/#prerequisites","title":"Prerequisites","text":""},{"location":"applications/rag/#install-tooling-required","title":"Install tooling (required)","text":"<p>Install the following on your computer: * Kubectl * Terraform * Helm * Gcloud</p>"},{"location":"applications/rag/#bring-your-own-cluster-optional","title":"Bring your own cluster (optional)","text":"<p>By default, this tutorial creates a cluster on your behalf. We highly recommend following the default settings.</p> <p>If you prefer to manage your own cluster, set <code>create_cluster = false</code> and make sure the <code>network_name</code> is set to your cluster's network in the Installation section. Creating a long-running cluster may be better for development, allowing you to iterate on Terraform components without recreating the cluster every time.</p> <p>Use gcloud to create a GKE Autopilot cluster. Note that RAG requires the latest Autopilot features, available on the latest versions of 1.28 and 1.29.</p> <pre><code>gcloud container clusters create-auto rag-cluster \\\n  --location us-central1 \\\n  --cluster-version 1.28\n</code></pre>"},{"location":"applications/rag/#bring-your-own-vpc-optional","title":"Bring your own VPC (optional)","text":"<p>By default, this tutorial creates a new network on your behalf with Private Service Connect already enabled. We highly recommend following the default settings.</p> <p>If you prefer to use your own VPC, set <code>create_network = false</code> in the in the Installation section. This also requires enabling Private Service Connect for your VPC. Without Private Service Connect, the RAG components cannot connect to the vector DB:</p> <ol> <li> <p>Create an IP allocation</p> </li> <li> <p>Create a private connection.</p> </li> </ol>"},{"location":"applications/rag/#installation","title":"Installation","text":"<p>This section sets up the RAG infrastructure in your GCP project using Terraform.</p> <p>NOTE: Terraform keeps state metadata in a local file called <code>terraform.tfstate</code>. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using <code>terraform destroy</code> before reapplying/reinstalling.</p> <ol> <li> <p><code>cd ai-on-gke/applications/rag</code></p> </li> <li> <p>Edit <code>workloads.tfvars</code> to set your project ID, location, cluster name, and GCS bucket name. Ensure the <code>gcs_bucket</code> name is globally unique (add a random suffix). Optionally, make the following changes:</p> <ul> <li>(Recommended) Enable authenticated access for JupyterHub, frontend chat and Ray dashboard services.</li> <li>(Optional) Set a custom <code>kubernetes_namespace</code> where all k8s resources will be created.</li> <li>(Optional) Set <code>autopilot_cluster = false</code> to deploy using GKE Standard.</li> <li>(Optional) Set <code>create_cluster = false</code> if you are bringing your own cluster. If using a GKE Standard cluster, ensure it has an L4 nodepool with autoscaling and node autoprovisioning enabled. You can simplify setup by following the Terraform instructions in <code>infrastructure/README.md</code>.</li> <li>(Optional) Set <code>create_network = false</code> if you are bringing your own VPC. Ensure your VPC has Private Service Connect enabled as described above.</li> </ul> </li> <li> <p>Run <code>terraform init</code></p> </li> <li> <p>Run <code>terraform apply --var-file workloads.tfvars</code></p> </li> </ol>"},{"location":"applications/rag/#generate-vector-embeddings-for-the-dataset","title":"Generate vector embeddings for the dataset","text":"<p>This section generates the vector embeddings for your input dataset. Currently, the default dataset is Netflix shows. We will use a Jupyter notebook to run a Ray job that generates the embeddings &amp; populates them into the <code>pgvector</code> instance created above.</p> <p>Set your the namespace, cluster name and location from <code>workloads.tfvars</code>):</p> <pre><code>export NAMESPACE=rag\nexport CLUSTER_LOCATION=us-east4\nexport CLUSTER_NAME=rag-cluster\n</code></pre> <p>Connect to the GKE cluster:</p> <pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${CLUSTER_LOCATION}\n</code></pre> <ol> <li>Connect and login to JupyterHub:</li> <li>If IAP is disabled (<code>jupyter_add_auth = false</code>):         - Port forward to the JupyterHub service: <code>kubectl port-forward service/proxy-public -n ${NAMESPACE} 8081:80 &amp;</code>         - Go to <code>localhost:8081</code> in a browser         - Login with these credentials:             * username: admin             * password: use <code>terraform output jupyterhub_password</code> to fetch the password value</li> <li> <p>If IAP is enabled (<code>jupyter_add_auth = true</code>):         - Fetch the domain: <code>terraform output jupyterhub_uri</code>         - If you used a custom domain, ensure you configured your DNS as described above.          - Verify the domain status is <code>Active</code>:             - <code>kubectl get managedcertificates jupyter-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code>             - Note: This can take up to 20 minutes to propagate.         - Once the domain status is Active, go to the domain in a browser and login with your Google credentials.         - To add additional users to your JupyterHub application, go to Google Cloud Platform IAP, select the <code>rag/proxy-public</code> service and add principals with the role <code>IAP-secured Web App User</code>.</p> </li> <li> <p>Load the notebook:</p> <ul> <li>Once logged in to JupyterHub, choose the <code>CPU</code> preset with <code>Default</code> storage. </li> <li>Click [File] -&gt; [Open From URL] and paste: <code>https://raw.githubusercontent.com/GoogleCloudPlatform/ai-on-gke/main/applications/rag/example_notebooks/rag-kaggle-ray-sql-interactive.ipynb</code></li> </ul> </li> <li> <p>Configure Kaggle:</p> <ul> <li>Create a Kaggle account.</li> <li>Generate an API token. See further instructions. This token is used in the notebook to access the Kaggle Netflix shows dataset.</li> <li>Replace the variables in the 1st cell of the notebook with your Kaggle credentials (can be found in the <code>kaggle.json</code> file created while generating the API token):<ul> <li><code>KAGGLE_USERNAME</code></li> <li><code>KAGGLE_KEY</code></li> </ul> </li> </ul> </li> <li> <p>Generate vector embeddings: Run all the cells in the notebook to generate vector embeddings for the Netflix shows dataset (https://www.kaggle.com/datasets/shivamb/netflix-shows) via a Ray job and store them in the <code>pgvector</code> CloudSQL instance.</p> <ul> <li>When the last cell succeeded, the vector embeddings have been generated and we can launch the frontend chat interface. Note that the Ray job can take up to 10 minutes to finish.</li> <li>Ray may take several minutes to create the runtime environment. During this time, the job will appear to be missing (e.g. <code>Status message: PENDING</code>).</li> <li>Connect to the Ray dashboard to check the job status or logs:<ul> <li>If IAP is disabled (<code>ray_dashboard_add_auth = false</code>):<ul> <li><code>kubectl port-forward -n ${NAMESPACE} service/ray-cluster-kuberay-head-svc 8265:8265</code></li> <li>Go to <code>localhost:8265</code> in a browser</li> </ul> </li> <li>If IAP is enabled (<code>ray_dashboard_add_auth = true</code>):<ul> <li>Fetch the domain: <code>terraform output ray-dashboard-managed-cert</code></li> <li>If you used a custom domain, ensure you configured your DNS as described above.</li> <li>Verify the domain status is <code>Active</code>:<ul> <li><code>kubectl get managedcertificates ray-dashboard-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>Note: This can take up to 20 minutes to propagate.</li> </ul> </li> <li>Once the domain status is Active, go to the domain in a browser and login with your Google credentials.</li> <li>To add additional users to your frontend application, go to Google Cloud Platform IAP, select the <code>rag/ray-cluster-kuberay-head-svc</code> service and add principals with the role <code>IAP-secured Web App User</code>.</li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"applications/rag/#launch-the-frontend-chat-interface","title":"Launch the frontend chat interface","text":"<ol> <li>Connect to the frontend:<ul> <li>If IAP is disabled (<code>frontend_add_auth = false</code>):<ul> <li>Port forward to the frontend service: <code>kubectl port-forward service/rag-frontend -n ${NAMESPACE} 8080:8080 &amp;</code></li> <li>Go to <code>localhost:8080</code> in a browser</li> </ul> </li> <li>If IAP is enabled (<code>frontend_add_auth = true</code>):<ul> <li>Fetch the domain: <code>terraform output frontend_uri</code></li> <li>If you used a custom domain, ensure you configured your DNS as described above.</li> <li>Verify the domain status is <code>Active</code>:<ul> <li><code>kubectl get managedcertificates frontend-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>Note: This can take up to 20 minutes to propagate.</li> </ul> </li> <li>Once the domain status is Active, go to the domain in a browser and login with your Google credentials.</li> <li>To add additional users to your frontend application, go to Google Cloud Platform IAP, select the <code>rag/rag-frontend</code> service and add principals with the role <code>IAP-secured Web App User</code>.</li> </ul> </li> </ul> </li> <li>Prompt the LLM<ul> <li>Start chatting! This will fetch context related to your prompt from the vector embeddings in the <code>pgvector</code> CloudSQL instance, augment the original prompt with the context &amp; query the inference model (<code>mistral-7b</code>) with the augmented prompt.</li> </ul> </li> </ol>"},{"location":"applications/rag/#configure-authenticated-access-via-iap-recommended","title":"Configure authenticated access via IAP (recommended)","text":"<p>We recommend you configure authenticated access via IAP for your services.</p> <p>1) Make sure the OAuth Consent Screen is configured for your project. Ensure <code>User type</code> is set to <code>Internal</code>. 2) Make sure Policy for Restrict Load Balancer Creation Based on Load Balancer Types allows EXTERNAL_HTTP_HTTPS. 3) Set the following variables in <code>workloads.tfvars</code>:     * <code>jupyter_add_auth = true</code>     * <code>frontend_add_auth = true</code>     * <code>ray_dashboard_add_auth = true</code> 4) Allowlist principals for your services via <code>jupyter_members_allowlist</code>, <code>frontend_members_allowlist</code> and <code>ray_dashboard_members_allowlist</code>. 5) Configure custom domains names via <code>jupyter_domain</code>, <code>frontend_domain</code> and <code>ray_dashboard_domain</code> for your services.  6) Configure DNS records for your custom domains:     - Register a Domain on Google Cloud Domains or use a domain registrar of your choice.     - Set up your DNS service to point to the public IP         * Run <code>terraform output frontend_ip_address</code> to get the public ip address of frontend, and add an A record in your DNS configuration to point to the public IP address.         * Run <code>terraform output jupyterhub_ip_address</code> to get the public ip address of jupyterhub, and add an A record in your DNS configuration to point to the public IP address.         * Run <code>terraform output ray_dashboard_ip_address</code> to get the public ip address of ray dashboard, and add an A record in your DNS configuration to point to the public IP address.     - Add an A record: If the DNS service of your domain is managed by Google Cloud DNS managed zone, there are two options to add the A record:         1. Go to https://console.cloud.google.com/net-services/dns/zones, select the zone and click ADD STANDARD, fill in your domain name and public IP address.         2. Run <code>gcloud dns record-sets create &lt;domain address&gt;. --zone=&lt;zone name&gt; --type=\"A\" --ttl=&lt;ttl in seconds&gt; --rrdatas=\"&lt;public ip address&gt;\"</code></p>"},{"location":"applications/rag/#cleanup","title":"Cleanup","text":"<ol> <li>Run <code>terraform destroy --var-file=\"workloads.tfvars\"</code><ul> <li>Network deletion issue: <code>terraform destroy</code> fails to delete the network due to a known issue in the GCP provider. For now, the workaround is to manually delete it.</li> </ul> </li> </ol>"},{"location":"applications/rag/#troubleshooting","title":"Troubleshooting","text":"<p>Set your the namespace, cluster name and location from <code>workloads.tfvars</code>:</p> <pre><code>export NAMESPACE=rag\nexport CLUSTER_LOCATION=us-central1\nexport CLUSTER_NAME=rag-cluster\n</code></pre> <p>Connect to the GKE cluster:</p> <pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --location=${CLUSTER_LOCATION}\n</code></pre> <ol> <li> <p>Troubleshoot Ray job failures:</p> <ul> <li>If the Ray actors fail to be scheduled, it could be due to a stockout or quota issue.<ul> <li>Run <code>kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=kuberay</code>. There should be a Ray head and Ray worker pod in <code>Running</code> state. If your ray pods aren't running, it's likely due to quota or stockout issues. Check that your project and selected <code>cluster_location</code> have L4 GPU capacity.</li> </ul> </li> <li>Often, retrying the Ray job submission (the last cell of the notebook) helps.</li> <li>The Ray job may take 15-20 minutes to run the first time due to environment setup.</li> </ul> </li> <li> <p>Troubleshoot IAP login issues:</p> <ul> <li>Verify the cert is Active:<ul> <li>For JupyterHub <code>kubectl get managedcertificates jupyter-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> <li>For the frontend: <code>kubectl get managedcertificates frontend-managed-cert -n ${NAMESPACE} --output jsonpath='{.status.domainStatus[0].status}'</code></li> </ul> </li> <li>Verify users are allowlisted for JupyterHub or frontend services:<ul> <li>JupyterHub: Go to Google Cloud Platform IAP, select the <code>rag/proxy-public</code> service and check if the user has role <code>IAP-secured Web App User</code>.</li> <li>Frontend: Go to Google Cloud Platform IAP, select the <code>rag/rag-frontend</code> service and check if the user has role <code>IAP-secured Web App User</code>.</li> </ul> </li> <li>Org error:<ul> <li>The OAuth Consent Screen has <code>User type</code> set to <code>Internal</code> by default, which means principals external to the org your project is in cannot log in. To add external principals, change <code>User type</code> to <code>External</code>.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot <code>terraform apply</code> failures:</p> <ul> <li>Inference server (<code>mistral</code>) fails to deploy:<ul> <li>This usually indicates a stockout/quota issue. Verify your project and chosen <code>cluster_location</code> have L4 capacity.</li> </ul> </li> <li>GCS bucket already exists:<ul> <li>GCS bucket names have to be globally unique, pick a different name with a random suffix.</li> </ul> </li> <li>Cloud SQL instance already exists:<ul> <li>Ensure the <code>cloudsql_instance</code> name doesn't already exist in your project.</li> </ul> </li> <li>GMP operator webhook connection refused:<ul> <li>This is a rare, transient error. Run <code>terraform apply</code> again to resume deployment.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot <code>terraform destroy</code> failures:</p> <ul> <li>Network deletion issue:<ul> <li><code>terraform destroy</code> fails to delete the network due to a known issue in the GCP provider. For now, the workaround is to manually delete it.</li> </ul> </li> </ul> </li> <li> <p>Troubleshoot error: <code>Repo model mistralai/Mistral-7B-Instruct-v0.1 is gated. You must be authenticated to access it.</code> for the pod of deployment <code>mistral-7b-instruct</code>.</p> </li> </ol> <p>The error is because the RAG deployments uses <code>Mistral-7B-instruct</code> which is now a gated model on Hugging Face. Deployments fail as they require a Hugging Face authentication token, which is not part of the current workflow.    While we are actively working on long-term fix. This is how to workaround the error:     - Use the guide as a reference to create an access token.     - Go to the model card in Hugging Face and click \"Agree and access repository\"     - Create a secret as noted in with the Hugging Face credential called <code>hf-secret</code> in the name space where your <code>mistral-7b-instruct</code> deployment is running.     - Add the following entry to <code>env</code> within the deployment <code>mistral-7b-instruct</code> via <code>kubectl edit</code>.</p> <pre><code>        - name: HUGGING_FACE_HUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: hf-secret\n              key: hf_api_token\n</code></pre>"},{"location":"applications/ray/","title":"Ray on GKE Templates","text":"<p>This repository contains a Terraform template for running Ray on Google Kubernetes Engine. See the Ray on GKE directory to see additional guides and references.</p>"},{"location":"applications/ray/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>GCP Project with following APIs enabled</p> <ul> <li>container.googleapis.com</li> <li>iap.googleapis.com (required when using authentication with Identity Aware Proxy)</li> </ul> </li> <li> <p>A functional GKE cluster.</p> <ul> <li>To create a new standard or autopilot cluster, follow the instructions in <code>infrastructure/README.md</code></li> <li>Alternatively, you can set the <code>create_cluster</code> variable to true in <code>workloads.tfvars</code> to provision a new GKE cluster. This will default to creating a GKE Autopilot cluster; if you want to provision a standard cluster you must also set <code>autopilot_cluster</code> to false.</li> </ul> </li> <li> <p>This module is configured to optionally use Identity Aware Proxy (IAP) to protect access to the Ray dashboard. It expects the brand &amp; the OAuth consent configured in your org. You can check the details here: OAuth consent screen</p> </li> <li> <p>Preinstall the following on your computer:</p> <ul> <li>Terraform</li> <li>Gcloud CLI</li> </ul> </li> </ol>"},{"location":"applications/ray/#installation","title":"Installation","text":""},{"location":"applications/ray/#configure-inputs","title":"Configure Inputs","text":"<ol> <li>If needed, clone the repo</li> </ol> <pre><code>git clone https://github.com/GoogleCloudPlatform/ai-on-gke\ncd ai-on-gke/applications/ray\n</code></pre> <ol> <li>Edit <code>workloads.tfvars</code> with your GCP settings.</li> </ol> <p>Important Note: If using this with the Jupyter module (<code>applications/jupyter/</code>), it is recommended to use the same k8s namespace for both i.e. set this to the same namespace as <code>applications/jupyter/workloads.tfvars</code>.</p> Variable Description Required project_id GCP Project Id Yes cluster_name GKE Cluster Name Yes cluster_location GCP Region Yes kubernetes_namespace The namespace that Ray and rest of the other resources will be installed in. Yes gcs_bucket GCS bucket to be used for Ray storage Yes create_service_account Create service accounts used for Workload Identity mapping Yes"},{"location":"applications/ray/#install","title":"Install","text":"<p>NOTE: Terraform keeps state metadata in a local file called <code>terraform.tfstate</code>. Deleting the file may cause some resources to not be cleaned up correctly even if you delete the cluster. We suggest using <code>terraform destory</code> before reapplying/reinstalling.</p> <ol> <li>Ensure your gcloud application default credentials are in place. </li> </ol> <pre><code>gcloud auth application-default login\n</code></pre> <ol> <li> <p>Run <code>terraform init</code></p> </li> <li> <p>Run <code>terraform apply --var-file=./workloads.tfvars</code>. </p> </li> </ol>"}]}