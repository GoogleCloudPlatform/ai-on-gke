# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mistral-7b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mistral-7b
  template:
    metadata:
      labels:
        app: mistral-7b
    spec:
      containers:
      - name: mistral-7b
        image: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - name: server-port
          containerPort: 8080
        env:
        - name: MODEL_ID
          value: mistralai/Mistral-7B-Instruct-v0.1
        - name: NUM_SHARD
          value: "1"
        - name: PORT 
          value: "8080"
        - name: QUANTIZE
          value: bitsandbytes-nf4
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /data
            name: data
      volumes:
         - name: dshm
           emptyDir:
              medium: Memory
         - name: data
           hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/mistral-data
---
apiVersion: v1
kind: Service
metadata:
  name: mistral-7b-service
  namespace: default
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: mistral-7b
---
# THIS BELOW IS AN STUB-EXAMPLE AND REQUIRES A QUITE A BIT OF CONFIGURATION TO WORK WITH DGCM (Please consider using prometheus)
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: mistral-7b-hpa
#   annotations:
# spec:
#   maxReplicas: 10
#   minReplicas: 2
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: mistral-7b
#   metrics:
#   - type: External
#     external:
#       metric:
#         name: "external.googleapis.com|prometheus|DCGM_FI_DEV_GPU_UTIL"
#         selector:
#           matchLabels:
#             vendor: "nvidia"
#             model: "L4"  # specific GPU model
#       target:
#         type: Value
#         value: 85  # Adjust to target a GPU utilization threshold for scaling
#   behavior:  # Move behavior section to spec
#     scaleDown:
#       stabilizationWindowSeconds: 300
#       selectPolicy: Max
#       policies:
#       - type: Pods
#         value: 1
#         periodSeconds: 60
#       - type: Percent
#         value: 10
#         periodSeconds: 60
#     scaleUp:
#       stabilizationWindowSeconds: 20
#       selectPolicy: Max
#       policies:
#       - type: Pods
#         value: 2
#         periodSeconds: 60
#       - type: Percent
#         value: 20
#         periodSeconds: 60
