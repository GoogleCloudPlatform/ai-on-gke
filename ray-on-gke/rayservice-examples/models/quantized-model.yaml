# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ConfigMap
metadata:
  name: quantized-model
data:
  model_nf4_mg.py: |
    import ray
    import os
    import transformers
    import torch
    from ray import serve
    from fastapi import FastAPI, Body
    from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, AutoConfig
    from pydantic import BaseModel

    ## quantized with bitsandbytes nf4, Multi-gpu single-host

    class Query(BaseModel):
        text: str

    # validator error - pip install pydantic==1.10.9

    app = FastAPI()

    model_id = os.environ["MODEL_ID"]

    @serve.deployment(num_replicas=1, ray_actor_options={"num_cpus": 20, "num_gpus": 2})

    @serve.ingress(app)
    class Chat:
        tokenizer = ""
        pipeline = ""
        
        def __init__(self):
            # Load bnb4
            nf4_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True
            )
              
            # Load model config
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                token=os.environ["HUGGING_FACE_HUB_TOKEN"]
            )

            # load the model with quantization
            model_nf4 = AutoModelForCausalLM.from_pretrained(
                model_id,
                quantization_config=nf4_config,
                device_map="auto",
                token=os.environ["HUGGING_FACE_HUB_TOKEN"]
            )

            # prepare the pipeline for inferencing
            self.pipeline = transformers.pipeline(
                "text-generation",
                model=model_nf4,
                torch_dtype=torch.float16,
                device_map="auto",
                tokenizer=self.tokenizer,
                token=os.environ["HUGGING_FACE_HUB_TOKEN"]
            )

        @app.post("/")
        async def chat(self, payload: dict = Body(...)) -> str:
            # Run inference
            print("query text: " + payload['text'])
            sequences = self.pipeline(
                payload['text'],
                do_sample=True,
                top_k=10,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                max_length=200        
            )

            val = ""
            for seq in sequences:
                val = seq['generated_text']
                print("val: " + val)

            return val 

    chat_app_nf4_mg = Chat.bind()