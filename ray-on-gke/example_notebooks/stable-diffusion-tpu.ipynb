{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a35dc8-3950-464a-9103-b54918075100",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray==2.6\n",
    "!pip install ray[serve]\n",
    "!pip install fastapi==0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb35d8-32f9-41f8-a2d5-d71fd236e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import Response\n",
    "import ray\n",
    "from ray import serve\n",
    "from typing import Any, List, Mapping\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad33e7-ab2b-4d89-956b-a0187db5e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    address=\"ray://example-cluster-kuberay-head-svc:10001\",\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"jax[tpu]==0.4.11\",\n",
    "            \"-f https://storage.googleapis.com/jax-releases/libtpu_releases.html\",\n",
    "            \"diffusers==0.7.2\",\n",
    "            \"transformers==4.24.0\",\n",
    "            \"flax\",\n",
    "            \"tensorboard-plugin-profile\",\n",
    "            \"tensorboard\",\n",
    "            \"ray[serve]\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69eeb9-4257-40ee-a7e7-88da8ee3ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=1, route_prefix=\"/\")\n",
    "@serve.ingress(app)\n",
    "class APIIngress:\n",
    "  \"\"\"`APIIngress`, e.g. the request router.\n",
    "\n",
    "  Arguments:\n",
    "    diffusion_model_handle: The handle that we use to access the Diffusion\n",
    "      model server that actually runs on TPU hardware.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, diffusion_model_handle) -> None:\n",
    "    self.handle = diffusion_model_handle\n",
    "\n",
    "  @serve.batch(batch_wait_timeout_s=10, max_batch_size=8)\n",
    "  async def batched_generate_handler(self, prompts: List[str]):\n",
    "    \"\"\"Sends a batch of prompts to the TPU model server.\n",
    "\n",
    "    This takes advantage of @serve.batch which is Ray Serve's built-in batching\n",
    "    mechanism.\n",
    "\n",
    "    We set `batch_wait_timeout_s`=10 and `max_batch_size`=8 which means that we\n",
    "    wait the minimum of 10s or the time it takes to retrieve 8 requests in a\n",
    "    batch to begin processing.\n",
    "\n",
    "    Args:\n",
    "      prompts: A list of input prompts\n",
    "\n",
    "    Returns:\n",
    "      A list of responses which contents are raw PNG.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Number of input prompts: \", len(prompts))\n",
    "    print(prompts)\n",
    "    assert len(prompts) <= 8, \"We should not have more than 8 prompts.\"\n",
    "\n",
    "    # Pad to 8 for now (unclear if this is necessary)\n",
    "    num_to_pad = 8 - len(prompts)\n",
    "    prompts += [\"\"] * num_to_pad\n",
    "\n",
    "    image_ref = await self.handle.generate.remote(prompts)\n",
    "    images = await image_ref\n",
    "\n",
    "    # Remove the padded responses.\n",
    "    images = images[:8 - num_to_pad]\n",
    "    results = []\n",
    "    for image in images:\n",
    "      file_stream = BytesIO()\n",
    "      image.save(file_stream, \"PNG\")\n",
    "      results.append(\n",
    "          Response(content=file_stream.getvalue(), media_type=\"image/png\"))\n",
    "    return results\n",
    "\n",
    "  @app.get(\n",
    "      \"/imagine\",\n",
    "      responses={200: {\"content\": {\"image/png\": {}}}},\n",
    "      response_class=Response,\n",
    "  )\n",
    "  async def generate(self, prompt: str):\n",
    "    \"\"\"Requests the generation of an individual prompt.\n",
    "\n",
    "    This implementation simply re-routes the requests to the batch handler.\n",
    "    @serve.batch will return to this function an individual response.\n",
    "\n",
    "    Note that we specify the endpoint (e.g. /imagine) through FastAPI.\n",
    "\n",
    "    Args:\n",
    "      prompt: An individual prompt.\n",
    "\n",
    "    Returns:\n",
    "      A Response.\n",
    "\n",
    "    \"\"\"\n",
    "    return await self.batched_generate_handler(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f986f93-84e9-4809-bd91-08dac4856b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\n",
    "        \"resources\": {\"TPU\": 4}\n",
    "    },\n",
    ")\n",
    "class StableDiffusion:\n",
    "  \"\"\"FLAX Stable Diffusion Ray Serve deployment.\n",
    "\n",
    "  This is the actual model server that runs on the TPU host.\n",
    "\n",
    "  Notes:\n",
    "    - We use custom resources to label a TPU host (note the name will change\n",
    "      once Ray Cluster on TPUs are standardized..)\n",
    "    - We can define the number of minimum and maximum replicas to the\n",
    "      autoscaler.\n",
    "    - Autoscaler will not be functional in this version (as we're using\n",
    "      tpu_controller) but should be functional on single TPU hosts using\n",
    "      the Ray Cluster launcher path OR through Kuberay\n",
    "    - Regardless of the route, Autoscaling works based on the load.\n",
    "      Documentation (https://docs.ray.io/en/latest/serve/architecture.html#ray-serve-autoscaling)\n",
    "      specifies that it is based on the ServeHandle queue and in-flight queries\n",
    "      for scaling decisions (e.g. I need to dig deeper to better understand).\n",
    "    - This example \"only\" uses a single model, but we could start composing\n",
    "      multiple handles together if we wanted to ensemble, or direct from\n",
    "      one model server to another.\n",
    "    - I suspect this could work on multi host TPUs, but not with autoscaling.\n",
    "\n",
    "  Attributes:\n",
    "    run_with_profiler: Whether or not to run with the profiler. Note that\n",
    "      this saves the profile to the separate TPU VM.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, run_with_profiler: bool = False):\n",
    "    from diffusers import FlaxStableDiffusionPipeline\n",
    "    from flax.jax_utils import replicate\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax import pmap\n",
    "\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "    \n",
    "    self.pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        revision=\"bf16\",\n",
    "        dtype=jnp.bfloat16)\n",
    "\n",
    "    self.p_params = replicate(params)\n",
    "    self.p_generate = pmap(self.pipeline._generate)\n",
    "    self._run_with_profiler = run_with_profiler\n",
    "    self._profiler_dir = \"/tmp/tensorboard\"\n",
    "\n",
    "  def generate(self, prompts: List[str]):\n",
    "    \"\"\"Generates a batch of images from Diffusion from a list of prompts.\n",
    "\n",
    "    Notes:\n",
    "      - One \"sharp edge\" is that we need to run imports within the function\n",
    "        as this function is what is called on the raylet. Outside imports\n",
    "        cannot be sent over Ray to the raylets.\n",
    "\n",
    "    Args:\n",
    "      prompts: a list of strings. Should be a factor of 4.\n",
    "\n",
    "    Returns:\n",
    "      A list of PIL Images.\n",
    "\n",
    "    \"\"\"\n",
    "    from flax.training.common_utils import shard\n",
    "    import jax\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    print(\"sanity check: \", jax.device_count())\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    rng = jax.random.split(rng, jax.device_count())\n",
    "\n",
    "    assert len(prompts), \"prompt parameter cannot be empty\"\n",
    "\n",
    "    print(\"Prompts: \", prompts)\n",
    "    prompt_ids = self.pipeline.prepare_inputs(prompts)\n",
    "    #print(\"Prompt IDs: \", prompt_ids)\n",
    "    prompt_ids = shard(prompt_ids)\n",
    "    print(\"Sharded prompt ids has shape:\", prompt_ids.shape)\n",
    "    if self._run_with_profiler:\n",
    "      jax.profiler.start_trace(self._profiler_dir)\n",
    "\n",
    "    time_start = time.time()\n",
    "    images = self.p_generate(prompt_ids, self.p_params, rng)\n",
    "    images = images.block_until_ready()\n",
    "    elapsed = time.time() - time_start\n",
    "    if self._run_with_profiler:\n",
    "      jax.profiler.stop_trace()\n",
    "\n",
    "    print(\"Inference time (in seconds): \", elapsed)\n",
    "    print(\"Shape of the predictions: \", images.shape)\n",
    "    images = images.reshape(\n",
    "        (images.shape[0] * images.shape[1],) + images.shape[-3:])\n",
    "    print(\"Shape of images afterwards: \", images.shape)\n",
    "    return self.pipeline.numpy_to_pil(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68062ad6-c2b0-4a26-b580-e5b5c93faa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_bound = StableDiffusion.bind()\n",
    "deployment = APIIngress.bind(diffusion_bound)\n",
    "serve.run(deployment, host=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf28ab-c6c0-4bf1-ae94-11adbc74dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import multiprocessing\n",
    "import random\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc766e-9ff8-40dc-8b90-abe8eb8419a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request_and_receive_image(prompt: str):\n",
    "  \"\"\"Sends a single prompt request and returns the Image.\"\"\"\n",
    "  inputs = \"%20\".join(prompt.split(\" \"))\n",
    "  resp = requests.get(f\"http://example-cluster-kuberay-head-svc:8000/imagine?prompt={inputs}\")\n",
    "  return BytesIO(resp.content)\n",
    "\n",
    "\n",
    "def send_requests():\n",
    "  \"\"\"Sends a list of requests and processes the responses.\"\"\"\n",
    "  prompts = [\n",
    "      \"Labrador in the style of Hokusai\",\n",
    "      \"Painting of a squirrel skating in New York\",\n",
    "      \"HAL-9000 in the style of Van Gogh\",\n",
    "      \"Times Square under water, with fish and a dolphin swimming around\",\n",
    "      \"A rocket race car driving around the milky way\",\n",
    "      \"A house carried by balloons across the rocky mountains\",\n",
    "      \"Armchair in the shape of an avocado\",\n",
    "      \"Clown astronaut in space, with Earth in the background\",\n",
    "  ]\n",
    "  with multiprocessing.Pool(processes=len(prompts)) as p:\n",
    "    raw_images = p.map(send_request_and_receive_image, prompts)\n",
    "\n",
    "  images = [Image.open(raw_image) for raw_image in raw_images]\n",
    "\n",
    "  def image_grid(imgs, rows, cols):\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, img in enumerate(imgs):\n",
    "      grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "  grid = image_grid(images, 2, 4)\n",
    "  grid.save(f\"./diffusion_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc092b83-b995-4c19-8537-73e8e75a0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed450f7-cdc6-4ba8-83bc-2b971baf5323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
