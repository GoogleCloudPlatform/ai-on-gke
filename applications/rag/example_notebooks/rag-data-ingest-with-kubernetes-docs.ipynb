{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-on-GKE Application\n",
        "\n",
        "This is a Python notebook for generating the vector embeddings based on [Kubernetes docs](https://github.com/dohsimpson/kubernetes-doc-pdf/) used by the RAG on GKE application.   \n",
        "For full information, please checkout the GitHub documentation [here](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/applications/rag/README.md).\n",
        "\n",
        "\n",
        "\n",
        "- Clone the kubernetes docs repo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir /data/kubernetes-docs -p\n",
        "!git clone https://github.com/dohsimpson/kubernetes-doc-pdf /data/kubernetes-docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pgvector\n",
        "!pip install langchain langchain-community sentence_transformers pypdf\n",
        "!pip install google cloud-sql-python-connector[pg8000] langchain-google-cloud-sql-pg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Import required functions and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import base libraries\n",
        "import os\n",
        "import uuid\n",
        "import glob\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\n",
        "from google.cloud.sql.connector import IPTypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Database Connection\n",
        "\n",
        "Let's now set up a connection to your CloudSQL database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize parameters\n",
        "INSTANCE_CONNECTION_NAME = os.environ.get(\"CLOUDSQL_INSTANCE_CONNECTION_NAME\", \"\")\n",
        "print(f\"Your instance connection name is: {INSTANCE_CONNECTION_NAME}\")\n",
        "cloud_variables = INSTANCE_CONNECTION_NAME.split(\":\")\n",
        "\n",
        "GCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", cloud_variables[0])\n",
        "GCP_CLOUD_SQL_REGION = os.environ.get(\"CLOUDSQL_INSTANCE_REGION\", cloud_variables[1])\n",
        "GCP_CLOUD_SQL_INSTANCE = os.environ.get(\"CLOUDSQL_INSTANCE\", cloud_variables[2])\n",
        "\n",
        "DB_NAME = os.environ.get(\"INSTANCE_CONNECTION_NAME\", \"pgvector-database\")\n",
        "VECTOR_EMBEDDINGS_TABLE_NAME = os.environ.get(\"EMBEDDINGS_TABLE_NAME\", \"rag_vector_embeddings\")\n",
        "\n",
        "db_username_file = open(\"/etc/secret-volume/username\", \"r\")\n",
        "DB_USER = db_username_file.read()\n",
        "db_username_file.close()\n",
        "\n",
        "db_password_file = open(\"/etc/secret-volume/password\", \"r\")\n",
        "DB_PASS = db_password_file.read()\n",
        "db_password_file.close()\n",
        "\n",
        "# Create Cloud SQL Postgres Engine\n",
        "pg_engine = PostgresEngine.from_instance(\n",
        "    project_id=GCP_PROJECT_ID,\n",
        "    instance=GCP_CLOUD_SQL_INSTANCE,\n",
        "    region=GCP_CLOUD_SQL_REGION,\n",
        "    database=DB_NAME,\n",
        "    user=DB_USER,\n",
        "    password=DB_PASS,\n",
        "    ip_type=IPTypes.PRIVATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we'll setup some parameters for the dataset processing steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SENTENCE_TRANSFORMER_MODEL = \"intfloat/multilingual-e5-small\"  # Transformer to use for converting text chunks to vector embeddings\n",
        "\n",
        "# the dataset has been pre-dowloaded to the GCS bucket as part of the notebook in the cell above. Ray workers will find the dataset readily mounted.\n",
        "SHARED_DATASET_BASE_PATH = \"/data/kubernetes-docs/\"\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "CHUNK_SIZE = 1000  # text chunk sizes which will be converted to vector embeddings\n",
        "CHUNK_OVERLAP = 10\n",
        "VECTOR_DIMENSION = 384  # Embeddings size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Vector Store Table\n",
        "\n",
        "We are ready to begin. Let's first create some code for generating the vector embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg_engine.init_vectorstore_table(\n",
        "    VECTOR_EMBEDDINGS_TABLE_NAME,\n",
        "    vector_size=VECTOR_DIMENSION,\n",
        "    overwrite_existing=True,  # Enabling this will recreate the table if exists.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_service = HuggingFaceEmbeddings(model_name=SENTENCE_TRANSFORMER_MODEL)\n",
        "vector_store = PostgresVectorStore.create_sync(\n",
        "    engine=pg_engine,\n",
        "    embedding_service=embeddings_service,\n",
        "    table_name=VECTOR_EMBEDDINGS_TABLE_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest PDF docs into CloudSQL DB\n",
        "\n",
        "### Load and Split the kubernetes docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_file_path = glob.glob(f\"{SHARED_DATASET_BASE_PATH}/PDFs/*.pdf\")\n",
        "\n",
        "documents = []\n",
        "for file_path in documents_file_path:\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load_and_split()\n",
        "    documents.extend(pages)\n",
        "    print(f\"Processed: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len\n",
        ")\n",
        "\n",
        "splits = splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add the splits on the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [str(uuid.uuid4()) for i in range(len(splits))]\n",
        "vector_store.add_documents(splits, ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trying the Vector Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What's kubernetes?\"\n",
        "query_vector = embeddings_service.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(query_vector, k=4)\n",
        "\n",
        "for i, document in enumerate(docs):\n",
        "  print(f\"Result #{i+1}\")\n",
        "  print(document.page_content)\n",
        "  print(\"-\" * 100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
    },
    "language_info": {
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.10.11"
    }
  },
 "nbformat": 4,
 "nbformat_minor": 5
}
