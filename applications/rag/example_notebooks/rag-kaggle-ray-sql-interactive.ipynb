{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5574f366-58e9-408b-aea4-1bf5b3351e4c",
   "metadata": {},
   "source": [
    "# RAG-on-GKE Application\n",
    "\n",
    "This is a Python notebook for generating the vector embeddings used by the RAG on GKE application. For full information, please checkout the GitHub documentation [here](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/applications/rag/README.md).\n",
    "\n",
    "\n",
    "## Setup Kaggle Credentials\n",
    "\n",
    "First we will setup your Kaggle credentials and use the Kaggle CLI to download the NetFlix shows dataset to the GCS bucket. Replace the following with your own settings from the Kaggle web page. Navigate to https://www.kaggle.com/settings/account and generate an API token to be used to setup the env variable. See https://www.kaggle.com/docs/api#authentication how to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee2bec-804f-4e22-9ba0-8b1db5a5d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"<username>\"\n",
    "os.environ['KAGGLE_KEY'] = \"<token>\"\n",
    "\n",
    "# Download the zip file to local storage and then extract the desired contents directly to the GKE GCS CSI mounted bucket. The bucket is mounted at the \"/persist-data\" path in the jupyter pod.\n",
    "!kaggle datasets download -d shivamb/netflix-shows -p ~/data --force\n",
    "!mkdir /data/netflix-shows -p\n",
    "!unzip -o ~/data/netflix-shows.zip -d /data/netflix-shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-google-cloud-sql-pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff518d-f4d2-481b-b408-2c2507565611",
   "metadata": {},
   "source": [
    "## Creating the Database Connection\n",
    "\n",
    "Let's now set up a connection to your CloudSQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff789e7-a32d-4dd7-afb8-d3a22c8f3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\n",
    "from google.cloud.sql.connector import IPTypes\n",
    "\n",
    "# initialize parameters\n",
    "INSTANCE_CONNECTION_NAME = os.environ.get(\"CLOUDSQL_INSTANCE_CONNECTION_NAME\")\n",
    "print(f\"Your instance connection name is: {INSTANCE_CONNECTION_NAME}\")\n",
    "cloud_variables = INSTANCE_CONNECTION_NAME.split(\":\")\n",
    "\n",
    "GCP_PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", cloud_variables[0])\n",
    "GCP_CLOUD_SQL_REGION = os.environ.get(\"CLOUDSQL_INSTANCE_REGION\", cloud_variables[1])\n",
    "GCP_CLOUD_SQL_INSTANCE = os.environ.get(\"CLOUDSQL_INSTANCE\", cloud_variables[2])\n",
    "\n",
    "DB_NAME = os.environ.get(\"INSTANCE_CONNECTION_NAME\", \"pgvector-database\")\n",
    "VECTOR_EMBEDDINGS_TABLE_NAME = os.environ.get(\"EMBEDDINGS_TABLE_NAME\", \"netflix_reviews_db\")\n",
    "CHAT_HISTORY_TABLE_NAME = os.environ.get(\"CHAT_HISTORY_TABLE_NAME\", \"message_store\")\n",
    "\n",
    "VECTOR_DIMENSION = os.environ.get(\"VECTOR_DIMENSION\", 384)\n",
    "\n",
    "try:\n",
    "    db_username_file = open(\"/etc/secret-volume/username\", \"r\")\n",
    "    DB_USER = db_username_file.read()\n",
    "    db_username_file.close()\n",
    "\n",
    "    db_password_file = open(\"/etc/secret-volume/password\", \"r\")\n",
    "    DB_PASS = db_password_file.read()\n",
    "    db_password_file.close()\n",
    "except:\n",
    "    DB_USER = os.environ.get(\"DB_USERNAME\", \"postgres\")\n",
    "    DB_PASS = os.environ.get(\"DB_PASS\", \"postgres\")\n",
    "\n",
    "engine = PostgresEngine.from_instance(\n",
    "        project_id=GCP_PROJECT_ID,\n",
    "        region=GCP_CLOUD_SQL_REGION,\n",
    "        instance=GCP_CLOUD_SQL_INSTANCE,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        ip_type=IPTypes.PRIVATE,\n",
    ")\n",
    "\n",
    "try:\n",
    "    engine.init_vectorstore_table(\n",
    "        VECTOR_EMBEDDINGS_TABLE_NAME,\n",
    "        vector_size=VECTOR_DIMENSION,\n",
    "        overwrite_existing=True,\n",
    "    )\n",
    "except Exception as err:\n",
    "    print(f\"Error: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156a6bd-1100-46c2-8ad6-22a923b3d6ac",
   "metadata": {},
   "source": [
    "Next we'll setup some parameters for the dataset processing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24267d7-3fad-47f1-8aa7-2fbe21fe8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TRANSFORMER_MODEL = 'intfloat/multilingual-e5-small' # Transformer to use for converting text chunks to vector embeddings\n",
    "\n",
    "# the dataset has been pre-dowloaded to the GCS bucket as part of the notebook in the cell above. Ray workers will find the dataset readily mounted.\n",
    "SHARED_DATASET_BASE_PATH=\"/data/netflix-shows/\"\n",
    "REVIEWS_FILE_NAME=\"netflix_titles.csv\"\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "CHUNK_SIZE = 1000 # text chunk sizes which will be converted to vector embeddings\n",
    "CHUNK_OVERLAP = 10\n",
    "TABLE_NAME = 'netflix_reviews_db'  # CloudSQL table name\n",
    "DIMENSION = 384  # Embeddings size\n",
    "ACTOR_POOL_SIZE = 1 # number of actors for the distributed map_batches function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7304035-21a4-4017-bce9-aba7e9f81c90",
   "metadata": {},
   "source": [
    "## Generating Documents splits\n",
    "\n",
    "We are ready to begin. Let's first create some code for generating the dataset splits:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb3750-7cd5-439f-a767-617cd5948e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "  def __init__(self):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len)\n",
    "\n",
    "  def __call__(self, text_batch):\n",
    "      text = text_batch[\"item\"]\n",
    "      chunks = []\n",
    "      for data in text:\n",
    "        splits = self.splitter.split_text(data)\n",
    "        chunks.extend(splits)\n",
    "\n",
    "      return {'results':chunks}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263b9db-9504-4177-acd6-5e1aba2ee332",
   "metadata": {},
   "source": [
    "Next we will initialize a Ray cluster to execute the remote task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9551ec-883e-4bde-9f12-f663aedc12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    address=\"ray://ray-cluster-kuberay-head-svc:10001\",\n",
    "    runtime_env={\n",
    "        \"pip\": [               \n",
    "            \"langchain==0.1.10\",\n",
    "            \"transformers==4.38.1\",\n",
    "            \"sentence-transformers==2.5.1\",\n",
    "            \"pyarrow\",\n",
    "            \"datasets==2.18.0\",\n",
    "            \"torch==2.0.1\",\n",
    "            \"huggingface_hub==0.21.3\",\n",
    "            \"langchain-google-cloud-sql-pg\"\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589048c-a0aa-4740-acde-5289cd4076f7",
   "metadata": {},
   "source": [
    "Generate vector embeddings using our Embed class above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392975f-3743-4b2c-8673-087b5633637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset first, wrap the csv file contents into a Ray dataset\n",
    "ray_ds = ray.data.read_csv(SHARED_DATASET_BASE_PATH + REVIEWS_FILE_NAME)\n",
    "print(ray_ds.schema)\n",
    "\n",
    "# Distributed flat map to extract the raw text fields.\n",
    "ds_batch = ray_ds.flat_map(lambda row: [{\n",
    "    'item': \"This is a \" + str(row[\"type\"]) + \" in \" + str(row[\"country\"]) + \" called \" + str(row[\"title\"]) + \n",
    "    \" added at \" + str(row[\"date_added\"]) + \" whose director is \" + str(row[\"director\"]) + \n",
    "    \" and with cast: \" + str(row[\"cast\"]) + \" released at \" + str(row[\"release_year\"]) + \n",
    "    \". Its rating is: \" + str(row['rating']) + \". Its duration is \" + str(row[\"duration\"]) + \n",
    "    \". Its description is \" + str(row['description']) + \".\"\n",
    "}])\n",
    "print(ds_batch.schema)\n",
    "\n",
    "# Distributed map batches to create chunks out of each row.\n",
    "ds_splitted = ds_batch.map_batches(\n",
    "    Splitter,\n",
    "    compute=ray.data.ActorPoolStrategy(size=ACTOR_POOL_SIZE),\n",
    "    batch_size=BATCH_SIZE,  # Large batch size to maximize GPU utilization.\n",
    "    num_gpus=1,  # 1 GPU for each actor.\n",
    "    # num_cpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697ac28-9815-409c-95ec-6ecdb862bb74",
   "metadata": {},
   "source": [
    "Retrieve the result data from Ray remote workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edbba2-8977-4afd-aaa2-2e6e3a298169",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def ray_data_task(ds_splitted):\n",
    "    results = []\n",
    "    for row in ds_splitted.iter_rows():\n",
    "        data_text = row[\"results\"]\n",
    "        data_id = str(uuid.uuid4()) \n",
    "\n",
    "        results.append((data_id, data_text))\n",
    "        \n",
    "    return results\n",
    "    \n",
    "results = ray.get(ray_data_task.remote(ds_splitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652832e-025d-4745-9fef-96615eea07e4",
   "metadata": {},
   "source": [
    "## Writing Results Back to MySQL\n",
    "\n",
    "Now that we have our vector embeddings, we can write our results back to the MySQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb5ec7-c4f7-4312-b0ce-ea07160bef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch cuda version\", torch.version.cuda)\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device cuda found\")\n",
    "    device=\"cuda\"\n",
    "    \n",
    "embeddings_service = HuggingFaceEmbeddings(model_name=SENTENCE_TRANSFORMER_MODEL, model_kwargs=dict(device=device))\n",
    "vector_store = PostgresVectorStore.create_sync(\n",
    "    engine=engine,\n",
    "    embedding_service=embeddings_service,\n",
    "    table_name=VECTOR_EMBEDDINGS_TABLE_NAME,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    id = result[0]\n",
    "    splits = result[1]\n",
    "    vector_store.add_texts(splits, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19b1c-a83b-4a83-94a9-5edf5ae7016a",
   "metadata": {},
   "source": [
    "Finally let's verify that our embeddings got stored in the database correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff4bbc-574d-4cc2-8c87-d0ff6d351626",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"List the cast of squid game\"\n",
    "query_vector = embeddings_service.embed_query(query)\n",
    "docs = vector_store.similarity_search_by_vector(query_vector, k=4)\n",
    "\n",
    "for i, document in enumerate(docs):\n",
    "  print(f\"Result #{i+1}\")\n",
    "  print(document.page_content)\n",
    "  print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
