apiVersion: blueprints.cloud.google.com/v1alpha1
kind: BlueprintMetadata
metadata:
  name: ai-on-gke-display
  annotations:
    config.kubernetes.io/local-config: 'true'
spec:
  info:
    title: HyperCompute Cluster on GKE
    source:
      repo: https://github.com/GoogleCloudPlatform/ai-on-gke
      sourceType: git
      dir: /applications/hcc
  ui:
    input:
      variables:
        goog_cm_deployment_name:
          name: goog_cm_deployment_name
          title: Goog Cm Deployment Name
        project_id:
          name: project_id
          title: Project Id
          invisible: true
        deployment_name:
          name: deployment_name
          title: Deployment Name
          section: required_config
        gpu_type:
          name: gpu_type
          title: Accelerator machine type
          section: required_config
          subtext: Select the accelerator machine type.
          enumValueLabels:
          - label: "A3 Mega, NVIDIA H100 80GB MEGA: a3-megagpu-8g"
            value: A3 Mega
          - label: "A3 Ultra, NVIDIA H200 141GB: a3-ultragpu-8g"
            value: A3 Ultra
        a3_mega_zone:
          name: a3_mega_zone
          title: Cluster zone
          section: required_config
          subtext: Select from locations with available accelerators. If you have a reservation, select the zone where your reservation is located.
          xGoogleProperty:
            type: ET_GCE_ZONE
            gce_zone:
              allowlisted_zones:
              - asia-northeast1-b
              - asia-southeast1-b
              - asia-southeast1-c
              - australia-southeast1-c
              - europe-west1-b
              - europe-west1-c
              - europe-west3-c
              - europe-west4-b
              - europe-west4-c
              - us-central1-c
              - us-central1-a
              - us-central1-b
              - us-east4-c
              - us-east4-b
              - us-east4-a
              - us-east5-a
              - us-east7-b
              - us-west1-b
              - us-west1-a
              - us-west4-a
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Mega
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        a3_ultra_zone:
          name: a3_ultra_zone
          title: Cluster zone
          section: required_config
          subtext: Select from locations with available accelerators. If you have a reservation, select the zone where your reservation is located.
          xGoogleProperty:
            type: ET_GCE_ZONE
            gce_zone:
              allowlisted_zones:
              - europe-west1-b
              - us-east5-a
              - us-east7-c
              - us-west1-c
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Ultra
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        a3_mega_consumption_model:
          name: a3_mega_consumption_model
          title: Consumption options 
          section: required_config
          subtext: For optimal performance with distributed AI workloads, reserve densely allocated accelerator capacity. See <a href="https://cloud.google.com/ai-hypercomputer/docs/consumption-models"><i>Consumption options</i></a> for more details.</br>
          enumValueLabels:
          - label: Reservation
            value: Reservation
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Mega
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        a3_ultra_consumption_model:
          name: a3_ultra_consumption_model
          title: Consumption options
          section: required_config
          subtext: For optimal performance with distributed AI workloads, reserve densely allocated accelerator capacity. See <a href="https://cloud.google.com/ai-hypercomputer/docs/consumption-models"><i>Consumption options</i></a> for more details.</br>
          enumValueLabels:
          - label: Reservation
            value: Reservation
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Ultra
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        reservation:
          name: reservation
          title: Reservation name
          section: required_config
          toggleUsingVariables:
          - variableName: a3_mega_consumption_model
            variableValues:
            - Reservation
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
          - variableName: a3_ultra_consumption_model
            variableValues:
            - Reservation
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        reservation_block:
          name: reservation_block
          title: Reservation block
          section: required_config
          toggleUsingVariables:
          - variableName: a3_ultra_consumption_model
            variableValues:
            - Reservation
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        placement_policy_name:
          name: placement_policy_name
          title: Placement policy
          section: required_config
          toggleUsingVariables:
          - variableName: a3_mega_consumption_model
            variableValues:
            - Reservation
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        a3mega_recipe:
          name: recipe
          title: Solution deployment option
          section: required_config
          subtext: Select your deployment option. 
          enumValueLabels:
          - label: GKE Cluster Only 
            value: gke
          - label: GKE Cluster with NCCL Test
            value: gke-nccl
          - label: GKE Cluster with Llama-3.1-7B pretraining benchmark using NeMo
            value: llama3.1_7b_nemo_pretraining
          - label: GKE Cluster with Llama-3.1-70B pretraining benchmark using NeMo
            value: llama3.1_70b_nemo_pretraining
          - label: GKE Cluster with Mixtral-8-7B pretraining benchmark using NeMo
            value: mixtral8_7b_nemo_pretraining
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Mega
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        a3ultra_recipe:
          name: recipe
          title: Solution deployment option
          section: required_config
          subtext: Select your deployment option.
          enumValueLabels:
          - label: GKE Cluster Only
            value: gke
          - label: GKE Cluster with NCCL Test
            value: gke-nccl
          - label: GKE Cluster with Llama-3.1-7B pretraining benchmark using NeMo
            value: llama3.1_7b_nemo_pretraining
          - label: GKE Cluster with Llama-3.1-70B pretraining benchmark using NeMo
            value: llama3.1_70b_nemo_pretraining
          - label: GKE Cluster with Llama-3.1-70B pretraining benchmark using MaxText
            value: llama3.1_70b_maxtext_pretraining
          - label: GKE Cluster with Mixtral-8-7B pretraining benchmark using NeMo
            value: mixtral8_7b_nemo_pretraining
          - label: GKE Cluster with Mixtral-8-7B pretraining benchmark using MaxText
            value: mixtral8_7b_maxtext_pretraining
          toggleUsingVariables:
          - variableName: gpu_type
            variableValues:
            - A3 Ultra 
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        node_count_gke:
          name: node_count_gke
          title: Node count
          section: required_config
          subtext: Please enter a value >= 0. If using a reservation, ensure that your reservation has the required capacity.
          toggleUsingVariables:
          - variableName: a3mega_recipe
            variableValues:
            - gke
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
          - variableName: a3ultra_recipe
            variableValues:
            - gke
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        node_count_gke_nccl:
          name: node_count_gke_nccl
          title: Node count
          section: required_config
          subtext: Please enter a value >= 2. If using a reservation, ensure that your reservation has the required capacity.
          toggleUsingVariables:
          - variableName: a3mega_recipe
            variableValues:
            - gke-nccl
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
          - variableName: a3ultra_recipe
            variableValues:
            - gke-nccl
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        node_count_llama_3_7b:
          name: node_count_llama_3_7b
          title: Node count
          section: required_config
          subtext: Some benchmarks require a minimum number of nodes. If using a reservation, ensure that your reservation has the required capacity
          enumValueLabels:
            - label: 2
              value: 2
            - label: 4
              value: 4
          toggleUsingVariables:
          - variableName: a3mega_recipe
            variableValues:
            - llama3.1_7b_nemo_pretraining
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
          - variableName: a3ultra_recipe
            variableValues:
            - llama3.1_7b_nemo_pretraining
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        node_count_nemo:
          name: node_count_nemo
          title: Node count
          section: required_config
          subtext: Some benchmarks require a minimum number of nodes. If using a reservation, ensure that your reservation has the required capacity
          enumValueLabels:
            - label: 8
              value: 8
            - label: 16
              value: 16
            - label: 24
              value: 24
            - label: 32
              value: 32
          toggleUsingVariables:
          - variableName: a3mega_recipe
            variableValues:
            - llama3.1_70b_nemo_pretraining
            - mixtral8_7b_nemo_pretraining
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
          - variableName: a3ultra_recipe
            variableValues:
            - llama3.1_70b_nemo_pretraining
            - mixtral8_7b_nemo_pretraining
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        node_count_maxtext:
          name: node_count_maxtext
          title: Node count
          section: required_config
          subtext: Some benchmarks require a minimum number of nodes. If using a reservation, ensure that your reservation has the required capacity
          enumValueLabels:
            - label: 16
              value: 16
          toggleUsingVariables:
          - variableName: a3ultra_recipe
            variableValues:
            - llama3.1_70b_maxtext_pretraining
            - mixtral8_7b_maxtext_pretraining
            type: DISPLAY_VARIABLE_TOGGLE_TYPE_UNSPECIFIED
        acknowledge:
          name: acknowledge
          title: Check to confirm you enabled Google APIs for your project with this
            command.
          section: acknowledge
          subtext: "<pre>\n  <code style=\"background: #f4f4f4;border: 1px solid #ddd;\
            \ border-left: 3px solid #3367d6; color: #6d6868; font-size: 12px; max-width:\
            \ 100%; padding: 0.5em 0.5em; display: inline; line-height: 45px;\">gcloud\
            \ services enable serviceusage.googleapis.com cloudresourcemanager.googleapis.com</code>\n\
            </pre>\n"
          enumValueLabels:
          - label: Confirm that all prerequisites have been met.
            value: 'true'
      sections:
      - name: acknowledge
        title: Before you begin
        subtext: This solution deploys a sample <a href="https://cloud.google.com/ai-hypercomputer/docs/create/gke-ai-hypercompute"><i>HyperCompute
          Cluster</i></a> with GKE in your project to run AI/ML and HPC workloads.</br>
      - name: required_config
        title: Required configuration
    runtime:
      outputMessage: Deployment can take several minutes to complete.
      suggestedActions:
      - heading: View and understand benchmark results
        description: If you chose a benchmark as your deployment option, view the progress of your benchmark.
          Once your benchmark workload completes, view and download the results from the results storage bucket.
          <a href="https://github.com/AI-Hypercomputer/gpu-recipes">Learn about the benchmark and results</a>.
      - heading: Schedule workloads
        description: Use Topology Aware Scheduling with Kueue, a Kubernetes system that manages quotas and how jobs should consume them. <a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/schedule-gke-workloads-tas">Learn how to schedule workloads</a>
      - heading: Manage your cluster
        description: GKE provides tools to perform maintenance on nodes, upgrade your cluster to the latest software versions, and report faulty nodes to Google Cloud Support. <a href="https://cloud.google.com/ai-hypercomputer/docs/manage/manage-gke-clusters">Learn how to manage your cluster</a>
      - heading: Monitor your VMs and clusters
        description: Use GPU metrics to monitor the performance of your VMs, GPUs, and network, and to ensure the health and stability of your clusters. Prebuilt dashboards and visualizations make it easy to identify and troubleshoot bottlenecks. <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics">Learn how to collect and view metrics</a>

