{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a3351d-24f9-45b3-b467-ae1ee0cf1448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 29 12:02:13 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   38C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4                       Off | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   39C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4                       Off | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   38C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6706c533-ac53-47dc-92ff-7a1ccde88d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b7ceb19-221f-4522-95cf-fdfcf473c4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fef15-a8dc-4d7c-9140-d2e1f4a8e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache torch==2.1.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83db85-72d2-4d06-8c28-28474902179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae1d314-97b4-4193-9e31-5485dc0a782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio -q\n",
    "!pip install --upgrade --force-reinstall  typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56bb6be-1a31-4be6-994c-13f72fc832ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8ff341-9bf8-49df-9978-54010498fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jovyan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a98ba2f-4227-445c-b62a-53038e7d055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girishspatil\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72ef7f9-9b38-464e-9d64-4fe0ce8995cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = 'HUGGINGFACE_TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b169b849-ce07-465c-b958-903c5c85e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76787967-268d-4602-8853-b558af023b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 10:14:11.373548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 10:14:11.557559: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 10:14:12.250631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-28 10:14:12.250798: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-28 10:14:12.250811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e62d2ca7314971aed4cc267422827e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    task='text-generation', # we pass model parameters here too\n",
    "    model=model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e4cac7c-3557-43ee-a3f1-4849dae7e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    get response from llama2 model pipeline\n",
    "\n",
    "    Parameters:\n",
    "        promt: The user's input/question to the model\n",
    "\n",
    "    Returns:\n",
    "        None: No return output, but prints the model response as part of the function\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    response = generate_text(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    print(\"response :\",response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8981b947-80a7-4f5a-93b1-39232dba2c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : Explain me the difference between Data Lakehouse and Data Warehouse.\n",
      "\n",
      "A data lakehouse is a centralized repository that stores raw, unprocessed data in its native format, while a data warehouse is a structured repository that stores data in a schema-defined format.\n",
      "\n",
      "Here are some key differences between the two:\n",
      "\n",
      "1. Data Structure: A data lakehouse stores data in its raw, unprocessed form, while a data warehouse stores data in a structured format, with a predefined schema.\n",
      "2. Data Processing: A data lakehouse does not perform any data processing, while a data warehouse performs various data processing tasks, such as data cleansing, transformation, and aggregation.\n",
      "3. Data Access: A data lakehouse provides direct access to raw data, while a data warehouse provides structured data through a query interface.\n",
      "4. Scalability: A data lakehouse is designed to handle large volumes of raw data and can scale horizontally by adding more nodes, while a data warehouse can become bottlenecked as the volume of data grows.\n",
      "5. Cost: A data lakehouse is generally less expensive than a data warehouse, as it does not require the same level of data processing or storage infrastructure.\n",
      "6. Use Cases: A data lakehouse is well-suited for use cases that involve raw, unprocessed data, such as data science, machine learning, and real-time analytics. A data warehouse is better suited for use cases that involve structured data, such as reporting and business intelligence.\n",
      "7. Data Security: A data lakehouse provides more flexible data security options, as data can be encrypted at the node level, while a data warehouse provides more rigid security controls, as data is stored in a structured format.\n",
      "8. Data Governance: A data lakehouse provides more flexibility in terms of data governance, as data can be stored in a decentralized manner, while a data warehouse provides more structured data governance, as data is stored in a centralized repository.\n",
      "\n",
      "In summary, a data lakehouse is a centralized repository that stores raw, unprocessed data in its native format, while a data warehouse is a structured repository that stores data in a schema-defined format. The choice between the two depends on the specific use case and\n"
     ]
    }
   ],
   "source": [
    "predict(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d2638-c9a1-4b9b-be93-cca3e4c07869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
