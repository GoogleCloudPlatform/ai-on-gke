# Fine-tuning

Fine-tune a gemma instruction tuned model using a flipkart processed catalog. The dataset used
for fine-tuning is generated by the Vertex AI Gemini Flash model. The fine-tuned model can be deployed 
with an inference serving engine.

## Preparation
- Environment variables
```
PROJECT_ID=gkebatchexpce3c8dcb
PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
TRAINING_DATASET_BUCKET=kh-finetune-ds
V_MODEL_BUCKET=kr-finetune
NAMESPACE=ml-team
KSA=ray-worker
HF_TOKEN=<your token>
```

## GCS
The training data set is retrieved from a storage bucket and the fine-tuned model weights are saved onto a locally mounted storage bucket.


### Reading training data set
- Setup Workload Identity Federation to access the bucket with the generated prompts
```
gcloud storage buckets add-iam-policy-binding gs://${TRAINING_DATASET_BUCKET} \
    --member "principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/${NAMESPACE}/sa/${KSA}" \
    --role "roles/storage.objectUser"
```

### Writing fine-tuned model weights
- Create the bucket for storing the training data set
```
gcloud storage buckets create gs://${V_MODEL_BUCKET} \
    --project ${PROJECT_ID} \
    --location us
```

- Setup Workload Identity Federation to access the bucket to write the model weights
```
gcloud storage buckets add-iam-policy-binding gs://${V_MODEL_BUCKET} \
    --member "principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/${NAMESPACE}/sa/${KSA}" \
    --role "roles/storage.objectUser"
```

## Build the image of the source
- Modify cloudbuild.yaml to specify the image url
```
gcloud builds submit . --project ${PROJECT_ID}
```

## Deploy your Hugging Face token in your cluster
- Create secret for HF in your namespace
```
kubectl create secret generic hf-secret \
  --from-literal=hf_api_token=${HF_TOKEN} \
  --dry-run=client -o yaml | kubectl apply -n ${NAMESPACE} -f -
```

# Deploy the Job
## Fine-tuning Job Inputs
| Variable | Description | Example |
| --- | --- | --- |
| IMAGE_URL | The image url for the finetune image | |
| MLFLOW_ENABLE | Enable MLflow, empty will also disable | true/false | 
| EXPERIMENT | If MLflow is enabled. experiment ID used in MLflow | experiment- | 
| MLFLOW_TRACKING_URI | If MLflow is enabled, the tracking server URI | http://mlflow-tracking-service.ml-tools:5000 |
| MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING | If MLflow is enabled, track system level metrics, CPU/Memory/GPU| true/false |
| TRAINING_DATASET_BUCKET | The bucket which contains the generated prompts for fine-tuning. | kh-finetune-ds |
| TRAINING_DATASET_PATH | The path where the generated prompt data is for fine-tuning. | dataset/output |
| V_MODEL_BUCKET | The bucket which will be the destination of the fine-tuned model. | kr-finetune |
| MODEL_PATH | The output folder path for the fine-tuned model. This location will be used by the inference serving engine and model evaluation. | /model-data/model-gemma2-a100/experiment |
| MODEL_NAME | The Hugging Face path to the base model for fine-tuning. | google/gemma-2-9b-it |
| HF_TOKEN | The Hugging Face token used to pull the base model. | |

## Deploy the respective resources for the job and type of resource
- L4 - DWS
```
kubectl apply -f yaml/provisioning-request-l4.yaml -n ml-team
kubectl apply -f yaml/fine-tune-l4.yaml -n ml-team
```

- A100 - DWS
```
kubectl apply -f yaml/provisioning-request-a100.yaml -n ml-team
kubectl apply -f yaml/fine-tune-a100.yaml -n ml-team
```

- H100 - DWS
```
kubectl apply -f yaml/provisioning-request-h100.yaml -n ml-team
kubectl apply -f yaml/fine-tune-h100.yaml -n ml-team
```
