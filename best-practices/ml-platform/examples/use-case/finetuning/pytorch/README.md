# Fine-tuning

Fine-tune a Gemma Instruction Tuned model using a flipkart processed catalog. The dataset used
for fine-tuning is generated by the Vertex AI Gemini Flash model. The fine-tuned model can be deployed 
with an inference serving engine.

## Preparation
- Set Environment variables
```
PROJECT_ID=<your-project-id>
PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
TRAINING_DATASET_BUCKET=<training-dataset-bucket-name>
V_MODEL_BUCKET=<model-artifacts-bucket>
CLUSTER_NAME=<your-gke-cluster>
NAMESPACE=ml-team
KSA=<k8s-service-account>
HF_TOKEN=<your-Hugging-Face-account-token>
DOCKER_IMAGE_URL=us-docker.pkg.dev/${PROJECT_ID}/llm-finetuning/finetune:v1.0.0
```

## GCS
The training data set is retrieved from a storage bucket and the fine-tuned model weights are saved onto a locally mounted storage bucket.


### Reading training data set
- Setup Workload Identity Federation to access the bucket with the generated prompts
```
gcloud storage buckets add-iam-policy-binding gs://${TRAINING_DATASET_BUCKET} \
    --member "principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/${NAMESPACE}/sa/${KSA}" \
    --role "roles/storage.objectUser"
```

### Writing fine-tuned model weights
- Create the bucket for storing the training data set
```
gcloud storage buckets create gs://${V_MODEL_BUCKET} \
    --project ${PROJECT_ID} \
    --location us \
    --uniform-bucket-level-access

```

- Setup Workload Identity Federation to access the bucket to write the model weights
```
gcloud storage buckets add-iam-policy-binding gs://${V_MODEL_BUCKET} \
    --member "principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/${NAMESPACE}/sa/${KSA}" \
    --role "roles/storage.objectUser"
```

## Build the image of the source
- Create Artifact Registry repository for your docker image  
```
gcloud artifacts repositories create llm-finetuning \
--repository-format=docker \
--location=us \
--project=${PROJECT_ID} \
--async
```

- Enable the Cloud Build APIs
```
gcloud services enable cloudbuild.googleapis.com --project ${PROJECT_ID}
```
    
- Build container image using Cloud Build and push the image to Artifact Registry
  Modify cloudbuild.yaml to specify the image url    
```
sed -i "s|IMAGE_URL|${DOCKER_IMAGE_URL}|" cloudbuild.yaml && \
gcloud builds submit . --project ${PROJECT_ID}
```

## Deploy your Hugging Face token in your cluster
- Create secret for HF in your namespace
```
kubectl create secret generic hf-secret \
  --from-literal=hf_api_token=${HF_TOKEN} \
  --dry-run=client -o yaml | kubectl apply -n ${NAMESPACE} -f -
```

# Deploy the Job

Get credentials for the GKE cluster

```
gcloud container fleet memberships get-credentials ${CLUSTER_NAME} --project ${PROJECT_ID}
```

## Fine-tuning Job Inputs
| Variable | Description | Example |
| --- | --- | --- |
| IMAGE_URL | The image url for the finetune image | |
| MLFLOW_ENABLE | Enable MLflow, empty will also disable | true/false | 
| EXPERIMENT | If MLflow is enabled. experiment ID used in MLflow | experiment- | 
| MLFLOW_TRACKING_URI | If MLflow is enabled, the tracking server URI | http://mlflow-tracking-service.ml-tools:5000 |
| MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING | If MLflow is enabled, track system level metrics, CPU/Memory/GPU| true/false |
| TRAINING_DATASET_BUCKET | The bucket which contains the generated prompts for fine-tuning. |  |
| TRAINING_DATASET_PATH | The path where the generated prompt data is for fine-tuning. | dataset/output |
| V_MODEL_BUCKET | The bucket which will be the destination of the fine-tuned model. | |
| MODEL_PATH | The output folder path for the fine-tuned model. This location will be used by the inference serving engine and model evaluation. | /model-data/model-gemma2/experiment |
| MODEL_NAME | The Hugging Face path to the base model for fine-tuning. | google/gemma-2-9b-it |
| HF_TOKEN | The Hugging Face token used to pull the base model. | |

Update variables in the respective job submission manifest to reflect your configuration.

```
MLFLOW_ENABLE="false"
TRAINING_DATASET_PATH="dataset/output"
MODEL_PATH="/model-data/model-gemma2/experiment"
MODEL_NAME="google/gemma-2-9b-it"
```

Choose the accelerator (l4 | a100 | h100) as per your configuration
```
ACCELERATOR="l4"
```

   ``` 
   sed -i -e "s|IMAGE_URL|${DOCKER_IMAGE_URL}|" \
      -i -e "s|KSA|${KSA}|" \
      -i -e "s|V_MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING|${MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING}|" \
      -i -e "s|V_MLFLOW_ENABLE|${MLFLOW_ENABLE}|" \
      -i -e "s|V_EXPERIMENT|${EXPERIMENT}|" \
      -i -e "s|V_MLFLOW_TRACKING_URI|${MLFLOW_TRACKING_URI}|" \
      -i -e "s|V_TRAINING_DATASET_BUCKET|${TRAINING_DATASET_BUCKET}|" \
      -i -e "s|V_TRAINING_DATASET_PATH|${TRAINING_DATASET_PATH}|" \
      -i -e "s|V_MODEL_BUCKET|${V_MODEL_BUCKET}|" \
      -i -e "s|V_MODEL_PATH|${MODEL_PATH}|" \
      -i -e "s|V_MODEL_NAME|${MODEL_NAME}|" \
      -i -e "s|HF_TOKEN|${HF_TOKEN}|" \
      yaml/fine-tune-${ACCELERATOR}-dws.yaml

   ```

## Deploy the respective resources for the job and type of resource

```
kubectl apply -f yaml/provisioning-request-${ACCELERATOR}.yaml -n ml-team
kubectl apply -f yaml/fine-tune-${ACCELERATOR}-dws.yaml -n ml-team
```
