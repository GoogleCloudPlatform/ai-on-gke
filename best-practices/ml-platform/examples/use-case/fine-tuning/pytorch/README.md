# Fine-tuning

Fine-tune a Gemma Instruction Tuned model using a flipkart processed catalog. The dataset used
for fine-tuning is generated by the Vertex AI Gemini Flash model. The fine-tuned model can be deployed
with an inference serving engine.

## Prerequisites

- This guide was developed to be run on the [playground machine learning platform](/best-practices/ml-platform/examples/platform/playground/README.md). If you are using a different environment the scripts and manifest will need to be modified for that environment.
- A bucket containing the processed and prepared data from the [Data Preparation example](../../data-preparation/gemma-it)

## Preparation

- Clone the repository and change directory to the guide directory

  ```
  git clone https://github.com/GoogleCloudPlatform/ai-on-gke && \
  cd ai-on-gke/best-practices/ml-platform/examples/use-case/fine-tuning/pytorch
  ```

### Project variables

- Set `PROJECT_ID` to the project ID of the project where your GKE cluster and other resources will reside

  ```
  PROJECT_ID=
  ```

- Populate `PROJECT_NUMBER` based on the `PROJECT_ID` environment variable

  ```
  PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
  ```

### GCS bucket variables

- Set `DATA_BUCKET` to the name of your Google Cloud Storage (GCS) bucket where the data from [Data Processing](../../data-processing/ray) and [Data Preparation](../../data-preparation/gemma-it) is stored

  ```
  DATA_BUCKET=
  ```

- Set `MODEL_BUCKET` to the name of your Google Cloud Storage (GCS) bucket where the models will be stored

  ```
  MODEL_BUCKET=
  ```

### Kubernetes variables

- Set `NAMESPACE` to the Kubernetes namespace to be used

  ```
  NAMESPACE="ml-team"
  ```

- Set `KSA` to the Kubernetes service account to be used

  ```
  KSA="app-sa"
  ```

- Set `CLUSTER_NAME` to the name of your GKE cluster

  ```
  CLUSTER_NAME=
  ```

### Container image variables

- Set `DOCKER_IMAGE_URL` to the URL for the container image that will be created

  ```
  DOCKER_IMAGE_URL="us-docker.pkg.dev/${PROJECT_ID}/llm-finetuning/finetune:v1.0.0"
  ```

### Access token variables

- Set `HF_TOKEN` to your HuggingFace access token. Go to https://huggingface.co/settings/tokens , click `Create new token` , provide a token name, select `Read` in token type and click `Create token`.

  ```
  HF_TOKEN=
  ```

## Configuration

- Setup Workload Identity Federation to access the bucket

  ```sh
  gcloud storage buckets add-iam-policy-binding gs://${MODEL_BUCKET} \
  --member "principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/${NAMESPACE}/sa/${KSA}" \
  --role "roles/storage.objectUser"
  ```

- Create the bucket for storing the models

  ```sh
  gcloud storage buckets create gs://${MODEL_BUCKET} \
  --project ${PROJECT_ID} \
  --location us \
  --uniform-bucket-level-access
  ```

## Build the container image

- Enable the Cloud Build APIs

  ```sh
  gcloud services enable cloudbuild.googleapis.com --project ${PROJECT_ID}
  ```

- Build the container image using Cloud Build and push the image to Artifact Registry

  ```
  cd src
  gcloud builds submit --config cloudbuild.yaml \
  --project ${PROJECT_ID} \
  --substitutions _DESTINATION=${DOCKER_IMAGE_URL}
  cd ..
  ```

## Run the job

- Get credentials for the GKE cluster

  ```sh
  gcloud container fleet memberships get-credentials ${CLUSTER_NAME} --project ${PROJECT_ID}
  ```

- Create a Kubernetes secret with your HuggingFace token

  ```sh
  kubectl create secret generic hf-secret \
  --from-literal=hf_api_token=${HF_TOKEN} \
  --dry-run=client -o yaml | kubectl apply -n ${NAMESPACE} -f -
  ```

- Accept the license HuggingFace license for the model

  - Go to https://huggingface.co/google/gemma-2-9b-it and accept the license

- Configure the job

  | Variable                             | Description                                                                                                                       | Example                                        |
  | ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
  | ACCELERATOR                          | Type of GPU accelerator to use (l4, a100, h100)                                                                                   | l4                                             |
  | DATA_BUCKET_DATASET_PATH             | The path where the generated prompt data is for fine-tuning.                                                                      | dataset/output/training                        |
  | EXPERIMENT                           | If MLflow is enabled. experiment ID used in MLflow                                                                                | experiment-                                    |
  | HF_BASE_MODEL_NAME                   | The Hugging Face path to the base model for fine-tuning.                                                                          | google/gemma-2-9b-it                           |
  | MLFLOW_ENABLE                        | Enable MLflow, empty will also disable                                                                                            | true/false                                     |
  | MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING | If MLflow is enabled, track system level metrics, CPU/Memory/GPU                                                                  | true/false                                     |
  | MLFLOW_TRACKING_URI                  | If MLflow is enabled, the tracking server URI                                                                                     | <http://mlflow-tracking-service.ml-tools:5000> |
  | MODEL_PATH                           | The output folder path for the fine-tuned model. This location will be used by the inference serving engine and model evaluation. | /model-data/model-gemma2/experiment            |

  ```sh
  ACCELERATOR="l4"
  DATA_BUCKET_DATASET_PATH="dataset/output/training"
  EXPERIMENT=""
  HF_BASE_MODEL_NAME="google/gemma-2-9b-it"
  MLFLOW_ENABLE="false"
  MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING="false"
  MLFLOW_TRACKING_URI=""
  MODEL_PATH="/model-data/model-gemma2/experiment"
  ```

  ```sh
  sed \
  -i -e "s|V_DATA_BUCKET|${DATA_BUCKET}|" \
  -i -e "s|V_EXPERIMENT|${EXPERIMENT}|" \
  -i -e "s|V_MODEL_NAME|${HF_BASE_MODEL_NAME}|" \
  -i -e "s|V_IMAGE_URL|${DOCKER_IMAGE_URL}|" \
  -i -e "s|V_KSA|${KSA}|" \
  -i -e "s|V_MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING|${MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING}|" \
  -i -e "s|V_MLFLOW_ENABLE|${MLFLOW_ENABLE}|" \
  -i -e "s|V_MLFLOW_TRACKING_URI|${MLFLOW_TRACKING_URI}|" \
  -i -e "s|V_MODEL_BUCKET|${MODEL_BUCKET}|" \
  -i -e "s|V_MODEL_PATH|${MODEL_PATH}|" \
  -i -e "s|V_TRAINING_DATASET_PATH|${DATA_BUCKET_DATASET_PATH}|" \
  manifests/fine-tune-${ACCELERATOR}-dws.yaml
  ```

- Create the provisioning request and job

```sh
kubectl --namespace ${NAMESPACE} apply -f manifests/provisioning-request-${ACCELERATOR}.yaml
kubectl --namespace ${NAMESPACE} apply -f manifests/fine-tune-${ACCELERATOR}-dws.yaml
```

- Verify the completion of the job

  In the Google Cloud console, go to the [Logs Explorer](https://console.cloud.google.com/logs) page to run the following query to see the completion of the job.

  ```sh
  labels."k8s-pod/app"="finetune-job"
  textPayload: "finetune - INFO - ### Completed ###"
  ```

- After the fine-tuning job is successful, the model bucket should have a checkpoint folder created.

  ```sh
  gcloud storage ls gs://${MODEL_BUCKET}/${MODEL_PATH}
  ```
