# Fine-tuning

Fine-tune a Gemma Instruction Tuned model using a flipkart processed catalog. The dataset used
for fine-tuning is generated by the Vertex AI Gemini Flash model. The fine-tuned model can be deployed
with an inference serving engine.

## Prerequisites

- This guide was developed to be run on the [playground machine learning platform](/best-practices/ml-platform/examples/platform/playground/README.md). If you are using a different environment the scripts and manifest will need to be modified for that environment.
- A bucket containing the processed and prepared data from the [Data Preparation example](../../data-preparation/gemma-it)

## Preparation

- Clone the repository and change directory to the guide directory

  ```
  git clone https://github.com/GoogleCloudPlatform/ai-on-gke && \
  cd ai-on-gke/best-practices/ml-platform/examples/use-case/fine-tuning/pytorch
  ```

- Ensure that your `MLP_ENVIRONMENT_FILE` is configured

  ```
  cat ${MLP_ENVIRONMENT_FILE} && \
  source ${MLP_ENVIRONMENT_FILE}
  ```

  > You should see the various variables populated with the information specific to your environment.

### Access token variables

- Set `HF_TOKEN` to your HuggingFace access token. Go to https://huggingface.co/settings/tokens , click `Create new token` , provide a token name, select `Read` in token type and click `Create token`.

  ```
  HF_TOKEN=
  ```

## Build the container image

- Build the container image using Cloud Build and push the image to Artifact Registry

  ```
  cd src
  sed -i -e "s|^serviceAccount:.*|serviceAccount: projects/${MLP_PROJECT_ID}/serviceAccounts/${MLP_BUILD_GSA}|" cloudbuild.yaml
  gcloud beta builds submit --config cloudbuild.yaml \
  --project ${MLP_PROJECT_ID} \
  --substitutions _DESTINATION=${MLP_FINE_TUNING_IMAGE}
  cd ..
  ```

## Run the job

- Get credentials for the GKE cluster

  ```sh
  gcloud container fleet memberships get-credentials ${MLP_CLUSTER_NAME} --project ${MLP_PROJECT_ID}
  ```

- Create a Kubernetes secret with your HuggingFace token

  ```sh
  kubectl create secret generic hf-secret \
  --from-literal=hf_api_token=${HF_TOKEN} \
  --dry-run=client -o yaml | kubectl apply -n ${MLP_KUBERNETES_NAMESPACE} -f -
  ```

- Accept the license HuggingFace license for the model

  - Go to https://huggingface.co/google/gemma-2-9b-it and accept the license

- Configure the job

  | Variable                             | Description                                                                                                                       | Example                                        |
  | ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
  | ACCELERATOR                          | Type of GPU accelerator to use (l4, a100, h100)                                                                                   | l4                                             |
  | DATA_BUCKET_DATASET_PATH             | The path where the generated prompt data is for fine-tuning.                                                                      | dataset/output/training                        |
  | EXPERIMENT                           | If MLflow is enabled. experiment ID used in MLflow                                                                                | experiment-                                    |
  | HF_BASE_MODEL_NAME                   | The Hugging Face path to the base model for fine-tuning.                                                                          | google/gemma-2-9b-it                           |
  | MLFLOW_ENABLE                        | Enable MLflow, empty will also disable                                                                                            | true/false                                     |
  | MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING | If MLflow is enabled, track system level metrics, CPU/Memory/GPU                                                                  | true/false                                     |
  | MLFLOW_TRACKING_URI                  | If MLflow is enabled, the tracking server URI                                                                                     | <http://mlflow-tracking-service.ml-tools:5000> |
  | MODEL_PATH                           | The output folder path for the fine-tuned model. This location will be used by the inference serving engine and model evaluation. | /model-data/model-gemma2/experiment            |

  ```sh
  ACCELERATOR="l4"
  DATA_BUCKET_DATASET_PATH="dataset/output/training"
  EXPERIMENT=""
  HF_BASE_MODEL_NAME="google/gemma-2-9b-it"
  MLFLOW_ENABLE="false"
  MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING="false"
  MLFLOW_TRACKING_URI=""
  MODEL_PATH="/model-data/model-gemma2/experiment"
  ```

  ```sh
  sed \
  -i -e "s|V_DATA_BUCKET|${MLP_DATA_BUCKET}|" \
  -i -e "s|V_EXPERIMENT|${EXPERIMENT}|" \
  -i -e "s|V_MODEL_NAME|${HF_BASE_MODEL_NAME}|" \
  -i -e "s|V_IMAGE_URL|${MLP_FINE_TUNING_IMAGE}|" \
  -i -e "s|V_KSA|${MLP_FINE_TUNING_KSA}|" \
  -i -e "s|V_MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING|${MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING}|" \
  -i -e "s|V_MLFLOW_ENABLE|${MLFLOW_ENABLE}|" \
  -i -e "s|V_MLFLOW_TRACKING_URI|${MLFLOW_TRACKING_URI}|" \
  -i -e "s|V_MODEL_BUCKET|${MLP_MODEL_BUCKET}|" \
  -i -e "s|V_MODEL_PATH|${MODEL_PATH}|" \
  -i -e "s|V_TRAINING_DATASET_PATH|${DATA_BUCKET_DATASET_PATH}|" \
  manifests/fine-tune-${ACCELERATOR}-dws.yaml
  ```

- Create the provisioning request and job

  ```sh
  kubectl --namespace ${MLP_KUBERNETES_NAMESPACE} apply -f manifests/provisioning-request-${ACCELERATOR}.yaml
  kubectl --namespace ${MLP_KUBERNETES_NAMESPACE} apply -f manifests/fine-tune-${ACCELERATOR}-dws.yaml
  ```

- Verify the completion of the job

  In the Google Cloud console, go to the [Logs Explorer](https://console.cloud.google.com/logs) page to run the following query to see the completion of the job.

  ```sh
  labels."k8s-pod/app"="finetune-job"
  textPayload: "finetune - INFO - ### Completed ###"
  ```

- After the fine-tuning job is successful, the model bucket should have a checkpoint folder created.

  ```sh
  gcloud storage ls gs://${MLP_MODEL_BUCKET}/${MODEL_PATH}
  ```
