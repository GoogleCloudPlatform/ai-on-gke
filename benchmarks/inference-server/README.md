This directory contains the inference server specific setup and the
Terraform templates associated with them.

The current supported options are:
- Text Generation Inference (aka TGI)
- TensorRT-LLM on Triton Inference Server 

You may also choose to manually deploy your own inference server.

To deploy an inference server, cd into the respective directory and follow
instructions on the respective README.md