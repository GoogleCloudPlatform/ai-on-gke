# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi
  namespace: ${namespace}
  labels:
    app: tgi
spec:
  selector:
    matchLabels:
      app: tgi
  template:
    metadata:
      labels:
        app: tgi
        ai.gke.io/inference-server: text-generation-inference
        examples.ai.gke.io/source: ai-on-gke-benchmarks
    spec:
      serviceAccountName: ${ksa}
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi # important for L4, see note https://github.com/huggingface/text-generation-inference#a-note-on-shared-memory-shm
%{ for hugging_face_token_secret in hugging_face_token_secret_list ~}
        - name: hftoken
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: "gcp-secret-provider"
%{ endfor ~}
        - name: data
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/data
      containers:
        - name: text-generation-inference
          ports:
            - containerPort: 80
          image: "ghcr.io/huggingface/text-generation-inference:1.4.2"
          args: ["--model-id", "${model_id}", "--num-shard", "${gpu_count}", "--max-concurrent-requests", "${max_concurrent_requests}"]
          env:
%{ for hugging_face_token_secret in hugging_face_token_secret_list ~}
            - name: HUGGING_FACE_HUB_TOKEN # Related token consumption
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: HF_TOKEN
%{ endfor ~}
%{ if quantization != "" ~}
            - name: QUANTIZE
              value: "${quantization}"
%{ endif ~}
          resources:
            limits:
              nvidia.com/gpu: ${gpu_count} # number of gpu's allocated to workload
            requests:
              cpu: "1"
          serviceAccountName: ${ksa}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /data
              name: data
%{ for hugging_face_token_secret in hugging_face_token_secret_list ~}
            - mountPath: "/var/secrets"
              name: hftoken
%{ endfor ~}
          # Hack: tgi won't generate any metrics until it's served at least one
          # request, and without that, autoscaling won't work. So generate one
          # request upon startup.
          startupProbe:
            failureThreshold: 3600
            periodSeconds: 10
            timeoutSeconds: 60
            exec:
              command:
                - /usr/bin/curl
                - http://localhost:80/generate
                - -X
                - POST
                - -d
                - '{"inputs":"test", "parameters":{"max_new_tokens":1}}'
                - -H
                - 'Content-Type: application/json'
                - --fail
