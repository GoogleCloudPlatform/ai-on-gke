# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mixtral8x7b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mixtral8x7b
  template:
    metadata:
      labels:
        app: mixtral8x7b
    spec:
      nodeSelector:
        cloud.google.com/gke-accelerator: "nvidia-l4"
      containers:
      - name: mixtral8x7b
        image: ghcr.io/huggingface/text-generation-inference:1.4.3
        ports:
        - name: server-port
          containerPort: 8080
        env:
        - name: QUANTIZE
          value: bitsandbytes-nf4
        - name: MODEL_ID
          value: mistralai/Mixtral-8x7B-Instruct-v0.1
        - name: NUM_SHARD
          value: "4"
        - name: PORT
          value: "8080"
        resources:
          requests:
            cpu: "5"
            memory: "42Gi"
            nvidia.com/gpu: "2"
          limits:
            cpu: "5"
            memory: "42Gi"
            nvidia.com/gpu: "2"
        volumeMounts:
          # mountPath is set to /data as it's the path where the HF_HOME environment
          # variable points to in the TGI container image i.e. where the downloaded model from the Hub will be
          # stored
          - mountPath: /data
            name: ephemeral-volume
          - mountPath: /dev/shm
            name: dshm
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: data
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/mixtral-data
        - name: ephemeral-volume
          ephemeral:
            volumeClaimTemplate:
              spec:
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 150Gi
                storageClassName: "premium-rwo"
              metadata:
                labels:
                  type: ephemeral
---
apiVersion: v1
kind: Service
metadata:
  name: mixtral8x7b-service
  namespace: default
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: mixtral8x7b

# THIS BELOW IS AN STUB-EXAMPLE AND REQUIRES A QUITE A BIT OF CONFIGURATION TO WORK WITH DGCM (Please consider using prometheus)
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: mistral-7b-hpa
#   annotations:
# spec:
#   maxReplicas: 10
#   minReplicas: 2
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: mixtral-8x7b
#   metrics:
#   - type: External
#     external:
#       metric:
#         name: "external.googleapis.com|prometheus|DCGM_FI_DEV_GPU_UTIL"
#         selector:
#           matchLabels:
#             vendor: "nvidia"
#             model: "L4"  # specific GPU model
#       target:
#         type: Value
#         value: 85  # Adjust to target a GPU utilization threshold for scaling
#   behavior:  # Move behavior section to spec
#     scaleDown:
#       stabilizationWindowSeconds: 300
#       selectPolicy: Max
#       policies:
#       - type: Pods
#         value: 1
#         periodSeconds: 60
#       - type: Percent
#         value: 10
#         periodSeconds: 60
#     scaleUp:
#       stabilizationWindowSeconds: 20
#       selectPolicy: Max
#       policies:
#       - type: Pods
#         value: 2
#         periodSeconds: 60
#       - type: Percent
#         value: 20
#         periodSeconds: 60
