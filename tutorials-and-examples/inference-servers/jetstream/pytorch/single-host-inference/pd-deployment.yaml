apiVersion: apps/v1
kind: Deployment
metadata:
  name: jetstream-pytorch-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jetstream-pytorch-server
  template:
    metadata:
      labels:
        app: jetstream-pytorch-server
    spec:
      nodeSelector:
        cloud.google.com/gke-tpu-topology: 2x4
        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
      containers:
      - name: jetstream-pytorch-server
        image: us-docker.pkg.dev/cloud-tpu-images/inference/jetstream-pytorch-server:v0.2.0
        args:
        - --param_size=7b
        - --batch_size=80
        - --max_cache_length=2048
        - --platform=tpu=8
        - --quantize_weights=False
        - --quantize_kv_cache=False
        - --tokenizer_path=/jetstream-pytorch/jetstream_pt/third_party/llama2/tokenizer.model
        - --checkpoint_path=/models/llama2-7b/bf16/model.safetensors
        ports:
        - containerPort: 9000
        volumeMounts:
          - mountPath: /models/llama2-7b/bf16
            name: checkpoint-volume
        resources:
          requests:
            google.com/tpu: 8
          limits:
            google.com/tpu: 8
      - name: jetstream-http
        image: us-docker.pkg.dev/cloud-tpu-images/inference/jetstream-http:v0.2.0
        ports:
        - containerPort: 8000
      volumes:
        - name: checkpoint-volume
          persistentVolumeClaim:
            claimName: checkpoint-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: jetstream-svc
spec:
  selector:
    app: jetstream-pytorch-server
  ports:
  - protocol: TCP
    name: jetstream-http
    port: 8000
    targetPort: 8000
  - protocol: TCP
    name: jetstream-grpc
    port: 9000
    targetPort: 9000