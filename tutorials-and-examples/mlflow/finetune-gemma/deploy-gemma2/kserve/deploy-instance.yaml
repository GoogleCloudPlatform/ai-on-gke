apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: gemma-2-9b-finetuned
spec:
  predictor:
    serviceAccountName: default
    annotations:
      gke-gcsfuse/volumes: "true"
    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-tesla-a100
    model:
      modelFormat:
        name: mlflow
      env:
      - name: MLSERVER_ENV_TARBALL
        value: "/data/mlflow-gemma2-env.tar.gz"
      protocolVersion: v2
      storageUri: gs://dima_mlflow_artifact_storage/artifacts/1012d72783ca4b8689612df905fe52d2/artifacts/model
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          nvidia.com/gpu: 1
      volumeMounts:
      - name: gcsfuse-conda-env
        mountPath: /data
        readOnly: true
    volumes:
    - name: gcsfuse-conda-env
      csi:
        driver: gcsfuse.csi.storage.gke.io
        volumeAttributes:
          bucketName: dima_mlflow_artifact_storage
          mountOptions: "implicit-dirs"
